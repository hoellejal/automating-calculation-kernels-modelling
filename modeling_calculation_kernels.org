# -*- coding: utf-8 -*-
# -*- mode: org -*-
#+Title:  Modeling calculation kernels with Stan
#+Author: HoÃ«l Jalmin \newline Tutored by Arnaud Legrand and Tom Cornebize
#+DATE: The 21th of June, 2019
#+LANGUAGE: en
#+STARTUP: beamer indent inlineimages logdrawer
#+TAGS: noexport(n)

#+PROPERTY: header-args  :session :eval never-export :exports both
#+DRAWERS: latex_headers

:latex_headers:
#+LaTeX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [10pt,presentation,xcolor={usenames,dvipsnames,svgnames,table}]
#+OPTIONS:   H:2 num:t toc:nil \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t
#+LATEX_COMPILER: lualatex
#+LATEX_HEADER: \usedescriptionitemofwidthas{bl}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{figlatex}
#+LATEX_HEADER: \usepackage[french]{babel}
#+LATEX_HEADER: %\usepackage{DejaVuSansMono}
#+LATEX_HEADER: \usepackage{ifthen,amsmath,amstext,gensymb,amssymb}
#+LATEX_HEADER: \usepackage{boxedminipage,xspace,multicol}
#+LATEX_HEADER: %%%%%%%%% Begin of Beamer Layout %%%%%%%%%%%%%
#+LATEX_HEADER: \ProcessOptionsBeamer
#+LATEX_HEADER: \usetheme[numbering=fraction,titleformat=smallcaps,progressbar=frametitle]{metropolis}
#+LATEX_HEADER: \usepackage{fontawesome}
#+LATEX_HEADER: \usecolortheme[named=BrickRed]{structure}
#+LATEX_HEADER: %%%%%%%%% End of Beamer Layout %%%%%%%%%%%%%
#+LATEX_HEADER: \usepackage{verbments}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{url} \urlstyle{sf}
#+LATEX_HEADER: \let\alert=\structure % to make sure the org * * works of tools
#+LATEX_HEADER: %\let\tmptableofcontents=\tableofcontents
#+LATEX_HEADER: %\def\tableofcontents{}
#+LATEX_HEADER:  \usepackage[normalem]{ulem}
#+LATEX_HEADER:  \usepackage{color,soul}
#+LATEX_HEADER:  \definecolor{lightorange}{rgb}{1,.9,.7}
#+LATEX_HEADER:  \sethlcolor{lightorange}
#+LATEX_HEADER:  \definecolor{lightgreen}{rgb}{.7,.9,.7}
#+LATEX_HEADER:  \let\hrefold=\href
#+LATEX_HEADER:  \renewcommand{\href}[2]{\hrefold{#1}{\SoulColor{lightorange}\hl{#2}}}
#+LATEX_HEADER: % \renewcommand{\uline}[1]{\SoulColor{lightorange}\hl{#1}}
#+LATEX_HEADER: \renewcommand{\emph}[1]{\SoulColor{lightorange}\hl{#1}}
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \newcommand\SoulColor[1]{%
#+LATEX_HEADER:   \sethlcolor{#1}%
#+LATEX_HEADER:   \let\set@color\beamerorig@set@color%
#+LATEX_HEADER:   \let\reset@color\beamerorig@reset@color}
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \let\oldtexttt=\texttt
#+LATEX_HEADER: % \renewcommand\texttt[1]{\SoulColor{lightgreen}\hl{\tt#1}}
#+LATEX_HEADER: % \renewcommand\alert[1]{\SoulColor{lightgreen}\hl{#1}}
#+LATEX_HEADER: % \AtBeginSection{\begin{frame}{Outline}\tableofcontents\end{frame}}
#+LATEX_HEADER: \graphicspath{{fig/}}
#+LATEX_HEADER: \usepackage{tikzsymbols}
#+LATEX_HEADER: \def\smiley{\Smiley[1][green!80!white]}
#+LATEX_HEADER: \def\frowny{\Sadey[1][red!80!white]}
#+LATEX_HEADER: \def\winkey{\Winkey[1][yellow]}

#+BEGIN_EXPORT latex
  \newcommand{\myfbox}[2][gray!20]{\bgroup\scalebox{.7}{\colorbox{#1}{{\vphantom{pS}#2}}}\egroup} % \fbox
  %\def\myfbox#1{#1} % \fbox
  \def\HPC{\myfbox[gray!40]{HPC}}
  \def\NET{\myfbox[gray!40]{Network}}
  \def\SG{\myfbox[gray!40]{Smart Grids}}
  \def\ECO{\myfbox[gray!40]{Economics}}
  \def\PRIV{\myfbox[gray!40]{Privacy}}
  \def\TRACING{\myfbox[red!20]{Tracing}}
  \def\SIM{\myfbox[green!20]{Simulation}}
  \def\VIZ{\myfbox[red!40]{Visualization}}
  \def\MODELING{\myfbox[green!40]{Stochastic Models}}
  \def\OPT{\myfbox[blue!20]{Optimization}}
  \def\GT{\myfbox[blue!40]{Game Theory}}
#+END_EXPORT


#+BEGIN_EXPORT latex
\def\changefont#1{%
  \setbeamertemplate{itemize/enumerate body begin}{#1}
  \setbeamertemplate{itemize/enumerate subbody begin}{#1}
  #1}
\makeatletter
\newcommand{\verbatimfont}[1]{\renewcommand{\verbatim@font}{\ttfamily#1}}
\makeatother
\verbatimfont{\scriptsize}%small
\let\endmintedbak=\endminted
\def\endminted{\endmintedbak\vspace{-1cm}}
#+END_EXPORT

#+BEGIN_EXPORT latex
\newcommand{\Norm}{\ensuremath{\mathcal{N}}\xspace}
\newcommand{\Unif}{\ensuremath{\mathcal{U}}\xspace}
\newcommand{\Triang}{\ensuremath{\mathcal{T}}\xspace}
\newcommand{\Exp}{\ensuremath{\mathcal{E}}\xspace}
\newcommand{\Bernouilli}{\ensuremath{\mathcal{B}}\xspace}
\newcommand{\Like}{\ensuremath{\mathcal{L}}\xspace}
\newcommand{\Model}{\ensuremath{\mathcal{M}}\xspace}
\newcommand{\E}{\ensuremath{\mathbb{E}}\xspace}
\def\T{\ensuremath{\theta}\xspace}
\def\Th{\ensuremath{\hat{\theta}}\xspace}
\def\Tt{\ensuremath{\tilde{\theta}}\xspace}
\def\Y{\ensuremath{y}\xspace}
\def\Yh{\ensuremath{\hat{y}}\xspace}
\def\Yt{\ensuremath{\tilde{y}}\xspace}
\let\epsilon=\varepsilon
\let\leq=\leqslant
\let\geq=\geqslant
#+END_EXPORT
:end:

# https://cran.r-project.org/web/packages/plot3D/vignettes/plot3D.pdf
# http://htmlpreview.github.io/?https://github.com/AckerDWM/gg3D/blob/master/gg3D-vignette.html

# http://bechtel.colorado.edu/~bracken/tutorials/stan/stan-tutorial.pdf
# http://jakewestfall.org/misc/SorensenEtAl.pdf
# https://github.com/AllenDowney/BayesMadeSimple

# https://github.com/bob-carpenter/prob-stats

#+BEGIN_EXPORT latex
#+END_EXPORT

** Context
With the current need for high performance computing, and the hardware
complexity:
- How to predict the duration of calculations?
- How to check if the performance is normal?

*** For this talk:
1. Brief presentation of the context
2. Introduction to Bayesian sampling
3. Examples of application

** Background on HPC and Polaris research
*** Modern context
  - HPC systems use thousands of nodes, cache, hyperthreading, etc -> makes it difficult to predict performance
  - Some functions are used everywhere, and called thousands of times

*** Polaris research
  - Simulating HPL on smaller supercomputers to optimize it at a
    lesser cost
  - Elaborated complex models but needed to evaluate and confirm the models

** HPL simulation
*** Examples
  - The blas library (especially matrix per matrix multiplication) is used by thousands of programs and constitutes
    most of HPL calculations.
  - Performance variability is also caused by network communications
  - Needs to check the models with bayesian sampling.

** Bayes model
- Model :: Let's say $y \sim \Norm(\mu,\sigma)$
  - *$\mu$*: Model *parameters*
  - *$y$*: Dependent *data* (posterior)
  - *$\sigma$*: Independent data (prior)
  We observe some data and need to find model parameters

*** The vocabulary
  - *Posterior*: The distribution of the parameters
  - *Likelihood*: A function of the parameters, the model
  - *Prior*: Existing knowledge of the system, guesses on the parameters
    values
$\boxed{\underbrace{p(\mu|y,\sigma)}_{\text{\alert{Posterior}}} \propto \underbrace{p(y|\mu,\sigma)}_{\text{\alert{Likelihood}}}\underbrace{p(\mu,\sigma)}_{\text{\alert{Prior}}}}$
  assuming $y \sim \Model(\mu,\sigma)$

* A Bayesian Sampler, Stan
** With a simple example
#+begin_src R :results output :session *R* :exports none
generate_dataset=function(intercept, coefficient, N, min_x=0, max_x=100, sigma=1){
    x = sample(min_x:max_x,N,replace=T) 
    y = coefficient * x + intercept + rnorm(N,sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}
df=generate_dataset(50, -2, 500, sigma=15)
#+end_src

[[file:./images/ex1_figure.png]]
Using this data, we'll try to find the parameters that were used to
generate it.

** The Stan model
#+begin_example
library(rstan)

modelString = "data { // the observations
    int<lower=1> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real coefficient;
    real<lower=0> sigma; // indication: sigma cannot be negative
} 
model {
    // We define our priors
    intercept   ~ normal(0, 10); // We know that all the parameters follow a normal distribution
    coefficient ~ normal(0, 10);
    sigma       ~ normal(0, 10);

    // Then, our likelihood function
    y ~ normal(coefficient*x + intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
#+end_example

#+begin_src R :results output :session *R* :exports none
library(rstan)

modelString = "data { // the observations
    int<lower=1> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real coefficient;
    real<lower=0> sigma; // indication: sigma cannot be negative
} 
model {
    // We define our priors
    intercept   ~ normal(0, 10); // We know that all the parameters follow a normal distribution
    coefficient ~ normal(0, 10);
    sigma       ~ normal(0, 10);

    // Then, our likelihood function
    y ~ normal(coefficient*x + intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
#+end_src

** Making the fit and checking the results
#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
#+end_src

#+begin_src R :results output :session *R* :exports both
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: ea4b5a288cf5f1d87215860103a9026e.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                mean se_mean   sd     2.5%      25%      50%      75%    97.5%
intercept      49.86    0.04 1.36    47.14    48.94    49.87    50.82    52.40
coefficient    -2.00    0.00 0.02    -2.04    -2.01    -2.00    -1.98    -1.95
sigma          15.03    0.01 0.47    14.18    14.70    15.02    15.35    15.99
lp__        -1615.90    0.04 1.12 -1618.80 -1616.45 -1615.62 -1615.05 -1614.58
            n_eff Rhat
intercept    1070 1.00
coefficient  1042 1.00
sigma        1042 1.01
lp__          871 1.00

Samples were drawn using NUTS(diag_e) at Wed Jun 19 17:07:18 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

** Checking the convergence

#+begin_src R :results output graphics :file ./images/ex1_stan_trace.png :exports both :session *R* 
stan_trace(fit)
#+end_src

#+RESULTS:
[[file:./images/ex1_stan_trace.png]]
** Generating new data
- Generating new data to check the results and model's accuracy.

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=1> N;
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real coefficient;
    real<lower=0> sigma;
} 
model {
    intercept   ~ normal(0, 10);
    coefficient ~ normal(0, 10);
    sigma       ~ normal(0, 10);
    y ~ normal(coefficient*x + intercept, sigma);
}
generated quantities {
   real x_pos = x[categorical_rng(rep_vector(1,N) / N)];
   real y_pos; // posterior predictions
   y_pos = normal_rng(coefficient*x_pos+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)

#+end_src

** Plotting the new data
#+begin_src R :results output :session *R* :exports both
extracted=rstan::extract(fit)
df_generated = data.frame(x=extracted$x_pos, y=extracted$y_pos, origin='generated')
ggplot(tmp, aes(x=x, y=y, color=origin)) + geom_point(alpha=0.5)
#+end_src

[[file:./images/ex3_reworked_comparative_plot.png]]

** The importance of the priors
- The priors are necessary to have convergence in the fit
- Non-informative prior vs informative (careful not to have a falsely
  informative one and introduce bias)
- A little bit of precision is better, but initialisation values can
  make the trick
* The different models for dgemm
** The theory behind it
- Dgemm's duration depends on the matrix size, but also on the CPU
  used to run it
[[file:./images/dgemm_duration_on_mnk_restrained.png]]
** The possible models
Different possible models, some more accurate than others:
[[file:./images/models_paper.png]]
** A M-2 N-2 model (polynomial with noise depending on x) 
#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] mnk;
    vector[N] duration;
    vector[5] mu_mean; //the priors for mu and sigma
    vector[5] mu_sd;
    vector[5] sigma_mean;
    vector[5] sigma_sd;
}
parameters {
    vector[5] mu_raw; vector[5] sigma_raw;
}
transformed parameters {
    vector[5] mu; vector[5] sigma;
    for(j in 1:5){
      mu[j] = mu_mean[j] + mu_raw[j] * mu_sd[j];
      sigma[j] = sigma_mean[j] + sigma_raw[j] * sigma_sd[j];}
}
model {
    mu_raw ~ normal(0,1); sigma_raw ~ normal(0,1);
    duration ~ normal(mu[1]*mnk + mu[2]*mn + mu[3]*mk + mu[4]*nk + mu[5], 
    sigma[1]*mnk + sigma[2]*mn + sigma[3]*mk + sigma[4]*nk + sigma[5]);
}
"
#+end_src

** The generated data 
[[file:./images/generated_quantities_dgemm_m-2_second_test.png]]
** A M_H-2 N_H-2 model (depending on the host)
- Much like the previous model, but with different observations for
  each host
- Added a variable for the number of hosts, and used matrixes instead
  of vectors for all the parameters.

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int hosts;
}
parameters {
    matrix[hosts,5] mu_raw; 
    matrix[hosts,5] sigma_raw;
}
transformed parameters {
    matrix[hosts,5] mu; 
    matrix[hosts,5] sigma;
}
model {
    for(i in 1:hosts){
      duration[i] ~ normal(mu[i,1]*mnk[i]+mu[i,2]*mn[i]+mu[i,3]*mk[i]+
      mu[i,4]*nk[i]+mu[i,5], sigma[i,1]*mnk[i]+sigma[i,2]*mn[i]+
      sigma[i,3]*mk[i]+sigma[i,4]*nk[i]+sigma[i,5]);}
}
"
#+end_src
** A hierarchical M_H-1 N_H-2 model
- Useful to find the value of hyperparameters from which we get the parameters
- From this we could calculate new parameters for new CPUs
- Here \mu-alpha and \sigma-alpha are the hyperparameters for alpha, and
  the same goes for the other parameters

$\mu-alpha \sim \Norm(alpha_\mu,alpha_\sigma)$ with alpha_\mu and alpha_\sigma the priors

$\sigma-alpha \sim \Norm(0,1)$

$alpha[i] \sim \Norm(\mu-alpha, \sigma-alpha)$

$duration[i] \sim \Norm(alpha[i]*mnk + beta[i], teta[i]*mnk + gamma[i])$

** Posterior visualisation
The posterior with models depending on the host shows a lot of
difference between hosts (here we have 3 "average" CPU and a slow one):

[[file:./images/intercept_on_mu_posterior.png]]
** Posterior visualisation

If we look at the means of the parameters' values for each host, we
get a range of values in which most hosts are.

[[file:./images/mu_on_intercept_means.png]]

** The follow up
  - Modeling other calculation kernels
  - Modeling the network communications
  - Parsing and converting Stan code to C, to generate new data more efficiently
  - Anomaly detection
