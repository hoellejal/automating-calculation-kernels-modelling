# -*- coding: utf-8 -*-
# -*- mode: org -*-
#+Title:  Modeling computation kernels with Stan
#+Author: HoÃ«l Jalmin \newline Tutored by Arnaud Legrand and Tom Cornebize
#+DATE: The 21th of June, 2019
#+LANGUAGE: en
#+STARTUP: beamer indent inlineimages logdrawer
#+TAGS: noexport(n)

#+PROPERTY: header-args  :session :eval never-export :exports both
#+DRAWERS: latex_headers

:latex_headers:
#+LaTeX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [10pt,presentation,xcolor={usenames,dvipsnames,svgnames,table}]
#+OPTIONS:   H:2 num:t toc:nil \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t
#+LATEX_COMPILER: lualatex
#+LATEX_HEADER: \usedescriptionitemofwidthas{bl}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{figlatex}
#+LATEX_HEADER: \usepackage[french]{babel}
#+LATEX_HEADER: %\usepackage{DejaVuSansMono}
#+LATEX_HEADER: \usepackage{ifthen,amsmath,amstext,gensymb,amssymb}
#+LATEX_HEADER: \usepackage{boxedminipage,xspace,multicol}
#+LATEX_HEADER: %%%%%%%%% Begin of Beamer Layout %%%%%%%%%%%%%
#+LATEX_HEADER: \ProcessOptionsBeamer
#+LATEX_HEADER: \usetheme[numbering=fraction,titleformat=smallcaps,progressbar=frametitle]{metropolis}
#+LATEX_HEADER: \usepackage{fontawesome}
#+LATEX_HEADER: \usecolortheme[named=BrickRed]{structure}
#+LATEX_HEADER: %%%%%%%%% End of Beamer Layout %%%%%%%%%%%%%
#+LATEX_HEADER: \usepackage{verbments}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{url} \urlstyle{sf}
#+LATEX_HEADER: \let\alert=\structure % to make sure the org * * works of tools
#+LATEX_HEADER: %\let\tmptableofcontents=\tableofcontents
#+LATEX_HEADER: %\def\tableofcontents{}
#+LATEX_HEADER:  \usepackage[normalem]{ulem}
#+LATEX_HEADER:  \usepackage{color,soul}
#+LATEX_HEADER:  \definecolor{lightorange}{rgb}{1,.9,.7}
#+LATEX_HEADER:  \sethlcolor{lightorange}
#+LATEX_HEADER:  \definecolor{lightgreen}{rgb}{.7,.9,.7}
#+LATEX_HEADER:  \let\hrefold=\href
#+LATEX_HEADER:  \renewcommand{\href}[2]{\hrefold{#1}{\SoulColor{lightorange}\hl{#2}}}
#+LATEX_HEADER: % \renewcommand{\uline}[1]{\SoulColor{lightorange}\hl{#1}}
#+LATEX_HEADER: \renewcommand{\emph}[1]{\SoulColor{lightorange}\hl{#1}}
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \newcommand\SoulColor[1]{%
#+LATEX_HEADER:   \sethlcolor{#1}%
#+LATEX_HEADER:   \let\set@color\beamerorig@set@color%
#+LATEX_HEADER:   \let\reset@color\beamerorig@reset@color}
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \let\oldtexttt=\texttt
#+LATEX_HEADER: % \renewcommand\texttt[1]{\SoulColor{lightgreen}\hl{\tt#1}}
#+LATEX_HEADER: % \renewcommand\alert[1]{\SoulColor{lightgreen}\hl{#1}}
#+LATEX_HEADER: % \AtBeginSection{\begin{frame}{Outline}\tableofcontents\end{frame}}
#+LATEX_HEADER: \graphicspath{{fig/}}
#+LATEX_HEADER: \usepackage{tikzsymbols}
#+LATEX_HEADER: \def\smiley{\Smiley[1][green!80!white]}
#+LATEX_HEADER: \def\frowny{\Sadey[1][red!80!white]}
#+LATEX_HEADER: \def\winkey{\Winkey[1][yellow]}

#+BEGIN_EXPORT latex
  \newcommand{\myfbox}[2][gray!20]{\bgroup\scalebox{.7}{\colorbox{#1}{{\vphantom{pS}#2}}}\egroup} % \fbox
  %\def\myfbox#1{#1} % \fbox
  \def\HPC{\myfbox[gray!40]{HPC}}
  \def\NET{\myfbox[gray!40]{Network}}
  \def\SG{\myfbox[gray!40]{Smart Grids}}
  \def\ECO{\myfbox[gray!40]{Economics}}
  \def\PRIV{\myfbox[gray!40]{Privacy}}
  \def\TRACING{\myfbox[red!20]{Tracing}}
  \def\SIM{\myfbox[green!20]{Simulation}}
  \def\VIZ{\myfbox[red!40]{Visualization}}
  \def\MODELING{\myfbox[green!40]{Stochastic Models}}
  \def\OPT{\myfbox[blue!20]{Optimization}}
  \def\GT{\myfbox[blue!40]{Game Theory}}
#+END_EXPORT


#+BEGIN_EXPORT latex
\def\changefont#1{%
  \setbeamertemplate{itemize/enumerate body begin}{#1}
  \setbeamertemplate{itemize/enumerate subbody begin}{#1}
  #1}
\makeatletter
\newcommand{\verbatimfont}[1]{\renewcommand{\verbatim@font}{\ttfamily#1}}
\makeatother
\verbatimfont{\scriptsize}%small
\let\endmintedbak=\endminted
\def\endminted{\endmintedbak\vspace{-1cm}}
#+END_EXPORT

#+BEGIN_EXPORT latex
\newcommand{\Norm}{\ensuremath{\mathcal{N}}\xspace}
\newcommand{\Unif}{\ensuremath{\mathcal{U}}\xspace}
\newcommand{\Triang}{\ensuremath{\mathcal{T}}\xspace}
\newcommand{\Exp}{\ensuremath{\mathcal{E}}\xspace}
\newcommand{\Bernouilli}{\ensuremath{\mathcal{B}}\xspace}
\newcommand{\Like}{\ensuremath{\mathcal{L}}\xspace}
\newcommand{\Model}{\ensuremath{\mathcal{M}}\xspace}
\newcommand{\E}{\ensuremath{\mathbb{E}}\xspace}
\def\T{\ensuremath{\theta}\xspace}
\def\Th{\ensuremath{\hat{\theta}}\xspace}
\def\Tt{\ensuremath{\tilde{\theta}}\xspace}
\def\Y{\ensuremath{y}\xspace}
\def\Yh{\ensuremath{\hat{y}}\xspace}
\def\Yt{\ensuremath{\tilde{y}}\xspace}
\let\epsilon=\varepsilon
\let\leq=\leqslant
\let\geq=\geqslant
#+END_EXPORT
:end:

# https://cran.r-project.org/web/packages/plot3D/vignettes/plot3D.pdf
# http://htmlpreview.github.io/?https://github.com/AckerDWM/gg3D/blob/master/gg3D-vignette.html

# http://bechtel.colorado.edu/~bracken/tutorials/stan/stan-tutorial.pdf
# http://jakewestfall.org/misc/SorensenEtAl.pdf
# https://github.com/AllenDowney/BayesMadeSimple

# https://github.com/bob-carpenter/prob-stats

#+BEGIN_EXPORT latex
#+END_EXPORT

** Introduction
With the current need for high performance computing, and the hardware
complexity:
- How to predict the duration of computations?
- How to quantify incertainty?

*** For this talk:
1. Brief presentation of the context
2. Introduction to Bayesian sampling through Stan
3. Examples of application
   
** Background on HPC and Polaris research
*** Modern context
  - HPC systems use thousands of nodes, cache, hyperthreading, etc $\rightarrow$ makes it difficult to predict performance
  - Some functions (like DGEMM in the BLAS library) are used
    everywhere, and called thousands of times in a program.

*** Previous work
  - Simulating high performance programs to optimize them at a
    lesser cost
  - Elaborated complex models within a few percent of reality but
    needed to evaluate and confirm them
** Bayes model
- Model :: Let's say $y \sim \Norm(\alpha*x+\beta,\sigma)$
  - *$\alpha,\beta,\sigma$*: Model *parameters*
  - *$y$*: Dependent *data* (posterior)
  - *$x$*: Independent data
  We observe some data and need to find model parameters

*** The vocabulary
  - *Posterior*: The distribution of the parameters
  - *Likelihood*: A function of the parameters, the model
  - *Prior*: Existing knowledge of the system, guesses on the parameters
    values (\sigma>0 per example)

* A Bayesian Sampler, Stan
** With a simple example
#+begin_src R :results output :session *R* :exports none
generate_dataset=function(intercept, coefficient, N, min_x=0, max_x=100, sigma=1){
    x = sample(min_x:max_x,N,replace=T) 
    y = coefficient * x + intercept + rnorm(N,sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}
df=generate_dataset(50, -2, 500, sigma=15)
#+end_src

[[file:./images/ex1_figure.png]]
Using this data, we'll try to find the parameters that were used to
generate it.

** The Stan model
#+begin_src R :results output :session *R* :exports none
library(rstan)

modelString = "data { // the observations
    int<lower=1> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real coefficient;
    real<lower=0> sigma; // indication: sigma cannot be negative
} 
model {
    // We define our priors
    intercept   ~ normal(0, 10); // We know that all the parameters follow a normal distribution
    coefficient ~ normal(0, 10);
    sigma       ~ normal(0, 10);

    // Then, our likelihood function
    y ~ normal(coefficient*x + intercept, sigma);
}
generated quantities {
   real x_pos = x[categorical_rng(rep_vector(1,N) / N)];
   real y_pos; // posterior predictions
   y_pos = normal_rng(coefficient*x_pos+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
library(rstan)

modelString = "data { // the observations
    int<lower=1> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real beta;
    real alpha;
    real<lower=0> sigma; // indication: sigma cannot be negative
} 
model {
    // We define our priors
    beta   ~ normal(0, 10);
    alpha ~ normal(0, 10);
    sigma       ~ normal(0, 10);

    // Then, our likelihood function
    y ~ normal(alpha*x + beta, sigma);
}
"
sm = stan_model(model_code = modelString)
#+end_src

** Looking at the posterior

[[file:./images/ex1_pairs.png]]

** Looking at the generated data

[[file:./images/ex1_generated_data.png]]

** The importance of the priors
- The priors are necessary to have convergence in the fit
- Non-informative prior vs informative (careful not to have a falsely
  informative one and introduce bias)
- A little bit of precision is better, but initialisation values can
  do the trick
* The different models for dgemm
** Spatial and temporal variability
- DGEMM's duration depends on the matrix size, on the CPU
  used to run it, and on residual noise coming from the system.
[[file:./images/duration_on_mnk_paper.png]]
** The possible models
(Source: Fast and Faithful Performance Prediction of MPI Applications:
the HPL Case Study)
[[file:./images/models_paper.png]].

** Model 1 : A polynomial model with noise depending on x 
A redo of the last model presented before, using Stan.
Like a linear model but with more parameters (in this case 10).

The model follows this: 

$duration \sim \Norm(\alpha_1*mnk+\alpha_2*mn+\alpha_3*mk+\alpha_4*nk+\beta , \\
\gamma_1*mnk+\gamma_2*mn+\gamma_3*mk+\gamma_4*nk+\delta)$

** The generated data 
[[file:./images/generated_quantities_dgemm_m-2_second_test.png]]
** Model 2 : A model with parameters depending on the host
- Much like the previous model, but with different observations for
  each host
- Added a variable for the number of hosts, and used matrices instead
  of vectors for all the parameters.

For this model we have:

$duration[i] \sim \Norm(\alpha_1[i]*mnk+\alpha_2[i]*mn+\alpha_3[i]*mk+\alpha_4[i]*nk+\beta[i] , \\
 \gamma_1[i]*mnk+\gamma_2[i]*mn+\gamma_3[i]*mk+\gamma_4[i]*nk+\delta[i])$

** Posterior visualisation
The posterior with models depending on the host shows a lot of
difference between hosts (here we have 3 "average" CPU and a slow one):

[[file:./images/intercept_on_mu_posterior.png]]
** Posterior visualisation

If we look at the means of the parameters' values for each host, we
get a range of values in which most hosts are.

[[file:./images/mu_on_intercept_means.png]]

** Model 3 : A hierarchical linear model
- Useful to find the value of hyperparameters from which we get the parameters
- From this we could calculate new parameters for new CPUs
- Here \mu_\alpha and \sigma_\alpha are the hyperparameters for \alpha, and
  the same goes for the other parameters

$\mu_\alpha \sim \Norm$(\alpha_moy,\alpha_sd) with \alpha_moy and \alpha_sd the priors

$\sigma_\alpha \sim \Norm(0,1)$

$\alpha[i] \sim \Norm(\mu_\alpha, \sigma_\alpha)$

$duration[i] \sim \Norm(\alpha[i]*mnk + \beta[i], \gamma[i]*mnk + \delta[i])$

** Conclusion
*** My contribution
- Created several models to represent the performance of a computation
  kernel within a few percent of reality
- Model adaptable to changes (addition/removal of CPUs)
*** Following up work
  - Implementing this work in Simgrid research (other computation
    kernels, network communications, etc)
  - Novelty detection and non regression performance tests
