# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages logdrawer
#+TITLE:       Journal
#+AUTHOR:      Hoël Jalmin
#+LANGUAGE:    en
#+TAGS: [ PROGRAMMING : R(r) STAN(S) ]
#+TAGS: [ TOOLS : ORGMODE(o) GIT(g)  ]
#+TAGS: PAPER(P) VIDEO(V) MEETING(m) SEMINAR(s)
#+SEQ_TODO: TODO(t!) STARTED(s!) INTERRUPTED(i!) DONE(d!) 
#+SEQ_TODO: REWORK(r!)
#+SEQ_TODO: EDIT(e!)

* Introduction
This file is the report for my internship, about automating
calculation kernels' modellings. I'll explain more about this later on.

* 2019
** 2019-04 april
*** 2019-04-30 monday
**** DONE Read [[https://hal.inria.fr/hal-02096571/document ][Faithful and Efficient Simulation of High Performance Linpack]] :PAPER:
:LOGBOOK:  
- State "DONE"       from "STARTED"    [2019-05-02 thursday 16:53]
:END:      
***** Introduction
With a power consumption of several MW per hour on a TOP500 machine,
running applications on supercomputers at scale solely to optimize
their performance is extremely expensive. Likewise, High-Performance
Linpack (HPL), the benchmark used to ran supercomputers in the TOP500,
requires a careful tuning of many parameters (problem size, grid
arrangement, granularity, collective operation algorithms, etc.) and
supports exploration of the most common and fundamental performance
issues and their solutions. In this article, we explain how we both
extended the SimGrid's SMPI simulator and slightly modified the
open source version of HPL to allow a fast emulation on a single
commodity server at the scale of a supercomputer. We explain how to
model the different components (network, BLAS, ...) and show that a
careful modeling of both spatial and temporal node variability allows us
to obtain predictions within a few percents of real experiments.

***** Notes 
The paper explains how to extend the Simgrid simulator to simulate the
HPL benchmark and get the rough performance of it, as it is quite
representative of usual performance issues, without actually running
it. The simulation reduces considerably memory consumption compared to
a normal run of HPL, so it can be ran on a smaller cluster with less
time. Other simulation models for HPL already existed, but were quite
inaccurate as they did not take all the parameters in account.

****** About HPL
HPL is a parallel implementation of a benchmark which measures how
fast a computer can solve a system of linear equations. HPL uses the
BLAS libraries to make matrix operations such as multiplication (using
dgemm). The sequential complexity depends on the matrix size: 
2/3N³ + 2N² + O(N), but the calculation time cannot be determined
without running HPL or a simulation of it (it depends on network
capacity and a lot of parameters). Even while running HPL there's a
difference between the theoretical peak performance and the actual one
reached depending on how MPI communications are done.


****** About SimGrid and MPI simulation
There are two approaches to MPI simulation: offline and online. The
first one gives a previously obtained trace of the application to a
simulator, which makes predictions depending on the performance
models. The issue is that a first run is necessary to get the trace,
which is only valid at the moment of the run, since it depicts a
precise behavior. Thus, predictions are extrapolated which can be an
issue if the execution is non deterministic. This simulation type is
not proper for HPL, whereas online simulation (used by SimGrid) means
the simulator directs the execution; it decides which process to run
at which time. SimGrid provides accurate performance models for
application with heavy network use, as it considers network topology
and heterogeneity. SMPI, based on SimGrid, runs MPI ranks on mutually
exclusive threads, so whenever a thread enters an MPI call, SMPI moves
its clock ahead of the time spent computing since the last MPI call.


****** DONE About how to emulate HPL
:LOGBOOK:  
- State "DONE"       from "STARTED"    [2019-05-02 thursday 10:51]
- State "STARTED"    from "TODO"       [2019-05-02 thursday 09:39]
:END:      
Most of the calculation time of HPL is spent in the dgemm and dtrsm
(A.x = b equations) kernels, so replacing these two by models greatly
reduces simulation time: it was only be necessary to make a SMPI call
when one of these kernels were used (this makes results incorrect). 
Likewise, other functions were replaced since they used incorrect data
because of dgemm and dstrsm. Initialization of pseudo-random matrices
and correctness verification were also skipped, changing the idamax
function to return random values, making the simulation deterministic.

Memory consumption was scaled down by sharing the input matrix A
between all MPI processes and indicating that its data can be
overwritten. For the panel matrix, containing matrix indexes, data
corruption cannot be risked so a partial shared malloc was introduced,
to only share the range of values that doesn't have matrix indexes. 
The number of allocations and page faults was also greatly reduced by
reusing the allocated panels instead of getting more of them. Also, a
lot of calls to memcopy were avoided by making SMPI aware of which
memory areas were private or not. Calls to mmap (used to remap the
data segment to the private copy of the MPI rank when context
switching) were also avoided by loading several times the data segment
into memory. Finally, huge pages were used so the page table wouldn't
be too large.

All of these changes allowed a reduced complexity by removing the
O(N³) part.


****** DONE About the chosen models for kernels and communications
:LOGBOOK:  
- State "DONE"       from "STARTED"    [2019-05-02 thursday 16:05]
- State "STARTED"    from "TODO"       [2019-05-02 thursday 10:51]
:END:      
Several modeling notations were chosen. M-0 means the duration is
roughly constant and independant of the input parameters. M-1 means
the performance depends on a combination of the parameters. M-2
is used when a polynomial model is required (for complex behavior). 
M_H means the platform has spatial variability and modeling should
be done for each host, and M' is used when the duration is linear for
specific parameters values. The same was done for the noise: N-0 means
no noise, N-1 means the noise has a normal distribution, N-2 means the
noise has to be modelised by a polynomial function. N_H means noise
estimations are made per-host and N' is when the noise is modelised by
several normal distributions.

Communications between MPI nodes are mostly linear in message size but
vary depending on the protocol used. The chosen model was a M'-1
(linear within each host but discontinuous) N'-1 (complex distribution
of linear noise) model, with its paramaters estimated by pytree: the
message size range and the 2-4 modes of the normal distribution mixture.

For dgemm, a polynomial model (M_H-2) was required because of the spatial
variability: depending on the value of M*N*K (so on the matrix size),
some durations will be higher than others regardless of the node used
(which means dgemm doesn't have a linear behavior). There is also some
temporal variability, modelized here by a random call. For other BLAS
and HPL kernels, a linear model M-1 is close enough to reality; but the
noise needs to follow a N-2 model because the variability it provides
increases with the value of the parameters (which indicates a
polynomial model).


****** DONE The simulation at scale
:LOGBOOK:  
- State "DONE"       from "STARTED"    [2019-05-02 thursday 16:53]
- State "STARTED"    from "TODO"       [2019-05-02 thursday 16:20]
:END:      
After working on these optimizations, an emulation was done at scale
using the Dahu cluster, with a high number of iterations, complex
communication patterns and more MPI processes than usual. The
emulation was stopped after five iterations, to compare to real runs:
the communication durations were a bit too optimistic, and it was
noted that using a complex model makes more realistic traces. Several
simulations were then done to figure out which model would be more
accurate, and the models that are the closest to reality are: M-1 N-2
for the kernels, M_H-2 N_H-2 for dgemm and M'-1 N-0 for the network. 
Adding a linear noise for the network doesn't have any visible effect.

This was compared to the run made on the Stampede cluster. Considering
the input parameters for this run and the result, an optimistic model
(M'-1 N-0) was chosen for the dgemm and dtrsm functions, as well as
for the MPI communications, while ignoring the other functions. 
Although usually the simulations are within of few percent of reality,
the performance of this one was much lower than the performance of the
Stamepde run, which used a modified version of HPL and different
parameters than the ones printed by HPL. It was also found that the
communications had been optimized for the run, which explains the
difference between the simulation and the reality.

*** 2019-04-30 tuesday
**** Installed Emacs and Org-mode and all required dependencies. :ORGMODE:
Followed the given MOOC to understand how Org-mode functions, and how
to take efficient notes. Learned the basics of org-mode, and of the
keyboard shortcuts introduced by the provided emacs initialization.

**** Began writing the journal
** 2019-05 may
*** 2019-05-02 thursday
**** Attended the keynote speech about contrasting artifical and human intelligence by Jean-Louis Dessales :SEMINAR:
***** Introduction
Some artificial intelligence techniques were recently able to scale
up, provoking what many consider as a technical revolution. However,
the type of AI that proved so successful in the past decade relies on
the exploitation of massive data, and is limited to narrow domains of
expertise. By contrast, human intelligence is very efficient at making
broad inferences from limited evidence. I will highlight a few
qualitative differences between artificial intelligence and human
intelligence. These differences are mainly due to a small set of
cognitive operations, such as contrast or simplicity detection, that
human beings perform on the fly. I will also suggest that attempting
to bridge the gap between these two forms of intelligence might be the
best way to improve artificial systems in the future.

***** Notes
- It is often said that artificial intelligence will eventually replace
 mankind, but Jean-Louis Dessales clearly doesn't think so. It is
 foolish for him to believe there is a loss function that can be
 applied for everything.

- There are specific characteristics of artifical intelligence that
  makes them too different from human beings to be able to solve every
  problem humans are able to.
 
  + For instance, neuronal networks work best when they can analyse a
    lot of data, to then be able to recognize it. This can work for
    fields such as image recognition, but not for particular fields
    such as criminal investigation where every case is different and
    the possible similarity between cases is only an average and
    cannot be exact. Also artificial intelligence work without biases,
    unlike human beings.

  + Artificial intelligence function in an isotropic way, they can
    learn how to recognize language even if the order of the words in
    the sentences is mixed up; whereas this will just confuse human
    beings. However, even though artificial intelligences recognize
    language they do not understand it, and if they try to speak they
    often make no sense because they look at semantics similarity and
    not semantics itself. Any idoms are lost to them, because they
    will not recognize it.

  + Neuronal networks are also unable to recognize a pattern if they
    were not introduced to it, and have very narrow expertise. For
    example, if given a sequence of numbers, they will be unable to
    find the next ones (ex: 1223334444). As they work with recognition
    instead of understanding, trying to broaden their expertise will
    sometimes fail. For example a neuronal network made to recognize
    skin cancer cannot recognize psoriasis, and trying to change it
    will reduce their ability to see skin cancer.

- When attempting to solve problems, human beings look for simplicity:
  the least complexity is the best. How humans see complexity and
  unexpectedness is by a difference between what they expect and what
  they see.

  + Unexpectedness can be defined as a complexity drop between what was
    expected and what happened. For example, seeing a "simple" (well
    known) person in a "complex" (rarely visited) place is unexpected,
    because of the difference in complexity.

- Because of the way humans learn by making connections (for example
  when learning a new word the context is immediately connected with
  it; if a child hears their parent talk about "chopping the fish"
  they are able to connect it to the place they heard it, to the food
  associated with it...), artificial intelligence cannot replace
  humans in the sense that this process of learning is completely
  foreign to them. They do not process the relations between objects,
  they focus on pattern recognition.
*** 2019-05-03 friday
**** STARTED Read [[http://xcelab.net/rm/statistical-rethinking ][A Bayesian Course with examples in R and Stan]] by Richard McElreath :PAPER:STAN:
:LOGBOOK:  
- State "STARTED"    from "INTERRUPTED" [2019-05-09 jeu. 16:42]
- State "INTERRUPTED" from "STARTED"    [2019-05-06 lun. 18:52]
:END:      
***** About models
- Models are like golems, powerful but easy to misuse.
- Use of models is wide spread and necessary but giving too many
  models is confusing, you need to find the right one. statistical
  tools are not diverse enough.
- Falsifying null hypothesis isn't enough. it would be more logical to
  falsify an explanatory model because falsifying a null model bring
  very little knowledge.
- Hypotheses correspond to several process models, and statistical
  models correspond to several process models
- Bayesian statistics are able to process small samples and don't need
  a prior hypothesis to confirm or reject. In bayesian statistics,
  probabilities are interpreted as uncertainty, models' parameters are
  modelled by probability laws and parameters are approximated by execution.
- The priors are observations/deductions that we know before the
  analysis and that helps guide the outcome.

***** About priors, likelihood and posteriors
- Small worlds are the logical worlds of the models. There are no
  surprises, and it's possible to verify the models's logic and if it
  works properly. Whereas in large worlds there might be unpredicted
  events, and the models never completely encompass large worlds.
- Bayesian analysis are garden of forking data: multiple paths exist,
  with each one branching more possibilities as we explore the
  paths. By getting more observations, we prune some path so to keep
  only the ones consistant to our data.
- Prior information can help us find the plausability of each path.
- The plausability of x knowing y is proportional to the number of
  ways x can produce y * the prior plausability of x.
- Models are built on the way, updated each time we get data that
  confirms or denies it's prior assumptions. They learn with each set
  of plausabilities.
- A model is a mix of a likelihood, which represents the plausability
  of the data given a fixed value as parameter, several parameters and
  a prior (the plausability of each value of the parameters).
- Bayes theorem: P(A∩B) = P(A|B)Pr(B) = P(B|A)Pr(A) 
  aka Posterior = Likelihood*Prior / Average Likelihood
  Pr(A|B) = Pr(B|A)*Pr(A)/(Pr(B|A)*Pr(A))+(Pr(B|A¯)*Pr(A¯))
- Grid approximation uses a finite grid of parameters values and
  scales poorly. When we have several parameters, we use a quadratic
  approximation (to describe the normal shape of the posterior).

***** About samples
- The false positive test: It's a counterintuitive result to a simple
  test. Knowing the amount of people afflicted in the general
  population, the probability for the test to correctly detect the
  illness, and the probability for it to have a false positive result,
  what is the probability, given that the test is positive, that the
  test subject has the illness? It's often lower than expected.
- Working on samples of parameters' values is easier and more
  intuitive, especially when models become complicated.
- Sampling is useful to summarize and understand the posterior
  distribution
- Sampling can also help simulate the model, to check it, or to
  simulate new predictions.

***** About linear and polynomial models
- Normal distributions are extremely used because they are common in
  nature and easy to calculate with.
- Two perspectives: ontological and epistemological. The first sees
  fluctuations, that when summed up make a symmetrical Gaussian
  distribution. For the second, we only know the mean and variance and
  considers Gaussian distribution because it is the most common and
  least surprising (maximu entropy).
- To build a model, we need to know the outcomes, the likelihood, the
  predictors and how they relate to the likelihood and the priors.
- Priors need to be at least a little precise, otherwise they bring
  nothing and the posterior will only depend on the data. Bad priors
  are relatively harmless, only useless.
- Polynomial regression is not advised

***** About multivariate linear models
- Multivariate linear models are much like linear models but with
  several variables. For example, the likelihood y follows a normal
  distribution based on alpha + beta_r x + beta_s z with beta_r and beta_s
  coefficients that measure the association between x and y.
- Sometimes, the association between two variables is quite hidden,
  and so a regression with both is needed to see it. In this case,
  both need to be considered or their relationship with the posterior
  will be hidden.
- Sometimes, adding variables is neither useful nor desirable: it may
  falsify the result of the regression because two predictor variables
  will be highly corrolated (multicollinearity). And including
  post-treatment variables will hide the relationship between the
  outcome and the first variable if there are unobserved confounders
  between the outcome and the post-treatment variable. 

***** About overfitting and regularization
**** STARTED [[https://www.youtube.com/watch?v=BWEtS3HuU5A&list=PLDcUM9US4XdM9_N6XUUFrhghGJ4K25bFc&index=10 ][Listened to several lectures by Richard McElreath]] :VIDEO:STAN:
:LOGBOOK:  
- State "STARTED"    from "INTERRUPTED" [2019-05-09 jeu. 17:08]
- State "INTERRUPTED" from "STARTED"    [2019-05-06 lun. 18:52]
:END:      
These lectures are available on his youtube channel, and explain in
more detail what his book is about.

*** 2019-05-06 monday
**** Continued to read the book, notes on May 3rd section
**** Talked to Tom about a simple Stan example and notebooks
**** Started to use Stan with simple cases, following Tom's example :STAN:R:

We start by generating some data following a relatively straight line,
with some noise.

#+begin_src R :results output :session *R* :exports both
generate_dataset=function(intercept, coefficient, N, min_x=0, max_x=100, sigma=1){
    x = sample(min_x:max_x,N,replace=T) 
    y = coefficient * x + intercept + rnorm(N,sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}
df=generate_dataset(50, -2, 500, sigma=15)
head(df)
#+end_src

#+RESULTS:
:  
:    x          y
: 1 53 -50.228060
: 2 60 -66.521457
: 3 26   4.949108
: 4 18   2.624106
: 5 15  24.501256
: 6 24  -9.271574

#+begin_src R :results output graphics :file ./images/ex1_figure.png :exports both :width 600 :height 400 :session *R* 
library(ggplot2)
ggplot(df, aes(x=x, y=y))+geom_point(alpha=0.3)
#+end_src

#+RESULTS:
[[file:./images/figure.png]]

Then, we define a stan model to find our parameters (intercept,
coefficient and sigma) that we now assume unknown 

#+begin_src R :results output :session *R* :exports both
library(rstan)

modelString = "data { // the observations
    int<lower=1> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real coefficient;
    real<lower=0> sigma; // indication: sigma cannot be negative
} 
model {
    // We define our priors
    intercept   ~ normal(0, 10); // We know that all the parameters follow a normal distribution
    coefficient ~ normal(0, 10);
    sigma       ~ normal(0, 10);

    // Then, our likelihood function
    y ~ normal(coefficient*x + intercept, sigma);
}
"
sm = stan_model(model_code = modelString)

#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: ea4b5a288cf5f1d87215860103a9026e.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                mean se_mean   sd     2.5%      25%      50%      75%    97.5%
intercept      49.74    0.05 1.36    47.04    48.85    49.74    50.63    52.53
coefficient    -2.00    0.00 0.02    -2.04    -2.01    -1.99    -1.98    -1.95
sigma          15.19    0.01 0.47    14.31    14.88    15.18    15.49    16.13
lp__        -1621.74    0.04 1.18 -1624.81 -1622.26 -1621.43 -1620.86 -1620.41
            n_eff Rhat
intercept     896 1.01
coefficient   953 1.01
sigma        1134 1.01
lp__          763 1.00

Samples were drawn using NUTS(diag_e) at Tue May  7 10:50:03 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

As we can see, Stan can find back the parameters given a close enough
model.

#+begin_src R :results output graphics :file ./images/ex1_stan_plot.png :exports both :width 600 :height 400 :session *R* 
stan_plot(fit)
#+end_src

#+RESULTS:
[[file:./images/stan_plot.png]]

#+begin_src R :results output graphics :file ./images/ex1_stan_trace.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)
#+end_src

#+RESULTS:
[[file:./images/stan_trace.png]]

#+begin_src R :results output graphics :file ./images/ex1_stan_hist.png :exports both :width 600 :height 400 :session *R* 
stan_hist(fit)
#+end_src

#+RESULTS:
[[file:./images/stan_hist.png]]

#+begin_src R :results output graphics :file ./images/ex1_stan_dens.png :exports both :width 600 :height 400 :session *R* 
stan_dens(fit) #attention à la densité, elle n'est pas toujours pertinente
#+end_src

#+RESULTS:
[[file:./images/stan_dens.png]]

*** 2019-05-07 tuesday
**** Search for a way to generate data once Stan has found the parameters :STAN:
- Looked at different packages such as rstanarm but they are too restricted
- Looked at Stan's program blocks
  + The data block lists the data we'll give to Stan. It's possible to
    use restrictions between < > (the data won't be negative...). 
    Within a block, anything declared, can then be used subsequently.
  + The transformed data block is used to create new data based on the
    input data. It has no particular use for this case.
  + The parameters block indicates the parameters that will be estimated
    by Stan.
  + The transformed parameters block includes optional parameters that
    are dependent on the previous parameters.
  + The model block specifies the priors and likelihood
  + Finally, the generated quantities block calculates any data based
    on the model's results, so it might be useful in this case.
**** Tried to use Stan with more complex examples :STAN:R:
***** With noise depending on x

We generate data, this time with noise depending on x. This time,
we're careful not to have any x=0 since that would produce bogus data
for sigma.

#+begin_src R :results output :session *R* :exports both
generate_dataset=function(intercept, coefficient, N, min_x=1, max_x=100, sigma=1){
    x = sample(min_x:max_x,N,replace=T) 
    y = coefficient * x + intercept + rnorm(N,sd=sigma*x)
    df = data.frame(x=x,y=y)
    return(df)
}
df=generate_dataset(12, 50, 500, sigma=15)
head(df)
#+end_src

#+RESULTS:
:  
:    x        y
: 1 17 1200.497
: 2 52 2663.708
: 3 85 4234.701
: 4 42 2494.610
: 5 68 4240.304
: 6 86 2746.118

#+begin_src R :results output graphics :file ./images/ex2_figure.png :exports both :width 600 :height 400 :session *R* 
library(ggplot2)
ggplot(df, aes(x=x, y=y))+geom_point(alpha=0.3)
#+end_src

#+RESULTS:
[[file:./images/ex2_figure.png]]

Then, we define a stan model to find our parameters

#+begin_src R :results output :session *R* :exports both
library(rstan)

modelString = "data { // the observations
    int<lower=1> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real coefficient;
    real<lower=0> sigma; // indication: sigma cannot be negative
} 
model {
    // We define our priors
    intercept   ~ normal(0, 10); // We know that all the parameters follow a normal distribution
    coefficient ~ normal(0, 10);
    sigma       ~ normal(0, 10);

    // Then, our likelihood function
    y ~ normal(coefficient*x + intercept, sigma*x);
}
"
sm = stan_model(model_code = modelString)

#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example

Inference for Stan model: 12d920cec7fd90b16640e34f6f5a767a.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                mean se_mean   sd     2.5%      25%      50%      75%    97.5%
intercept       9.42    0.13 4.87    -0.17     6.16     9.28    12.77    19.16
coefficient    50.18    0.02 0.68    48.86    49.73    50.20    50.65    51.48
sigma          14.42    0.01 0.45    13.60    14.10    14.41    14.71    15.32
lp__        -3438.43    0.04 1.20 -3441.45 -3438.98 -3438.13 -3437.57 -3437.08
            n_eff Rhat
intercept    1364 1.00
coefficient  1629 1.00
sigma        1901 1.00
lp__          933 1.01

Samples were drawn using NUTS(diag_e) at Tue May  7 11:03:57 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

This time, the intercept is farther from its actual value, because of
a high standard deviation in its estimation (4.87). This high
deviation is seen in the plot as a red line.

#+begin_src R :results output graphics :file ./images/ex2_plot.png :exports both :width 600 :height 400 :session *R* 
stan_plot(fit)
#+end_src

#+RESULTS:
[[file:./images/ex2_plot.png]]

#+begin_src R :results output graphics :file ./images/ex2_trace.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)
#+end_src

#+RESULTS:
[[file:./images/ex2_trace.png]]

#+begin_src R :results output graphics :file ./images/ex2_hist.png :exports both :width 600 :height 400 :session *R* 
stan_hist(fit)
#+end_src

#+RESULTS:
[[file:./images/ex2_hist.png]]
*** 2019-05-09 thursday
**** Continued to read the book, notes on May 3rd section
**** Continued watching the MOOC, particularly the first and fourth modules :ORGMODE:
**** DONE An example with generated quantities ([[Generated Quantities][Reworked in friday's entry]]) :STAN:R:
:LOGBOOK:  
- State "DONE"       from "STARTED" [2019-05-09 jeu. 13:51]
- State "STARTED"    from "TODO"       [2019-05-09 jeu. 09:42]
:END:      
As usual we start by generating data. We'll take a simple example.

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(rstan)
generate_dataset=function(intercept, coefficient, N, min_x=0, max_x=100, sigma=1){
    x = sample(min_x:max_x,N,replace=T) 
    y = coefficient * x + intercept + rnorm(N,sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}
df=generate_dataset(12, 3, 500, sigma=15)
head(df)
#+end_src

#+RESULTS:
#+begin_example
   x         y
1 50 162.69930
2 46 160.77595
3 16  60.37135
4 24 100.82896
5  7  35.89649
6 62 198.99881
#+end_example

#+begin_src R :results output graphics :file ./images/ex3_figure.png :exports both :width 600 :height 400 :session *R* 
library(ggplot2)
ggplot(df, aes(x=x, y=y))+geom_point(alpha=0.3)
#+end_src

#+RESULTS:
[[file:./images/ex3_figure.png]]

Then, we define a stan model to find our parameters, and we generate
some y after making the sample.

#+begin_src R :results output :session *R* :exports both
library(rstan)

modelString = "data { // the observations
    int<lower=1> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real coefficient;
    real<lower=0> sigma; // indication: sigma cannot be negative
} 
model {
    // We define our priors
    intercept   ~ normal(0, 10); // We know that all the parameters follow a normal distribution
    coefficient ~ normal(0, 10);
    sigma       ~ normal(0, 10);

    // Then, our likelihood function
    y ~ normal(coefficient*x + intercept, sigma);
}
generated quantities {
  vector[N] y_pos; // posterior predictions
  for (n in 1:N)
    y_pos[n] = normal_rng(coefficient*x[n]+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)

#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
#+end_src

#+begin_src R :results output graphics :file ./images/ex3_plot.png :exports both :width 600 :height 400 :session *R* 
stan_plot(fit, pars = c("intercept", "coefficient", "sigma"))
#+end_src

#+RESULTS:
[[file:./images/ex3_plot.png]]

#+begin_src R :results output graphics :file ./images/ex3_trace.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit, pars = c("intercept", "coefficient", "sigma"))
#+end_src

#+RESULTS:
[[file:./images/ex3_trace.png]]

#+begin_src R :results output graphics :file ./images/ex3_hist.png :exports both :width 600 :height 400 :session *R* 
stan_hist(fit, pars = c("intercept", "coefficient", "sigma"))
#+end_src

#+RESULTS:
[[file:./images/ex3_hist.png]]

Now we look at the generated y

#+begin_src R :results output :session *R* :exports both
extracted=rstan::extract(fit)
mean(extracted$y_pos)
mean(data$y)
#+end_src

#+RESULTS:
: 
: [1] 162.9266
: 
: [1] 162.9518

From this mean, we can say the generated y are reasonably close enough
to the data we generated at the beginning, but not exactly similar of
course, since we have an approximation of the parameters (intercept,
coefficient and sigma). We'll compare the two on a same plot.

#+begin_src R :results output graphics :file ./images/ex3_comparative_plot.png :exports both :width 600 :height 400 :session *R* 
library(matrixStats)
median=colMedians(extracted$y_pos, na.rm = TRUE)
mean=colMeans(extracted$y_pos, na.rm = FALSE)
data2=data.frame(x=data$x,y_pos=mean, y_prev=data$y, y_med=median)

ggplot() + geom_point(data=data2, aes(x=x, y=y_prev), alpha=0.3, color="darkgreen")+ geom_point(data = data2,aes(x = x,y = y_pos), alpha=0.5, color="darkblue") + geom_point(data = data2,aes(x = x,y = y_med), alpha=0.1, color="darkred")
#+end_src

#+RESULTS:
[[file:./images/ex3_comparative_plot.png]]

Surprisingly, the y generated by Stan are very linear compared to the
ones we generated in the beginning. It looks as if the noise wasn't
taken in account. It's probably a result of the colMeans. We looked at
colMedian, to see if the result would be different but it seems it
gives almost the same points.
*** 2019-05-10 friday
**** REWORK <<Generated Quantities>>  :STAN:R:
Following Tom's advice, we changed the model for the generated
quantities test.

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(rstan)
generate_dataset=function(intercept, coefficient, N, min_x=0, max_x=100, sigma=1){
    x = sample(min_x:max_x,N,replace=T) 
    y = coefficient * x + intercept + rnorm(N,sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}
df=generate_dataset(12, 3, 500, sigma=15)
#+end_src

#+begin_src R :results output :session *R* :exports both
library(rstan)

modelString = "data { // the observations
    int<lower=1> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real coefficient;
    real<lower=0> sigma; // indication: sigma cannot be negative
} 
model {
    // We define our priors
    intercept   ~ normal(0, 10); // We know that all the parameters follow a normal distribution
    coefficient ~ normal(0, 10);
    sigma       ~ normal(0, 10);

    // Then, our likelihood function
    y ~ normal(coefficient*x + intercept, sigma);
}
generated quantities {
   real x_pos = x[categorical_rng(rep_vector(1,N) / N)];
   real y_pos; // posterior predictions
   y_pos = normal_rng(coefficient*x_pos+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)

#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
                mean se_mean    sd     2.5%      25%      50%      75%    97.5%
intercept      11.84    0.04  1.30     9.35    10.93    11.85    12.72    14.35
coefficient     2.98    0.00  0.02     2.94     2.97     2.98     3.00     3.03
sigma          14.76    0.01  0.49    13.83    14.42    14.75    15.08    15.74
x_pos          50.89    0.67 29.37     3.00    25.00    51.00    77.00    98.00
y_pos         164.20    2.02 89.09    15.22    87.91   165.14   242.90   308.71
lp__        -1595.10    0.05  1.25 -1598.47 -1595.65 -1594.77 -1594.20 -1593.70
            n_eff Rhat
intercept     831 1.00
coefficient   871 1.00
sigma        1399 1.00
x_pos        1897 1.00
y_pos        1940 1.00
lp__          636 1.01

Samples were drawn using NUTS(diag_e) at Fri May 10 14:05:37 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

Now we look at the generated y

#+begin_src R :results output graphics :file ./images/ex3_reworked_comparative_plot.png :exports both :width 600 :height 400 :session *R* 
extracted=rstan::extract(fit)
df_generated = data.frame(x=extracted$x_pos, y=extracted$y_pos, origin='generated')
df_generated = df_generated[sample(nrow(df_generated), nrow(df)), ]  # there are more points in df_generated, so to have a nicer plot we sample exactly the same number than in df
df$origin = 'initial'
tmp = rbind(df, df_generated)
ggplot(tmp, aes(x=x, y=y, color=origin)) + geom_point(alpha=0.5)
#+end_src

#+RESULTS:
[[file:./images/ex3_reworked_comparative_plot.png]]

This way we don't have to make a mean that erases the noise's effect,
and the generated y could definitely be used for further calculation. 

**** Tried to use Stan with more complex examples :STAN:R:
***** With a full polynomial model ([[Polynomial Model][Reworked in tuesday's entry]]):
We'll assume for this one that the noise follows a simple model, to
make it easier. We generate data, this time with three
parameters. We'll call them second_ degree, first_ degree and intercept.

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(rstan)

generate_dataset=function(intercept, second_degree, first_degree, N, min_x=0, max_x=100, sigma){
    x = sample(min_x:max_x,N,replace=T) 
    y = second_degree * x^2 + first_degree * x + intercept + rnorm(N, sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}

df=generate_dataset(100,2,6, 500, sigma=150)
head(df)
#+end_src

#+RESULTS:
#+begin_example
   x          y
1  9   419.2124
2 90 16933.1164
3 93 18001.0911
4 89 16522.1687
5 20  1013.0832
6  9   534.0954
#+end_example

#+begin_src R :results output graphics :file ./images/ex4_figure.png :exports both :width 600 :height 400 :session *R* 
ggplot(df, aes(x=x, y=y))+geom_point(alpha=0.3, color="darkblue")
#+end_src

#+RESULTS:
[[file:./images/ex4_figure.png]]

#+begin_src R :results output :session *R* :exports both
modelString = "data { // the observations
    int<lower=0> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real second_degree;
    real first_degree;
    real<lower=0> sigma; // indication: sigma cannot be negative
}

model {
     // We define our priors
    second_degree ~ normal(0, 10);
    first_degree ~ normal(0, 10);
    intercept   ~ normal(0, 10);
    sigma       ~ normal(0, 10);
    // Then, our likelihood function
    for (n in 1:(N)){
      y ~ normal(second_degree*(x[n]*x[n]) + first_degree*x[n] + intercept, sigma);
    }
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Warning messages:
1: There were 155 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
2: There were 8 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
3: Examine the pairs() plot to diagnose sampling problems

Inference for Stan model: e6c4a717223c83fc91c9c4018606219b.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                     mean se_mean      sd        2.5%         25%         50%
intercept          216.01  170.46  347.33        1.02        5.15       16.60
second_degree       -2.18    0.06    0.11       -2.26       -2.24       -2.24
first_degree       262.91    6.67   13.61      237.07      264.29      270.41
sigma             4715.81   10.28   21.48     4672.10     4713.75     4725.89
lp__          -2463195.40  827.57 1668.03 -2464315.63 -2464275.36 -2464166.68
                      75%       97.5% n_eff  Rhat
intercept          184.50      864.26     4  6.60
second_degree       -2.19       -1.96     4  5.75
first_degree       271.17      272.17     4  6.17
sigma             4729.56     4735.32     4  3.92
lp__          -2462778.64 -2460240.49     4 12.27

Samples were drawn using NUTS(diag_e) at Fri May 10 12:34:16 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

The sampling issued several warnings, which isn't surprising because
the parameters found by Stan don't match at all with the ones we chose
to generate the data.

#+begin_src R :results output graphics :file ./images/ex4_trace.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit, pars = c("intercept", "second_degree", "first_degree", "sigma"))+ scale_color_manual(values = blues9)
#+end_src

#+RESULTS:
[[file:./images/ex4_trace.png]]

If we look at the trace it becomes clear that our model must have been
wrong, because the chains don't converge towards each other at
all. It's apparent polynomial regression cannot be done with a
model based on linear regression and only slightly changed.

Stan's warnings advises us to look at the pairs plot.

#+begin_src R :results output graphics :file ./images/ex4_pairs.png :exports both :width 600 :height 400 :session *R* 
pairs(fit)
#+end_src

#+RESULTS:
[[file:./images/ex4_pairs.png]]

Apparently, there should be a negative relationship between lp__ and
energy__, however this plot is quite difficult to understand without
indication.

***** EDIT 2019-05-14 Changed the model following Tom's advice

Tom figured out a mistake in the declaration of the likelihood
function.

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,2,6, 500, sigma=150)

modelString = "data { // the observations
    int<lower=0> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma; // indication: sigma cannot be negative
}

model {
    second_degree ~ normal(0, 10);
    first_degree ~ normal(0, 10);
    intercept   ~ normal(0, 10);
    sigma       ~ normal(0, 10);
    // Then, our likelihood function
    for (n in 1:(N)){
      y[n] ~ normal(second_degree*(x[n]*x[n]) + first_degree*x[n] + intercept, sigma);
    }
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 10461bf4fda3fe871ad7e125904745bb.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean   sd     2.5%      25%      50%      75%
intercept        28.85    0.46 8.98    12.14    22.61    28.56    34.55
first_degree      9.43    0.02 0.55     8.36     9.07     9.44     9.80
second_degree     1.97    0.00 0.01     1.96     1.97     1.97     1.97
sigma           134.36    0.09 3.26   128.36   132.07   134.29   136.65
lp__          -2881.15    0.07 1.43 -2884.70 -2881.83 -2880.84 -2880.10
                 97.5% n_eff Rhat
intercept        47.97   381 1.01
first_degree     10.50   577 1.01
second_degree     1.98   740 1.00
sigma           140.60  1293 1.00
lp__          -2879.40   479 1.01

Samples were drawn using NUTS(diag_e) at Tue May 14 14:42:59 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

Surprisingly, despite the correction, the results are still wildly
off. We think the outcome might be influenced by faulty priors, so we
try removing them.

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,2,6, 500, sigma=150)

modelString = "data { // the observations
    int<lower=0> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma; // indication: sigma cannot be negative
}

model {
    // Then, our likelihood function
    for (n in 1:(N)){
      y[n] ~ normal(second_degree*(x[n]*x[n]) + first_degree*x[n] + intercept, sigma);
    }
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: d2b16cc8b2214d87ccd7963b321901c3.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean    sd     2.5%      25%      50%      75%
intercept       105.29    1.07 17.86    71.14    92.35   105.77   117.64
first_degree      5.66    0.05  0.88     4.01     5.05     5.64     6.29
second_degree     2.00    0.00  0.01     1.99     2.00     2.00     2.01
sigma           148.09    0.17  4.88   139.01   144.77   147.95   151.36
lp__          -2743.12    0.05  1.43 -2746.50 -2743.85 -2742.79 -2742.08
                 97.5% n_eff Rhat
intercept       139.21   278 1.02
first_degree      7.34   331 1.02
second_degree     2.02   476 1.01
sigma           158.16   805 1.01
lp__          -2741.28   680 1.01

Samples were drawn using NUTS(diag_e) at Tue May 14 14:49:24 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

This time the results are much closer to the truth, and very similar
to the results we found with the second method (on monday's 13th
entry). The conclusion is that we need to find more precise priors,
or give a very general one (or none) so it won't influence the outcome
in a way we don't want it to.

**** Continued to read the book, notes on May 3rd section
Surprisingly, the part on polynomial regression is very sparse and
doesn't really help us make a functional model.
*** 2019-05-13 monday
**** Continued to read the book, notes on May 3rd section
**** A second try with a polynomial model :STAN:R:
We still have a simple model for the noise, and three parameters:
second_ degree, first_ degree and intercept.

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(rstan)

generate_dataset=function(intercept, first_degree, second_degree, N, min_x=0, max_x=100, sigma){
    x = sample(min_x:max_x,N,replace=T) 
    y = second_degree * (x^2) + first_degree * x + intercept + rnorm(N, sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}

df=generate_dataset(100,6,2, 500, sigma=150)
x2=df$x^2 #Need to add it here because we cannot in the stan model
head(df)
#+end_src

#+RESULTS:
#+begin_example
   x          y
1  0   257.9807
2 45  4353.7896
3 65  8872.7711
4 91 17196.1885
5 86 15519.8198
6 42  3911.2678
#+end_example

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] x2;
    vector[N] y;
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}

model {
    // We use no prior, as the previous ones are too vague to be useful for computing the posterior
    y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,x2=x2,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:

#+begin_example
Inference for Stan model: 6001096c2a1f8ca4aec729de06bdbead.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean    sd     2.5%      25%      50%      75%
intercept        74.78    0.87 18.35    39.43    61.72    74.65    87.40
first_degree      6.94    0.04  0.88     5.18     6.36     6.94     7.53
second_degree     1.99    0.00  0.01     1.97     1.98     1.99     1.99
sigma           150.35    0.14  4.71   141.63   146.97   150.29   153.73
lp__          -2751.41    0.05  1.45 -2755.14 -2752.10 -2751.08 -2750.32
                 97.5% n_eff Rhat
intercept       111.46   448 1.01
first_degree      8.70   501 1.01
second_degree     2.01   597 1.01
sigma           159.24  1214 1.00
lp__          -2749.61   713 1.01

Samples were drawn using NUTS(diag_e) at Tue May 14 11:28:37 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

This time, the parameters found by Stan are quite close to the ones we
chose to generate the data, but it's still quite inaccurate,
especially for the intercept.

#+begin_src R :results output graphics :file ./images/ex4_reworked_trace.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)+ scale_color_manual(values = blues9)
#+end_src

#+RESULTS:
[[file:./images/ex4_reworked_trace.png]]

At least this time the chains converge towards each other, so we can
conclude this model is better than the previous but still needs to be
worked on to correct some inaccuracies (we probably need to add some
informative priors).

The surprising part is that depending on the value of sigma, the trace
can show a very different behavior. For example, with a small value of
sigma:

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,6,2, 500, sigma=4)
x2=df$x^2
modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] x2;
    vector[N] y;
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}

model {
    // We use no prior, as the previous ones are too vague to be useful for computing the posterior
    y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
data = list(N=nrow(df),x=df$x,x2=x2,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:

#+begin_example
Warning messages:
1: There were 229 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: There were 1 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
3: There were 4 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
4: Examine the pairs() plot to diagnose sampling problems

Inference for Stan model: 6001096c2a1f8ca4aec729de06bdbead.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean     sd     2.5%      25%     50%     75%   97.5%
intercept        93.74    7.03  15.26    57.08    89.96  100.45  100.97  103.14
first_degree      6.25    0.28   0.61     5.86     5.96    5.98    6.40    7.69
second_degree     2.00    0.00   0.01     1.99     2.00    2.00    2.00    2.00
sigma             6.33    1.70   3.89     3.68     3.87    4.81    7.13   15.05
lp__          -1056.63  103.39 215.81 -1592.99 -1110.91 -951.71 -921.93 -920.60
              n_eff Rhat
intercept         5 3.37
first_degree      5 3.28
second_degree     5 3.22
sigma             5 2.75
lp__              4 4.73

Samples were drawn using NUTS(diag_e) at Tue May 14 11:30:45 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/ex4_reworked_trace_small_sigma.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)+ scale_color_manual(values = blues9)
#+end_src

#+RESULTS:
[[file:./images/ex4_reworked_trace_small_sigma.png]]

This time the chains don't converge again, despite the fact that the
model is still the same.

If we try with a much higher value of sigma, the chains do converge
but the found parameters have a higher standard deviation for their
value (which is not surprising).

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,6,2, 500, sigma=986)
x2=df$x^2
modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] x2;
    vector[N] y;
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}

model {
    // We use no prior, as the previous ones are too vague to be useful for computing the posterior
    y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
data = list(N=nrow(df),x=df$x,x2=x2,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 6001096c2a1f8ca4aec729de06bdbead.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean     sd     2.5%      25%      50%      75%
intercept        54.93    5.63 132.23  -202.71   -34.30    49.64   143.42
first_degree      5.57    0.27   5.96    -6.25     1.39     5.94     9.69
second_degree     2.02    0.00   0.06     1.91     1.98     2.01     2.06
sigma          1006.73    1.16  32.68   945.09   984.52  1006.43  1028.69
lp__          -3700.35    0.06   1.50 -3704.02 -3701.05 -3700.00 -3699.23
                 97.5% n_eff Rhat
intercept       318.61   552 1.01
first_degree     16.65   501 1.01
second_degree     2.13   556 1.01
sigma          1071.30   792 1.01
lp__          -3698.51   672 1.00

Samples were drawn using NUTS(diag_e) at Tue May 14 11:32:12 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

As we can see here, the parameters are much too inaccurate to model
properly the situation.

#+begin_src R :results output graphics :file ./images/ex4_reworked_trace_high_sigma.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)+ scale_color_manual(values = blues9)
#+end_src

#+RESULTS:
[[file:./images/ex4_reworked_trace_high_sigma.png]]

**** A look at generated quantities with a polynomial model :STAN:R:

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,6,2, 500, sigma=125)
x2=df$x^2
modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] x2;
    vector[N] y;
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}

model {
    y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
}
generated quantities {
   real x_pos = x[categorical_rng(rep_vector(1,N) / N)];
   real y_pos; // posterior predictions
   y_pos = normal_rng(second_degree*(x_pos^2)+first_degree*x_pos+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
data = list(N=nrow(df),x=df$x,x2=x2,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
#+end_src

#+begin_src R :results output graphics :file ./images/ex4_reworked_comparative_plot.png :exports both :width 600 :height 400 :session *R* 
extracted=rstan::extract(fit)
df_generated = data.frame(x=extracted$x_pos, y=extracted$y_pos, origin='generated')
df_generated = df_generated[sample(nrow(df_generated), nrow(df)), ]
df$origin = 'initial'
tmp = rbind(df, df_generated)
ggplot(tmp, aes(x=x, y=y, color=origin)) + geom_point(alpha=0.6)
#+end_src

#+RESULTS:
[[file:./images/ex4_reworked_comparative_plot.png]]

Once again, the data generated from the parameters found by stan seems
quite precise and close enough to the data we generated at the
beginning.
*** 2019-05-14 tuesday
**** REWORK <<Polynomial Model>>
***** A third model
We already tried two different models for this one; the first ended up
with unprecise parameters found, and so did the second one which also
had the disadvantage to need the vector of x squared as data.

Following Tom's advice, we try to do this in the transformed data
block.

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,2,6, 500, sigma=150)

modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] y;
}
transformed data{
    vector[N] x2;
    for (n in 1:N)
      x2[n]=x[n]*x[n];
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}
model {
    y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 91966ecf1347dea6d1c3f249ec4a3d80.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean    sd     2.5%      25%      50%      75%
intercept        96.08    1.35 20.83    57.95    81.53    95.94   110.73
first_degree      5.73    0.05  0.97     3.81     5.07     5.72     6.38
second_degree     2.00    0.00  0.01     1.98     2.00     2.00     2.01
sigma           153.74    0.14  4.77   144.75   150.37   153.67   157.03
lp__          -2761.46    0.07  1.39 -2764.76 -2762.21 -2761.16 -2760.42
                 97.5% n_eff Rhat
intercept       135.65   239 1.01
first_degree      7.61   317 1.01
second_degree     2.02   420 1.01
sigma           163.07  1205 1.00
lp__          -2759.69   435 1.02

Samples were drawn using NUTS(diag_e) at Tue May 14 15:44:11 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

Once again, this new model doesn't change the precision of the results
but it is perhaps easier to understand and use.

***** DONE Figuring out the right priors
:LOGBOOK:  
- State "DONE"       from "STARTED" [2019-05-16 jeu. 10:46]
- State "STARTED"    from "TODO"       [2019-05-15 mer. 16:06]
:END:      

With the help of [[http://modernstatisticalworkflow.blogspot.com/2017/04/an-easy-way-to-simulate-fake-data-from.htm][An easy way to simulate fake data from your Stan
model]], and other articles by Jim Savage, as well as [[https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations ][Stan's wiki entry
on priors]], we took a closer look at our priors, which used to be:
variable ~ normal(0,10). However we ran into several issues with that
prior: not only is it not precise enough, it also can push the
posterior in a direction we don't want, and we would end up with wrong
results. Therefore, we made a model that let us look more closely at
the influence of priors.

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(rstan)

generate_dataset=function(intercept, second_degree, first_degree, N, min_x=0, max_x=100, sigma){
    x = sample(min_x:max_x,N,replace=T) 
    y = second_degree * x^2 + first_degree * x + intercept + rnorm(N, sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}
df=generate_dataset(100,2,6, 500, sigma=150)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] y;
    int<lower = 0, upper = 1> run_estimation; // a switch to evaluate the likelihood
    real it_mean; //the center of the intercept's distribution
    real it_sd; //the variance of the intercept's distribution
    real f_degree_mean; //the center of the first degree's distribution
    real s_degree_mean; //the center of the second degree's distribution
    real degree_sd;  //the variance of the first and second degrees's distribution
    real sigma_mean; //the center of the sigma's distribution
    real sigma_sd; //the variance of the sigma's distribution
}
transformed data{
    vector[N] x2;
    for (n in 1:N)
      x2[n]=x[n]*x[n];
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}
model {
    intercept ~ normal(it_mean, it_sd);
    first_degree ~ normal(f_degree_mean, degree_sd);
    second_degree ~ normal(s_degree_mean, degree_sd);
    sigma ~ normal(sigma_mean, sigma_sd);
    if(run_estimation==1){ //we have two simulation modes: if run_estimation=0 then the posterior is the prior, else the simulation is the same as before
      y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
    }
}

generated quantities {
  real x_pos = x[categorical_rng(rep_vector(1,N) / N)];
  real y_sim;
  y_sim = normal_rng(second_degree*(x_pos^2)+first_degree*x_pos+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y,run_estimation=0,it_mean=1,it_sd=10,f_degree_mean=1,s_degree_mean=1,degree_sd=10,sigma_mean=1,sigma_sd=10)
fit = sampling(sm,data=data, iter=500, chains=8)
#+end_src

#+RESULTS:
#+begin_example

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 8e-06 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 1: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 1: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 1: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 1: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 1: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 1: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 1: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 1: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 1: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 1: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 1: Iteration: 500 / 500 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.01738 seconds (Warm-up)
Chain 1:                0.005843 seconds (Sampling)
Chain 1:                0.023223 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 4e-06 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 2: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 2: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 2: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 2: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 2: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 2: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 2: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 2: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 2: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 2: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 2: Iteration: 500 / 500 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.015096 seconds (Warm-up)
Chain 2:                0.005844 seconds (Sampling)
Chain 2:                0.02094 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 4e-06 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 3: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 3: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 3: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 3: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 3: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 3: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 3: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 3: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 3: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 3: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 3: Iteration: 500 / 500 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.01803 seconds (Warm-up)
Chain 3:                0.007696 seconds (Sampling)
Chain 3:                0.025726 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 6e-06 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 4: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 4: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 4: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 4: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 4: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 4: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 4: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 4: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 4: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 4: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 4: Iteration: 500 / 500 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.017028 seconds (Warm-up)
Chain 4:                0.005188 seconds (Sampling)
Chain 4:                0.022216 seconds (Total)
Chain 4: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 5).
Chain 5: 
Chain 5: Gradient evaluation took 4e-06 seconds
Chain 5: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 5: Adjust your expectations accordingly!
Chain 5: 
Chain 5: 
Chain 5: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 5: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 5: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 5: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 5: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 5: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 5: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 5: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 5: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 5: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 5: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 5: Iteration: 500 / 500 [100%]  (Sampling)
Chain 5: 
Chain 5:  Elapsed Time: 0.017953 seconds (Warm-up)
Chain 5:                0.004923 seconds (Sampling)
Chain 5:                0.022876 seconds (Total)
Chain 5: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 6).
Chain 6: 
Chain 6: Gradient evaluation took 4e-06 seconds
Chain 6: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 6: Adjust your expectations accordingly!
Chain 6: 
Chain 6: 
Chain 6: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 6: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 6: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 6: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 6: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 6: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 6: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 6: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 6: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 6: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 6: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 6: Iteration: 500 / 500 [100%]  (Sampling)
Chain 6: 
Chain 6:  Elapsed Time: 0.017574 seconds (Warm-up)
Chain 6:                0.004908 seconds (Sampling)
Chain 6:                0.022482 seconds (Total)
Chain 6: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 7).
Chain 7: 
Chain 7: Gradient evaluation took 4e-06 seconds
Chain 7: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 7: Adjust your expectations accordingly!
Chain 7: 
Chain 7: 
Chain 7: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 7: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 7: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 7: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 7: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 7: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 7: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 7: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 7: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 7: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 7: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 7: Iteration: 500 / 500 [100%]  (Sampling)
Chain 7: 
Chain 7:  Elapsed Time: 0.017471 seconds (Warm-up)
Chain 7:                0.004945 seconds (Sampling)
Chain 7:                0.022416 seconds (Total)
Chain 7: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 8).
Chain 8: 
Chain 8: Gradient evaluation took 4e-06 seconds
Chain 8: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 8: Adjust your expectations accordingly!
Chain 8: 
Chain 8: 
Chain 8: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 8: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 8: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 8: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 8: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 8: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 8: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 8: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 8: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 8: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 8: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 8: Iteration: 500 / 500 [100%]  (Sampling)
Chain 8: 
Chain 8:  Elapsed Time: 0.01777 seconds (Warm-up)
Chain 8:                0.00615 seconds (Sampling)
Chain 8:                0.02392 seconds (Total)
Chain 8: 
Warning messages:
1: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: Examine the pairs() plot to diagnose sampling problems
#+end_example

#+begin_src R :results output :session *R* :exports both
print(fit, pars=c("intercept", "first_degree", "second_degree", "sigma"))
y_sim = extract(fit, pars = "y_sim")
mean(df$y)
mean(y_sim$y_sim)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: ba67be2a2b0f008b97473baf3d15d10d.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

              mean se_mean    sd   2.5%   25%  50%   75% 97.5% n_eff Rhat
intercept     0.75    0.24 10.04 -19.48 -6.00 0.89  7.50 20.32  1738    1
first_degree  1.21    0.24  9.88 -17.40 -5.76 1.08  7.73 21.74  1757    1
second_degree 0.99    0.26 10.01 -18.16 -5.82 0.80  7.93 19.88  1485    1
sigma         8.48    0.16  6.24   0.32  3.52 7.44 12.07 22.49  1462    1

Samples were drawn using NUTS(diag_e) at Thu May 16 13:50:33 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
 
[1] 7144.907

[1] 3530.052
#+end_example

#+begin_src R :results output graphics :file ./images/generated_data_vague_prior.png :exports both :width 600 :height 400 :session *R* 
extracted=rstan::extract(fit)
df_generated = data.frame(x=extracted$x_pos, y=extracted$y_sim, origin='generated')
df_generated = df_generated[sample(nrow(df_generated), nrow(df)), ]  # there are more points in df_generated, so to have a nicer plot we sample exactly the same number than in df
df$origin = 'initial'
tmp = rbind(df, df_generated)
ggplot(tmp, aes(x=x, y=y, color=origin)) + geom_point(alpha=0.5)
#+end_src

#+RESULTS:
[[file:./images/generated_data_vague_prior.png]]

This way we can see more easily the distribution of the previously
given priors, and how that makes the generated data
inaccurate. Indeed, the parameters are really far from their actual
value, and so the generated data y_ sim is completely off.

To figure out the right priors, the best way would be to approximate
the value of our parameters and give that approximation as the prior.

#+begin_src R :results output :session *R* :exports both
mean_0=0 #a vector of means of y values for x=0
mean_10=0 #a vector of means of y values for x=10

for(x in 1:1000){ #we run 1000 simulations to get a pretty accurate approximation
  df=generate_dataset(100,2,6, 500, sigma=150)
  #for each simulation we get the mean of y values for x=0 and x=10
  mean_0[x]=mean(df$y[which(df$x %in% 0)])
  mean_10[x]=mean(df$y[which(df$x %in% 10)])
}

sd_intercept=abs(mean(diff(mean_0),na.rm = T))
#we get the difference between each means and we makes a mean of it to get a sd for the intercept.
#we use the absolute value for it to be positive.

sd_intercept
abs(mean(diff(mean_10),na.rm = T))

mean_intercept=round(mean(mean_0,na.rm = T)) 
#we make a mean of 1000 observations of the values at x=0 (so y=intercept)

intercept=rnorm(500,mean_intercept,sd_intercept)
#we simulate the intercept to remove it from the y values at x=10

mean_10=mean_10-intercept
#we removed the value of the intercept so y=first_degree*10+second_degree*100 

mean_sec= mean(mean_10,na.rm = T)%/%100 
#we divide the mean by 100 and store only the integer part to get the value of the second_degree

mean_first=mean(mean_10,na.rm = T)%%100 %/%10 
#this time we get the remainder, and divide it by ten to get the value of the first_degree

#+end_src

#+RESULTS:
:  
: [1] 0.1241611
: 
: [1] 0.03740519

As we can see, the values of the sd are really small, and can be
replaced by a sd of 1. Now we run our model again but with the
approximated means of intercept, first-degree and second-degree.

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y,run_estimation=0,it_mean=mean_intercept,it_sd=1,f_degree_mean=mean_first,s_degree_mean=mean_sec,degree_sd=1,sigma_mean=0,sigma_sd=10)

fit = sampling(sm,data=data, iter=500, chains=8)
#+end_src

#+RESULTS:
#+begin_example
 
SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 5e-06 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 1: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 1: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 1: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 1: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 1: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 1: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 1: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 1: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 1: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 1: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 1: Iteration: 500 / 500 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.010822 seconds (Warm-up)
Chain 1:                0.005554 seconds (Sampling)
Chain 1:                0.016376 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 6e-06 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 2: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 2: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 2: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 2: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 2: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 2: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 2: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 2: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 2: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 2: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 2: Iteration: 500 / 500 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.009115 seconds (Warm-up)
Chain 2:                0.007346 seconds (Sampling)
Chain 2:                0.016461 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 6e-06 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 3: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 3: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 3: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 3: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 3: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 3: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 3: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 3: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 3: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 3: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 3: Iteration: 500 / 500 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.009098 seconds (Warm-up)
Chain 3:                0.007334 seconds (Sampling)
Chain 3:                0.016432 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 5e-06 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 4: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 4: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 4: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 4: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 4: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 4: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 4: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 4: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 4: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 4: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 4: Iteration: 500 / 500 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.012249 seconds (Warm-up)
Chain 4:                0.00712 seconds (Sampling)
Chain 4:                0.019369 seconds (Total)
Chain 4: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 5).
Chain 5: 
Chain 5: Gradient evaluation took 4e-06 seconds
Chain 5: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 5: Adjust your expectations accordingly!
Chain 5: 
Chain 5: 
Chain 5: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 5: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 5: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 5: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 5: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 5: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 5: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 5: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 5: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 5: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 5: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 5: Iteration: 500 / 500 [100%]  (Sampling)
Chain 5: 
Chain 5:  Elapsed Time: 0.006398 seconds (Warm-up)
Chain 5:                0.004923 seconds (Sampling)
Chain 5:                0.011321 seconds (Total)
Chain 5: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 6).
Chain 6: 
Chain 6: Gradient evaluation took 4e-06 seconds
Chain 6: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 6: Adjust your expectations accordingly!
Chain 6: 
Chain 6: 
Chain 6: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 6: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 6: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 6: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 6: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 6: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 6: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 6: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 6: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 6: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 6: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 6: Iteration: 500 / 500 [100%]  (Sampling)
Chain 6: 
Chain 6:  Elapsed Time: 0.00574 seconds (Warm-up)
Chain 6:                0.007349 seconds (Sampling)
Chain 6:                0.013089 seconds (Total)
Chain 6: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 7).
Chain 7: 
Chain 7: Gradient evaluation took 5e-06 seconds
Chain 7: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
Chain 7: Adjust your expectations accordingly!
Chain 7: 
Chain 7: 
Chain 7: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 7: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 7: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 7: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 7: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 7: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 7: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 7: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 7: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 7: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 7: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 7: Iteration: 500 / 500 [100%]  (Sampling)
Chain 7: 
Chain 7:  Elapsed Time: 0.009162 seconds (Warm-up)
Chain 7:                0.008283 seconds (Sampling)
Chain 7:                0.017445 seconds (Total)
Chain 7: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 8).
Chain 8: 
Chain 8: Gradient evaluation took 5e-06 seconds
Chain 8: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
Chain 8: Adjust your expectations accordingly!
Chain 8: 
Chain 8: 
Chain 8: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 8: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 8: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 8: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 8: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 8: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 8: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 8: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 8: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 8: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 8: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 8: Iteration: 500 / 500 [100%]  (Sampling)
Chain 8: 
Chain 8:  Elapsed Time: 0.008495 seconds (Warm-up)
Chain 8:                0.009114 seconds (Sampling)
Chain 8:                0.017609 seconds (Total)
Chain 8:
#+end_example

#+begin_src R :results output :session *R* :exports both
y_sim = extract(fit, pars = "y_sim")
mean(df$y)
mean(y_sim$y_sim)
print(fit,pars=c("intercept","first_degree","second_degree","sigma"))
stan_trace(fit,pars=c("intercept","first_degree","second_degree","sigma"))
#+end_src

#+RESULTS:
#+begin_example

[1] 6668.037

[1] 6612.556

Inference for Stan model: ba67be2a2b0f008b97473baf3d15d10d.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
intercept     102.01    0.03 1.00 100.09 101.33 101.98 102.64 104.04  1604    1
first_degree    5.99    0.02 0.99   4.16   5.27   5.97   6.70   7.89  1572    1
second_degree   2.00    0.03 1.01   0.00   1.32   2.03   2.70   3.88  1272    1
sigma           7.95    0.19 6.17   0.25   3.06   6.69  11.60  23.05  1108    1

Samples were drawn using NUTS(diag_e) at Thu May 16 13:51:42 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

Now this looks much better! The chains converge towards each other as
they should, and the generated data from the parameters looks quite
close to the actual data. Let's run our model for real this time.

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y,run_estimation=1,it_mean=mean_intercept,it_sd=1,f_degree_mean=mean_first,s_degree_mean=mean_sec,degree_sd=1,sigma_mean=150,sigma_sd=1)

fit = sampling(sm,data=data, iter=500, chains=8)
#+end_src

#+RESULTS:
#+begin_example
 
SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.00013 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.3 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 1: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 1: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 1: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 1: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 1: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 1: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 1: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 1: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 1: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 1: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 1: Iteration: 500 / 500 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.65015 seconds (Warm-up)
Chain 1:                0.240311 seconds (Sampling)
Chain 1:                0.890461 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 6.5e-05 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.65 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 2: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 2: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 2: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 2: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 2: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 2: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 2: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 2: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 2: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 2: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 2: Iteration: 500 / 500 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.730258 seconds (Warm-up)
Chain 2:                1.27852 seconds (Sampling)
Chain 2:                2.00877 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 8.7e-05 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.87 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 3: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 3: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 3: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 3: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 3: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 3: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 3: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 3: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 3: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 3: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 3: Iteration: 500 / 500 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.392354 seconds (Warm-up)
Chain 3:                0.529003 seconds (Sampling)
Chain 3:                0.921357 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 8.3e-05 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.83 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 4: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 4: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 4: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 4: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 4: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 4: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 4: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 4: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 4: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 4: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 4: Iteration: 500 / 500 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.409167 seconds (Warm-up)
Chain 4:                0.626242 seconds (Sampling)
Chain 4:                1.03541 seconds (Total)
Chain 4: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 5).
Chain 5: 
Chain 5: Gradient evaluation took 6.4e-05 seconds
Chain 5: 1000 transitions using 10 leapfrog steps per transition would take 0.64 seconds.
Chain 5: Adjust your expectations accordingly!
Chain 5: 
Chain 5: 
Chain 5: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 5: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 5: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 5: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 5: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 5: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 5: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 5: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 5: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 5: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 5: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 5: Iteration: 500 / 500 [100%]  (Sampling)
Chain 5: 
Chain 5:  Elapsed Time: 0.427934 seconds (Warm-up)
Chain 5:                0.283541 seconds (Sampling)
Chain 5:                0.711475 seconds (Total)
Chain 5: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 6).
Chain 6: 
Chain 6: Gradient evaluation took 8.5e-05 seconds
Chain 6: 1000 transitions using 10 leapfrog steps per transition would take 0.85 seconds.
Chain 6: Adjust your expectations accordingly!
Chain 6: 
Chain 6: 
Chain 6: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 6: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 6: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 6: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 6: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 6: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 6: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 6: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 6: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 6: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 6: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 6: Iteration: 500 / 500 [100%]  (Sampling)
Chain 6: 
Chain 6:  Elapsed Time: 0.870979 seconds (Warm-up)
Chain 6:                0.562292 seconds (Sampling)
Chain 6:                1.43327 seconds (Total)
Chain 6: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 7).
Chain 7: 
Chain 7: Gradient evaluation took 6.4e-05 seconds
Chain 7: 1000 transitions using 10 leapfrog steps per transition would take 0.64 seconds.
Chain 7: Adjust your expectations accordingly!
Chain 7: 
Chain 7: 
Chain 7: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 7: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 7: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 7: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 7: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 7: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 7: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 7: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 7: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 7: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 7: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 7: Iteration: 500 / 500 [100%]  (Sampling)
Chain 7: 
Chain 7:  Elapsed Time: 1.0137 seconds (Warm-up)
Chain 7:                0.291937 seconds (Sampling)
Chain 7:                1.30564 seconds (Total)
Chain 7: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 8).
Chain 8: 
Chain 8: Gradient evaluation took 6.5e-05 seconds
Chain 8: 1000 transitions using 10 leapfrog steps per transition would take 0.65 seconds.
Chain 8: Adjust your expectations accordingly!
Chain 8: 
Chain 8: 
Chain 8: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 8: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 8: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 8: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 8: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 8: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 8: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 8: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 8: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 8: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 8: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 8: Iteration: 500 / 500 [100%]  (Sampling)
Chain 8: 
Chain 8:  Elapsed Time: 0.638479 seconds (Warm-up)
Chain 8:                0.317678 seconds (Sampling)
Chain 8:                0.956157 seconds (Total)
Chain 8: 
Warning messages:
1: There were 1 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
2: Examine the pairs() plot to diagnose sampling problems
#+end_example

#+begin_src R :results output :session *R* :exports both
y_sim = extract(fit, pars = "y_sim")
mean(df$y)
mean(y_sim$y_sim)
print(fit,pars=c("intercept","first_degree","second_degree","sigma"))
#+end_src

#+RESULTS:
#+begin_example

[1] 6668.037

[1] 6640.834

Inference for Stan model: ba67be2a2b0f008b97473baf3d15d10d.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
intercept     100.97    1.30 7.10  97.79 101.36 102.02 102.66 103.96    30 1.25
first_degree    5.43    0.02 0.43   4.57   5.12   5.43   5.73   6.32   534 1.00
second_degree   2.00    0.00 0.01   1.99   2.00   2.00   2.01   2.01   550 1.00
sigma         150.28    0.03 0.96 148.35 149.64 150.31 150.93 152.12  1197 1.00

Samples were drawn using NUTS(diag_e) at Thu May 16 13:52:14 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

And now with the data taken in account, the found parameters are much
more accurate than before, and this shows in the generated data y-sim.

#+begin_src R :results output graphics :file ./images/generated_data_precise_prior.png :exports both :width 600 :height 400 :session *R* 
extracted=rstan::extract(fit)
df_generated = data.frame(x=extracted$x_pos, y=extracted$y_sim, origin='generated')
df_generated = df_generated[sample(nrow(df_generated), nrow(df)), ]  # there are more points in df_generated, so to have a nicer plot we sample exactly the same number than in df
df$origin = 'initial'
tmp = rbind(df, df_generated)
ggplot(tmp, aes(x=x, y=y, color=origin)) + geom_point(alpha=0.5)
#+end_src

#+RESULTS:
[[file:./images/generated_data_precise_prior.png]]

*** 2019-05-16 thursday
**** Attended the seminar about how to use Big Data Solutions to Improve HPC systems by Thomas Ropars :SEMINAR:
***** Introduction
Supercomputers are producing large amount of data that need to be
analyzed. They produce mostly two kinds of data: scientific data and
monitoring data. Scientific data are the results of the execution of
numerical simulations and need to be analyzed to extract knowledge. 
Monitoring data are produced by all kinds of sensors and software
components, and can be analyzed to detect, among other things,
reliability and performance issues. Considering the scale of such
systems, the amount of data to process is huge and analyzing these
data with short response time is often necessary. Using techniques and
algorithms coming from the Big Data community seems appealing in this
context.

***** Notes
- This talk presented how to apply Big Data and Machine Learning
  techniques in a High Performance Computing context using Apache
  Spark Streaming as a tool for in-situ data analysis. Apache Spark
  Streaming is a scalable fault-tolerant streaming processing
  system, that allows real-time processing of data, which means that
  the data can come in a continuous flow. Data will be accumulated
  during a certain duration (every N period of time), and Spark
  Streaming will produce a Resilient Distributed Dataset from these
  accumulated data.

- CPU overheating issues can cause damage to hardware so it's
  important to prevent them as much as possible. To do so, the idea
  would be to predict them in advance, so to reduce the frequency of
  use of the CPU precisely one minute before the upcoming overheat. In
  many cases, this would prevent overheating, and for false positive
  cases (cases where the CPU was not going to overheat), it's not an
  important loss in performance so it's worth reducing the frequency
  just in case.
*** 2019-05-17 friday
**** Started to look at the model of dgemm with the provided data :STAN:R:
Read [[https://github.com/Ezibenroc/calibration_analysis/blob/master/dahu/blas/dgemm_heterogeneous_model.ipynb ][Tom's analysis on the heterogenous model of dgemm]].

First we import the data from the dgemm.csv file.

#+begin_src R :results output :session *R* :exports both
library(rstan)
library(dplyr)

data=read.csv(file="~/Documents/org/dgemm.csv", header=TRUE, sep=",")

data=data.frame(as.numeric(data$m),as.numeric(data$n),as.numeric(data$k),data$duration,data$cpu)
#we change the type of the data from int to numeric to avoid int overflow when we compute m*n*k

names(data)=c("m","n","k","duration","cpu")

data$mnk=data$m * data$n * data$k
data$mn = data$m * data$n
data$mk = data$m * data$k
data$nk = data$m * data$k

head(data)
#+end_src

#+RESULTS:
#+begin_example
Le chargement a nécessité le package : ggplot2
Registered S3 methods overwritten by 'ggplot2':
  method         from 
  [.quosures     rlang
  c.quosures     rlang
  print.quosures rlang
Le chargement a nécessité le package : StanHeaders
rstan (Version 2.18.2, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)

Attachement du package : ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union
 
     m    n    k  duration cpu        mnk      mn       mk       nk
1  378 7640 2427 0.4859466  20 7008981840 2887920   917406   917406
2  378 7640 2427 0.4861293  20 7008981840 2887920   917406   917406
3  378 7640 2427 0.4868529  20 7008981840 2887920   917406   917406
4 9441  640 1160 0.4551385  20 7008998400 6042240 10951560 10951560
5 9441  640 1160 0.4535278  20 7008998400 6042240 10951560 10951560
6 9441  640 1160 0.4544535  20 7008998400 6042240 10951560 10951560
#+end_example

So we have our dataframe with the coefficients mnk, mn, mk and nk as
well as the dgemm's run duration depending on the coefficients and the
cpu.

#+begin_src R :results output :session *R* :exports both
ggplot(sample_n(data, 10000), aes(x=mnk, y=duration, color='factor(cpu)')) + geom_point(alpha=0.3) +geom_smooth(method='lm')
#+end_src

We can see on this graph that the majority of the data is between the
values 0 and 2e+10 of mnk. We sample our data frame to keep only the
data between these values.

#+begin_src R :results output graphics :file ./images/dgemm_duration_on_mnk.png :exports both :width 600 :height 400 :session *R* 
tmp=select(filter(data, data$mnk < 2e10),everything())

ggplot(sample_n(tmp, 10000), aes(x=mnk, y=duration, color=factor(cpu))) + geom_point(alpha=0.3) +geom_smooth(method='lm')
#+end_src

#+RESULTS:
[[file:./images/dgemm_duration_on_mnk.png]]

Now we can easily find back the heterogeneity that Tom observed.
Indeed, the duration of a dgemm run depends on a combinaison of the m,
n and k parameters (the coefficients mn, mk and nk play a part too but
we started with an easier model with only mnk), but also on the cpu.

We'll start with a M_H-1/N_H-2 model, so a linear model with noise
modeled by a polynomial function, with per-host estimations.

*** 2019-05-20 monday
**** Continued trying to model dgemm :STAN:R:

#+begin_src R :results output :session *R* :exports both
data=read.csv(file="~/Documents/org/dgemm.csv", header=TRUE, sep=",")
data=data.frame(as.numeric(data$m),as.numeric(data$n),as.numeric(data$k),data$duration,data$cpu)
names(data)=c("m","n","k","duration","cpu")

data$mnk=data$m * data$n * data$k
data$mn = data$m * data$n
data$mk = data$m * data$k
data$nk = data$m * data$k
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
library(rstan)
modelString = "data {
    int<lower=0> N;
    vector[N] mnk; //m*n*k
    vector<lower=0>[N] duration; //the duration of dgemm
    real it_mean; //the center of the intercept's distribution
    real it_sd; //the variance of the intercept's distribution
    real mu_mean; //the center of the mnk coefficient's distribution
    real mu_sd; //the variance of the mnk coefficient's distribution
    real sigma_mean; //the center of the noise's distribution
    real sigma_sd;  //the variance of the noise's distribution
}
parameters {
    real intercept;
    real mu;
    real<lower=0> sigma;
}
model {
    intercept ~ normal(it_mean, it_sd);
    mu ~ normal(mu_mean, mu_sd);
    sigma ~ normal(sigma_mean, sigma_sd);
    duration ~ normal(mu*mnk+intercept, sigma*mnk); //duration=mu*mnk+intercept +- sigma*mnk
}
generated quantities {
  real x_pos = mnk[categorical_rng(rep_vector(1,N) / N)];
  real y_sim = normal_rng(mu*x_pos+intercept, sigma*x_pos);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
library(dplyr)
t=sample_n(data,500) #We use only a sample of 500 values
reg = lm(data=t, duration~mnk) 
#We conduct a linear regression on the sample, to get an approximation of our coefficients
out=summary(reg)
out
#+end_src

#+RESULTS:
#+begin_example
 
Call:
lm(formula = duration ~ mnk, data = t)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.095917 -0.014563 -0.007010  0.004494  0.206556 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 1.232e-02  2.211e-03   5.571 4.14e-08 ***
mnk         6.612e-11  3.370e-13 196.192  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.0308 on 498 degrees of freedom
Multiple R-squared:  0.9872,	Adjusted R-squared:  0.9872 
F-statistic: 3.849e+04 on 1 and 498 DF,  p-value: < 2.2e-16
#+end_example

#+begin_src R :results output :session *R* :exports both
dt = list(N=500,mnk=t$mnk,duration=t$duration,run_estimation=1,it_mean=out$coefficients[1,1],it_sd=out$coefficients[1,2],mu_mean=out$coefficients[2,1],mu_sd=out$coefficients[2,2],sigma_mean=0,sigma_sd=0.1)
fit = sampling(sm,data=dt, iter=500, chains=8)
#+end_src

#+RESULTS:
#+begin_example

SAMPLING FOR MODEL '026d3a101ad7cfe8c65a4e6d7a90fbdf' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.000115 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.15 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 1: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 1: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 1: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 1: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 1: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 1: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 1: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 1: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 1: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 1: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 1: Iteration: 500 / 500 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 9.67789 seconds (Warm-up)
Chain 1:                14.8252 seconds (Sampling)
Chain 1:                24.5031 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL '026d3a101ad7cfe8c65a4e6d7a90fbdf' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 7.8e-05 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.78 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 2: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 2: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 2: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 2: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 2: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 2: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 2: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 2: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 2: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 2: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 2: Iteration: 500 / 500 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 9.5827 seconds (Warm-up)
Chain 2:                14.0612 seconds (Sampling)
Chain 2:                23.6439 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL '026d3a101ad7cfe8c65a4e6d7a90fbdf' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 7.7e-05 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.77 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 3: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 3: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 3: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 3: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 3: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 3: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 3: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 3: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 3: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 3: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 3: Iteration: 500 / 500 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 10.4662 seconds (Warm-up)
Chain 3:                13.9052 seconds (Sampling)
Chain 3:                24.3714 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL '026d3a101ad7cfe8c65a4e6d7a90fbdf' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 7.7e-05 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.77 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 4: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 4: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 4: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 4: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 4: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 4: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 4: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 4: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 4: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 4: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 4: Iteration: 500 / 500 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 10.3357 seconds (Warm-up)
Chain 4:                14.6877 seconds (Sampling)
Chain 4:                25.0234 seconds (Total)
Chain 4: 

SAMPLING FOR MODEL '026d3a101ad7cfe8c65a4e6d7a90fbdf' NOW (CHAIN 5).
Chain 5: 
Chain 5: Gradient evaluation took 0.00011 seconds
Chain 5: 1000 transitions using 10 leapfrog steps per transition would take 1.1 seconds.
Chain 5: Adjust your expectations accordingly!
Chain 5: 
Chain 5: 
Chain 5: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 5: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 5: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 5: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 5: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 5: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 5: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 5: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 5: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 5: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 5: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 5: Iteration: 500 / 500 [100%]  (Sampling)
Chain 5: 
Chain 5:  Elapsed Time: 9.41052 seconds (Warm-up)
Chain 5:                13.3315 seconds (Sampling)
Chain 5:                22.742 seconds (Total)
Chain 5: 

SAMPLING FOR MODEL '026d3a101ad7cfe8c65a4e6d7a90fbdf' NOW (CHAIN 6).
Chain 6: 
Chain 6: Gradient evaluation took 7.4e-05 seconds
Chain 6: 1000 transitions using 10 leapfrog steps per transition would take 0.74 seconds.
Chain 6: Adjust your expectations accordingly!
Chain 6: 
Chain 6: 
Chain 6: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 6: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 6: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 6: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 6: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 6: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 6: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 6: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 6: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 6: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 6: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 6: Iteration: 500 / 500 [100%]  (Sampling)
Chain 6: 
Chain 6:  Elapsed Time: 8.45629 seconds (Warm-up)
Chain 6:                13.5582 seconds (Sampling)
Chain 6:                22.0145 seconds (Total)
Chain 6: 

SAMPLING FOR MODEL '026d3a101ad7cfe8c65a4e6d7a90fbdf' NOW (CHAIN 7).
Chain 7: 
Chain 7: Gradient evaluation took 7.7e-05 seconds
Chain 7: 1000 transitions using 10 leapfrog steps per transition would take 0.77 seconds.
Chain 7: Adjust your expectations accordingly!
Chain 7: 
Chain 7: 
Chain 7: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 7: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 7: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 7: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 7: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 7: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 7: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 7: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 7: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 7: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 7: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 7: Iteration: 500 / 500 [100%]  (Sampling)
Chain 7: 
Chain 7:  Elapsed Time: 9.80917 seconds (Warm-up)
Chain 7:                13.8915 seconds (Sampling)
Chain 7:                23.7007 seconds (Total)
Chain 7: 

SAMPLING FOR MODEL '026d3a101ad7cfe8c65a4e6d7a90fbdf' NOW (CHAIN 8).
Chain 8: 
Chain 8: Gradient evaluation took 0.000108 seconds
Chain 8: 1000 transitions using 10 leapfrog steps per transition would take 1.08 seconds.
Chain 8: Adjust your expectations accordingly!
Chain 8: 
Chain 8: 
Chain 8: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 8: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 8: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 8: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 8: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 8: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 8: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 8: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 8: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 8: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 8: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 8: Iteration: 500 / 500 [100%]  (Sampling)
Chain 8: 
Chain 8:  Elapsed Time: 9.31903 seconds (Warm-up)
Chain 8:                14.2888 seconds (Sampling)
Chain 8:                23.6079 seconds (Total)
Chain 8: 
Warning messages:
1: There were 1717 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
2: Examine the pairs() plot to diagnose sampling problems
#+end_example

#+begin_src R :results output :session *R* :exports both
print(fit)
extracted=rstan::extract(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 026d3a101ad7cfe8c65a4e6d7a90fbdf.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                   mean      se_mean           sd          2.5%           25%
intercept          0.02         0.41 8.300000e-01 -1.620000e+00 -4.000000e-01
mu                 0.00         0.00 0.000000e+00  0.000000e+00  0.000000e+00
sigma              2.00         1.04 2.080000e+00  2.900000e-01  3.700000e-01
x_pos     5039832266.78  82110933.00 3.818791e+09  3.293576e+08  2.325543e+09
y_sim     -381188467.75 473708002.65 2.095719e+10 -2.986312e+10 -2.883216e+09
lp__          -81659.41     44705.26 8.950006e+04 -2.838172e+05 -1.096817e+05
                    50%           75%         97.5% n_eff        Rhat
intercept          0.16          0.43  1.210000e+00     4 893567818.4
mu                 0.00          0.00  0.000000e+00  2056         1.0
sigma              1.61          2.35  7.050000e+00     4 648954749.3
x_pos     4659071544.00 7666356180.00  9.666527e+09  2163         1.0
y_sim      -77336981.48 2232032359.47  3.064594e+10  1957         1.0
lp__          -36636.90     -17260.24 -1.316877e+04     4    129822.4

Samples were drawn using NUTS(diag_e) at Mon May 20 16:54:02 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

The result of our model isn't very conclusive, the mu coefficient is always equal
to zero with no variance, which seems very surprising and probably
incorrect. Likewise, there is obviously an issue with the generated
data. The mean of y_sim should be between 0 and 1, since most of the
values of duration are.

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R* 
stan_trace(fit,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
[[file:/tmp/babel-5750T4n/figure5750tM0.png]]

The trace shows as well that the fit didn't work properly, since the
only parameter for which the chains converge is mu, which appears to
have an extremely low value, but different from zero. We assume that
the value shown in the summary of the fit must be truncated.

For the generated data, let's look at what happens if we try to
generate data ourselves with the found coefficients.

#+begin_src R :results output :session *R* :exports both
y=extracted$x_pos*extracted$mu+extracted$intercept
y_gen=(y+extracted$sigma*extracted$x_pos)+(y-extracted$sigma*extracted$x_pos)

mean(t$duration)
mean(y_gen)
mean(extracted$y_sim)
#+end_src

#+RESULTS:
:  
: [1] 0.3517019
: 
: [1] 0.7104626
: 
: [1] -381188468

If we generate data from the coefficients we found, we notice that
it could be more accurate (since the fit didn't work properly) but
it's already much closer to the real data than the data generated by
stan. So for some reason, there's an issue with generating data with
stan on this model.

