# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages logdrawer
#+TITLE:       Journal
#+AUTHOR:      Hoël Jalmin
#+LANGUAGE:    en
#+TAGS: [ PROGRAMMING : R(r) STAN(S) ]
#+TAGS: [ TOOLS : ORGMODE(o) GIT(g)  ]
#+TAGS: PAPER(P) VIDEO(V) MEETING(m) SEMINAR(s)
#+SEQ_TODO: TODO(t!) STARTED(s!) INTERRUPTED(i!) DONE(d!)
#+SEQ_TODO: REWORK(r!)
#+SEQ_TODO: EDIT(e!)

* Introduction
This file is the report for my internship, about automating
calculation kernels' modellings. I'll explain more about this later on.

* 2019
** 2019-04 april
*** 2019-04-30 monday
**** DONE Read [[https://hal.inria.fr/hal-02096571/document ][Faithful and Efficient Simulation of High Performance Linpack]] :PAPER:
:LOGBOOK:  
- State "DONE"       from "STARTED"    [2019-05-02 thursday 16:53]
:END:      
***** Introduction
With a power consumption of several MW per hour on a TOP500 machine,
running applications on supercomputers at scale solely to optimize
their performance is extremely expensive. Likewise, High-Performance
Linpack (HPL), the benchmark used to ran supercomputers in the TOP500,
requires a careful tuning of many parameters (problem size, grid
arrangement, granularity, collective operation algorithms, etc.) and
supports exploration of the most common and fundamental performance
issues and their solutions. In this article, we explain how we both
extended the SimGrid's SMPI simulator and slightly modified the
open source version of HPL to allow a fast emulation on a single
commodity server at the scale of a supercomputer. We explain how to
model the different components (network, BLAS, ...) and show that a
careful modeling of both spatial and temporal node variability allows us
to obtain predictions within a few percents of real experiments.

***** Notes 
The paper explains how to extend the Simgrid simulator to simulate the
HPL benchmark and get the rough performance of it, as it is quite
representative of usual performance issues, without actually running
it. The simulation reduces considerably memory consumption compared to
a normal run of HPL, so it can be ran on a smaller cluster with less
time. Other simulation models for HPL already existed, but were quite
inaccurate as they did not take all the parameters in account.

****** About HPL
HPL is a parallel implementation of a benchmark which measures how
fast a computer can solve a system of linear equations. HPL uses the
BLAS libraries to make matrix operations such as multiplication (using
dgemm). The sequential complexity depends on the matrix size: 
2/3N³ + 2N² + O(N), but the calculation time cannot be determined
without running HPL or a simulation of it (it depends on network
capacity and a lot of parameters). Even while running HPL there's a
difference between the theoretical peak performance and the actual one
reached depending on how MPI communications are done.


****** About SimGrid and MPI simulation
There are two approaches to MPI simulation: offline and online. The
first one gives a previously obtained trace of the application to a
simulator, which makes predictions depending on the performance
models. The issue is that a first run is necessary to get the trace,
which is only valid at the moment of the run, since it depicts a
precise behavior. Thus, predictions are extrapolated which can be an
issue if the execution is non deterministic. This simulation type is
not proper for HPL, whereas online simulation (used by SimGrid) means
the simulator directs the execution; it decides which process to run
at which time. SimGrid provides accurate performance models for
application with heavy network use, as it considers network topology
and heterogeneity. SMPI, based on SimGrid, runs MPI ranks on mutually
exclusive threads, so whenever a thread enters an MPI call, SMPI moves
its clock ahead of the time spent computing since the last MPI call.


****** DONE About how to emulate HPL
:LOGBOOK:  
- State "DONE"       from "STARTED"    [2019-05-02 thursday 10:51]
- State "STARTED"    from "TODO"       [2019-05-02 thursday 09:39]
:END:      
Most of the calculation time of HPL is spent in the dgemm and dtrsm
(A.x = b equations) kernels, so replacing these two by models greatly
reduces simulation time: it was only be necessary to make a SMPI call
when one of these kernels were used (this makes results incorrect). 
Likewise, other functions were replaced since they used incorrect data
because of dgemm and dstrsm. Initialization of pseudo-random matrices
and correctness verification were also skipped, changing the idamax
function to return random values, making the simulation deterministic.

Memory consumption was scaled down by sharing the input matrix A
between all MPI processes and indicating that its data can be
overwritten. For the panel matrix, containing matrix indexes, data
corruption cannot be risked so a partial shared malloc was introduced,
to only share the range of values that doesn't have matrix indexes. 
The number of allocations and page faults was also greatly reduced by
reusing the allocated panels instead of getting more of them. Also, a
lot of calls to memcopy were avoided by making SMPI aware of which
memory areas were private or not. Calls to mmap (used to remap the
data segment to the private copy of the MPI rank when context
switching) were also avoided by loading several times the data segment
into memory. Finally, huge pages were used so the page table wouldn't
be too large.

All of these changes allowed a reduced complexity by removing the
O(N³) part.


****** DONE About the chosen models for kernels and communications
:LOGBOOK:  
- State "DONE"       from "STARTED"    [2019-05-02 thursday 16:05]
- State "STARTED"    from "TODO"       [2019-05-02 thursday 10:51]
:END:      
Several modeling notations were chosen. M-0 means the duration is
roughly constant and independant of the input parameters. M-1 means
the performance depends on a combination of the parameters. M-2
is used when a polynomial model is required (for complex behavior). 
M_H means the platform has spatial variability and modeling should
be done for each host, and M' is used when the duration is linear for
specific parameters values. The same was done for the noise: N-0 means
no noise, N-1 means the noise has a normal distribution, N-2 means the
noise has to be modelised by a polynomial function. N_H means noise
estimations are made per-host and N' is when the noise is modelised by
several normal distributions.

Communications between MPI nodes are mostly linear in message size but
vary depending on the protocol used. The chosen model was a M'-1
(linear within each host but discontinuous) N'-1 (complex distribution
of linear noise) model, with its paramaters estimated by pytree: the
message size range and the 2-4 modes of the normal distribution mixture.

For dgemm, a polynomial model (M_H-2) was required because of the spatial
variability: depending on the value of M*N*K (so on the matrix size),
some durations will be higher than others regardless of the node used
(which means dgemm doesn't have a linear behavior). There is also some
temporal variability, modelized here by a random call. For other BLAS
and HPL kernels, a linear model M-1 is close enough to reality; but the
noise needs to follow a N-2 model because the variability it provides
increases with the value of the parameters (which indicates a
polynomial model).


****** DONE The simulation at scale
:LOGBOOK:  
- State "DONE"       from "STARTED"    [2019-05-02 thursday 16:53]
- State "STARTED"    from "TODO"       [2019-05-02 thursday 16:20]
:END:      
After working on these optimizations, an emulation was done at scale
using the Dahu cluster, with a high number of iterations, complex
communication patterns and more MPI processes than usual. The
emulation was stopped after five iterations, to compare to real runs:
the communication durations were a bit too optimistic, and it was
noted that using a complex model makes more realistic traces. Several
simulations were then done to figure out which model would be more
accurate, and the models that are the closest to reality are: M-1 N-2
for the kernels, M_H-2 N_H-2 for dgemm and M'-1 N-0 for the network. 
Adding a linear noise for the network doesn't have any visible effect.

This was compared to the run made on the Stampede cluster. Considering
the input parameters for this run and the result, an optimistic model
(M'-1 N-0) was chosen for the dgemm and dtrsm functions, as well as
for the MPI communications, while ignoring the other functions. 
Although usually the simulations are within of few percent of reality,
the performance of this one was much lower than the performance of the
Stamepde run, which used a modified version of HPL and different
parameters than the ones printed by HPL. It was also found that the
communications had been optimized for the run, which explains the
difference between the simulation and the reality.

*** 2019-04-30 tuesday
**** Installed Emacs and Org-mode and all required dependencies. :ORGMODE:
Followed the given MOOC to understand how Org-mode functions, and how
to take efficient notes. Learned the basics of org-mode, and of the
keyboard shortcuts introduced by the provided emacs initialization.

**** Began writing the journal
** 2019-05 may
*** 2019-05-02 thursday
**** Attended the keynote speech about contrasting artifical and human intelligence by Jean-Louis Dessales :SEMINAR:
***** Introduction
Some artificial intelligence techniques were recently able to scale
up, provoking what many consider as a technical revolution. However,
the type of AI that proved so successful in the past decade relies on
the exploitation of massive data, and is limited to narrow domains of
expertise. By contrast, human intelligence is very efficient at making
broad inferences from limited evidence. I will highlight a few
qualitative differences between artificial intelligence and human
intelligence. These differences are mainly due to a small set of
cognitive operations, such as contrast or simplicity detection, that
human beings perform on the fly. I will also suggest that attempting
to bridge the gap between these two forms of intelligence might be the
best way to improve artificial systems in the future.

***** Notes
- It is often said that artificial intelligence will eventually replace
 mankind, but Jean-Louis Dessales clearly doesn't think so. It is
 foolish for him to believe there is a loss function that can be
 applied for everything.

- There are specific characteristics of artifical intelligence that
  makes them too different from human beings to be able to solve every
  problem humans are able to.
 
  + For instance, neuronal networks work best when they can analyse a
    lot of data, to then be able to recognize it. This can work for
    fields such as image recognition, but not for particular fields
    such as criminal investigation where every case is different and
    the possible similarity between cases is only an average and
    cannot be exact. Also artificial intelligence work without biases,
    unlike human beings.

  + Artificial intelligence function in an isotropic way, they can
    learn how to recognize language even if the order of the words in
    the sentences is mixed up; whereas this will just confuse human
    beings. However, even though artificial intelligences recognize
    language they do not understand it, and if they try to speak they
    often make no sense because they look at semantics similarity and
    not semantics itself. Any idoms are lost to them, because they
    will not recognize it.

  + Neuronal networks are also unable to recognize a pattern if they
    were not introduced to it, and have very narrow expertise. For
    example, if given a sequence of numbers, they will be unable to
    find the next ones (ex: 1223334444). As they work with recognition
    instead of understanding, trying to broaden their expertise will
    sometimes fail. For example a neuronal network made to recognize
    skin cancer cannot recognize psoriasis, and trying to change it
    will reduce their ability to see skin cancer.

- When attempting to solve problems, human beings look for simplicity:
  the least complexity is the best. How humans see complexity and
  unexpectedness is by a difference between what they expect and what
  they see.

  + Unexpectedness can be defined as a complexity drop between what was
    expected and what happened. For example, seeing a "simple" (well
    known) person in a "complex" (rarely visited) place is unexpected,
    because of the difference in complexity.

- Because of the way humans learn by making connections (for example
  when learning a new word the context is immediately connected with
  it; if a child hears their parent talk about "chopping the fish"
  they are able to connect it to the place they heard it, to the food
  associated with it...), artificial intelligence cannot replace
  humans in the sense that this process of learning is completely
  foreign to them. They do not process the relations between objects,
  they focus on pattern recognition.
*** 2019-05-03 friday
**** STARTED Read [[http://xcelab.net/rm/statistical-rethinking ][A Bayesian Course with examples in R and Stan]] by Richard McElreath :PAPER:STATISTICS:STAN:
:LOGBOOK:  
- State "STARTED"    from "INTERRUPTED" [2019-05-09 jeu. 16:42]
- State "INTERRUPTED" from "STARTED"    [2019-05-06 lun. 18:52]
:END:      
***** About models
- Models are like golems, powerful but easy to misuse.
- Use of models is wide spread and necessary but giving too many
  models is confusing, you need to find the right one. statistical
  tools are not diverse enough.
- Falsifying null hypothesis isn't enough. it would be more logical to
  falsify an explanatory model because falsifying a null model bring
  very little knowledge.
- Hypotheses correspond to several process models, and statistical
  models correspond to several process models
- Bayesian statistics are able to process small samples and don't need
  a prior hypothesis to confirm or reject. In bayesian statistics,
  probabilities are interpreted as uncertainty, models' parameters are
  modelled by probability laws and parameters are approximated by execution.
- The priors are observations/deductions that we know before the
  analysis and that helps guide the outcome.

***** About priors, likelihood and posteriors
- Small worlds are the logical worlds of the models. There are no
  surprises, and it's possible to verify the models's logic and if it
  works properly. Whereas in large worlds there might be unpredicted
  events, and the models never completely encompass large worlds.
- Bayesian analysis are garden of forking data: multiple paths exist,
  with each one branching more possibilities as we explore the
  paths. By getting more observations, we prune some path so to keep
  only the ones consistant to our data.
- Prior information can help us find the plausability of each path.
- The plausability of x knowing y is proportional to the number of
  ways x can produce y * the prior plausability of x.
- Models are built on the way, updated each time we get data that
  confirms or denies it's prior assumptions. They learn with each set
  of plausabilities.
- A model is a mix of a likelihood, which represents the plausability
  of the data given a fixed value as parameter, several parameters and
  a prior (the plausability of each value of the parameters).
- Bayes theorem: P(A∩B) = P(A|B)Pr(B) = P(B|A)Pr(A) 
  aka Posterior = Likelihood*Prior / Average Likelihood
  Pr(A|B) = Pr(B|A)*Pr(A)/(Pr(B|A)*Pr(A))+(Pr(B|A¯)*Pr(A¯))
- Grid approximation uses a finite grid of parameters values and
  scales poorly. When we have several parameters, we use a quadratic
  approximation (to describe the normal shape of the posterior).

***** About samples
- The false positive test: It's a counterintuitive result to a simple
  test. Knowing the amount of people afflicted in the general
  population, the probability for the test to correctly detect the
  illness, and the probability for it to have a false positive result,
  what is the probability, given that the test is positive, that the
  test subject has the illness? It's often lower than expected.
- Working on samples of parameters' values is easier and more
  intuitive, especially when models become complicated.
- Sampling is useful to summarize and understand the posterior
  distribution
- Sampling can also help simulate the model, to check it, or to
  simulate new predictions.

***** About linear and polynomial models
- Normal distributions are extremely used because they are common in
  nature and easy to calculate with.
- Two perspectives: ontological and epistemological. The first sees
  fluctuations, that when summed up make a symmetrical Gaussian
  distribution. For the second, we only know the mean and variance and
  considers Gaussian distribution because it is the most common and
  least surprising (maximu entropy).
- To build a model, we need to know the outcomes, the likelihood, the
  predictors and how they relate to the likelihood and the priors.
- Priors need to be at least a little precise, otherwise they bring
  nothing and the posterior will only depend on the data. Bad priors
  are relatively harmless, only useless.
- Polynomial regression is not advised

***** About multivariate linear models
- Multivariate linear models are much like linear models but with
  several variables. For example, the likelihood y follows a normal
  distribution based on alpha + beta_r x + beta_s z with beta_r and beta_s
  coefficients that measure the association between x and y.
- Sometimes, the association between two variables is quite hidden,
  and so a regression with both is needed to see it. In this case,
  both need to be considered or their relationship with the posterior
  will be hidden.
- Sometimes, adding variables is neither useful nor desirable: it may
  falsify the result of the regression because two predictor variables
  will be highly corrolated (multicollinearity). And including
  post-treatment variables will hide the relationship between the
  outcome and the first variable if there are unobserved confounders
  between the outcome and the post-treatment variable. 

***** About overfitting and regularization
**** STARTED [[https://www.youtube.com/watch?v=BWEtS3HuU5A&list=PLDcUM9US4XdM9_N6XUUFrhghGJ4K25bFc&index=10 ][Listened to several lectures by Richard McElreath]] :VIDEO:STATISTICS:STAN:
:LOGBOOK:  
- State "STARTED"    from "INTERRUPTED" [2019-05-09 jeu. 17:08]
- State "INTERRUPTED" from "STARTED"    [2019-05-06 lun. 18:52]
:END:      
These lectures are available on his youtube channel, and explain in
more detail what his book is about.

*** 2019-05-06 monday
**** Continued to read the book, notes on May 3rd section
**** Talked to Tom about a simple Stan example and notebooks
**** Started to use Stan with simple cases, following Tom's example :STAN:R:

We start by generating some data following a relatively straight line,
with some noise.

#+begin_src R :results output :session *R* :exports both
generate_dataset=function(intercept, coefficient, N, min_x=0, max_x=100, sigma=1){
    x = sample(min_x:max_x,N,replace=T) 
    y = coefficient * x + intercept + rnorm(N,sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}
df=generate_dataset(50, -2, 500, sigma=15)
head(df)
#+end_src

#+RESULTS:
:  
:    x           y
: 1 71  -89.970034
: 2 19   34.653228
: 3 11   47.257220
: 4 84 -105.552794
: 5 34  -12.029547
: 6 30   -2.636534

#+begin_src R :results output graphics :file ./images/ex1_figure.png :exports both :width 600 :height 400 :session *R* 
library(ggplot2)
ggplot(df, aes(x=x, y=y))+geom_point(alpha=0.3)
#+end_src

#+RESULTS:
[[file:./images/ex1_figure.png]]

Then, we define a stan model to find our parameters (intercept,
coefficient and sigma) that we now assume unknown 

#+begin_src R :results output :session *R* :exports both
library(rstan)

modelString = "data { // the observations
    int<lower=1> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real coefficient;
    real<lower=0> sigma; // indication: sigma cannot be negative
} 
model {
    // We define our priors
    intercept   ~ normal(0, 10); // We know that all the parameters follow a normal distribution
    coefficient ~ normal(0, 10);
    sigma       ~ normal(0, 10);

    // Then, our likelihood function
    y ~ normal(coefficient*x + intercept, sigma);
}
"
sm = stan_model(model_code = modelString)

#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example

Inference for Stan model: ea4b5a288cf5f1d87215860103a9026e.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                mean se_mean   sd     2.5%      25%      50%      75%    97.5%
intercept      49.34    0.04 1.36    46.78    48.43    49.33    50.26    52.09
coefficient    -1.99    0.00 0.02    -2.04    -2.00    -1.99    -1.97    -1.94
sigma          15.20    0.01 0.47    14.30    14.88    15.19    15.51    16.14
lp__        -1621.01    0.05 1.27 -1624.18 -1621.57 -1620.68 -1620.06 -1619.56
            n_eff Rhat
intercept    1217 1.01
coefficient  1219 1.01
sigma        1242 1.00
lp__          718 1.01

Samples were drawn using NUTS(diag_e) at Mon May 27 14:29:54 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

As we can see, Stan can find back the parameters given a close enough
model.

#+begin_src R :results output graphics :file ./images/ex1_stan_plot.png :exports both :width 600 :height 400 :session *R* 
stan_plot(fit)
#+end_src

#+RESULTS:
[[file:./images/ex1_stan_plot.png]]

#+begin_src R :results output graphics :file ./images/ex1_stan_trace.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)
#+end_src

#+RESULTS:
[[file:./images/ex1_stan_trace.png]]

#+begin_src R :results output graphics :file ./images/ex1_stan_hist.png :exports both :width 600 :height 400 :session *R* 
stan_hist(fit)
#+end_src

#+RESULTS:
[[file:./images/ex1_stan_hist.png]]

#+begin_src R :results output graphics :file ./images/ex1_stan_dens.png :exports both :width 600 :height 400 :session *R* 
stan_dens(fit) #attention à la densité, elle n'est pas toujours pertinente
#+end_src

#+RESULTS:
[[file:./images/ex1_stan_dens.png]]

*** 2019-05-07 tuesday
**** Search for a way to generate data once Stan has found the parameters :STAN:
- Looked at different packages such as rstanarm but they are too restricted
- Looked at Stan's program blocks
  + The data block lists the data we'll give to Stan. It's possible to
    use restrictions between < > (the data won't be negative...). 
    Within a block, anything declared, can then be used subsequently.
  + The transformed data block is used to create new data based on the
    input data. It has no particular use for this case.
  + The parameters block indicates the parameters that will be estimated
    by Stan.
  + The transformed parameters block includes optional parameters that
    are dependent on the previous parameters.
  + The model block specifies the priors and likelihood
  + Finally, the generated quantities block calculates any data based
    on the model's results, so it might be useful in this case.
**** Tried to use Stan with more complex examples :STAN:R:
***** With noise depending on x

We generate data, this time with noise depending on x. This time,
we're careful not to have any x=0 since that would produce bogus data
for sigma.

#+begin_src R :results output :session *R* :exports both
generate_dataset=function(intercept, coefficient, N, min_x=1, max_x=100, sigma=1){
    x = sample(min_x:max_x,N,replace=T) 
    y = coefficient * x + intercept + rnorm(N,sd=sigma*x)
    df = data.frame(x=x,y=y)
    return(df)
}
df=generate_dataset(12, 50, 500, sigma=15)
head(df)
#+end_src

#+RESULTS:
:  
:    x        y
: 1 17 1200.497
: 2 52 2663.708
: 3 85 4234.701
: 4 42 2494.610
: 5 68 4240.304
: 6 86 2746.118

#+begin_src R :results output graphics :file ./images/ex2_figure.png :exports both :width 600 :height 400 :session *R* 
library(ggplot2)
ggplot(df, aes(x=x, y=y))+geom_point(alpha=0.3)
#+end_src

#+RESULTS:
[[file:./images/ex2_figure.png]]

Then, we define a stan model to find our parameters

#+begin_src R :results output :session *R* :exports both
library(rstan)

modelString = "data { // the observations
    int<lower=1> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real coefficient;
    real<lower=0> sigma; // indication: sigma cannot be negative
} 
model {
    // We define our priors
    intercept   ~ normal(0, 10); // We know that all the parameters follow a normal distribution
    coefficient ~ normal(0, 10);
    sigma       ~ normal(0, 10);

    // Then, our likelihood function
    y ~ normal(coefficient*x + intercept, sigma*x);
}
"
sm = stan_model(model_code = modelString)

#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example

Inference for Stan model: 12d920cec7fd90b16640e34f6f5a767a.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                mean se_mean   sd     2.5%      25%      50%      75%    97.5%
intercept       9.42    0.13 4.87    -0.17     6.16     9.28    12.77    19.16
coefficient    50.18    0.02 0.68    48.86    49.73    50.20    50.65    51.48
sigma          14.42    0.01 0.45    13.60    14.10    14.41    14.71    15.32
lp__        -3438.43    0.04 1.20 -3441.45 -3438.98 -3438.13 -3437.57 -3437.08
            n_eff Rhat
intercept    1364 1.00
coefficient  1629 1.00
sigma        1901 1.00
lp__          933 1.01

Samples were drawn using NUTS(diag_e) at Tue May  7 11:03:57 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

This time, the intercept is farther from its actual value, because of
a high standard deviation in its estimation (4.87). This high
deviation is seen in the plot as a red line.

#+begin_src R :results output graphics :file ./images/ex2_plot.png :exports both :width 600 :height 400 :session *R* 
stan_plot(fit)
#+end_src

#+RESULTS:
[[file:./images/ex2_plot.png]]

#+begin_src R :results output graphics :file ./images/ex2_trace.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)
#+end_src

#+RESULTS:
[[file:./images/ex2_trace.png]]

#+begin_src R :results output graphics :file ./images/ex2_hist.png :exports both :width 600 :height 400 :session *R* 
stan_hist(fit)
#+end_src

#+RESULTS:
[[file:./images/ex2_hist.png]]
*** 2019-05-09 thursday
**** Continued to read the book, notes on May 3rd section
**** Continued watching the MOOC, particularly the first and fourth modules :ORGMODE:
**** DONE An example with generated quantities ([[Generated Quantities][Reworked in friday's entry]]) :STAN:R:
:LOGBOOK:  
- State "DONE"       from "STARTED" [2019-05-09 jeu. 13:51]
- State "STARTED"    from "TODO"       [2019-05-09 jeu. 09:42]
:END:      
As usual we start by generating data. We'll take a simple example.

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(rstan)
generate_dataset=function(intercept, coefficient, N, min_x=0, max_x=100, sigma=1){
    x = sample(min_x:max_x,N,replace=T) 
    y = coefficient * x + intercept + rnorm(N,sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}
df=generate_dataset(12, 3, 500, sigma=15)
head(df)
#+end_src

#+RESULTS:
#+begin_example
   x         y
1 50 162.69930
2 46 160.77595
3 16  60.37135
4 24 100.82896
5  7  35.89649
6 62 198.99881
#+end_example

#+begin_src R :results output graphics :file ./images/ex3_figure.png :exports both :width 600 :height 400 :session *R* 
library(ggplot2)
ggplot(df, aes(x=x, y=y))+geom_point(alpha=0.3)
#+end_src

#+RESULTS:
[[file:./images/ex3_figure.png]]

Then, we define a stan model to find our parameters, and we generate
some y after making the sample.

#+begin_src R :results output :session *R* :exports both
library(rstan)

modelString = "data { // the observations
    int<lower=1> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real coefficient;
    real<lower=0> sigma; // indication: sigma cannot be negative
} 
model {
    // We define our priors
    intercept   ~ normal(0, 10); // We know that all the parameters follow a normal distribution
    coefficient ~ normal(0, 10);
    sigma       ~ normal(0, 10);

    // Then, our likelihood function
    y ~ normal(coefficient*x + intercept, sigma);
}
generated quantities {
  vector[N] y_pos; // posterior predictions
  for (n in 1:N)
    y_pos[n] = normal_rng(coefficient*x[n]+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)

#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
#+end_src

#+begin_src R :results output graphics :file ./images/ex3_plot.png :exports both :width 600 :height 400 :session *R* 
stan_plot(fit, pars = c("intercept", "coefficient", "sigma"))
#+end_src

#+RESULTS:
[[file:./images/ex3_plot.png]]

#+begin_src R :results output graphics :file ./images/ex3_trace.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit, pars = c("intercept", "coefficient", "sigma"))
#+end_src

#+RESULTS:
[[file:./images/ex3_trace.png]]

#+begin_src R :results output graphics :file ./images/ex3_hist.png :exports both :width 600 :height 400 :session *R* 
stan_hist(fit, pars = c("intercept", "coefficient", "sigma"))
#+end_src

#+RESULTS:
[[file:./images/ex3_hist.png]]

Now we look at the generated y

#+begin_src R :results output :session *R* :exports both
extracted=rstan::extract(fit)
mean(extracted$y_pos)
mean(data$y)
#+end_src

#+RESULTS:
: 
: [1] 162.9266
: 
: [1] 162.9518

From this mean, we can say the generated y are reasonably close enough
to the data we generated at the beginning, but not exactly similar of
course, since we have an approximation of the parameters (intercept,
coefficient and sigma). We'll compare the two on a same plot.

#+begin_src R :results output graphics :file ./images/ex3_comparative_plot.png :exports both :width 600 :height 400 :session *R* 
library(matrixStats)
median=colMedians(extracted$y_pos, na.rm = TRUE)
mean=colMeans(extracted$y_pos, na.rm = FALSE)
data2=data.frame(x=data$x,y_pos=mean, y_prev=data$y, y_med=median)

ggplot() + geom_point(data=data2, aes(x=x, y=y_prev), alpha=0.3, color="darkgreen")+ geom_point(data = data2,aes(x = x,y = y_pos), alpha=0.5, color="darkblue") + geom_point(data = data2,aes(x = x,y = y_med), alpha=0.1, color="darkred")
#+end_src

#+RESULTS:
[[file:./images/ex3_comparative_plot.png]]

Surprisingly, the y generated by Stan are very linear compared to the
ones we generated in the beginning. It looks as if the noise wasn't
taken in account. It's probably a result of the colMeans. We looked at
colMedian, to see if the result would be different but it seems it
gives almost the same points.
*** 2019-05-10 friday
**** REWORK <<Generated Quantities>>  :STAN:R:
Following Tom's advice, we changed the model for the generated
quantities test.

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(rstan)
generate_dataset=function(intercept, coefficient, N, min_x=0, max_x=100, sigma=1){
    x = sample(min_x:max_x,N,replace=T) 
    y = coefficient * x + intercept + rnorm(N,sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}
df=generate_dataset(12, 3, 500, sigma=15)
#+end_src

#+begin_src R :results output :session *R* :exports both
library(rstan)

modelString = "data { // the observations
    int<lower=1> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real coefficient;
    real<lower=0> sigma; // indication: sigma cannot be negative
} 
model {
    // We define our priors
    intercept   ~ normal(0, 10); // We know that all the parameters follow a normal distribution
    coefficient ~ normal(0, 10);
    sigma       ~ normal(0, 10);

    // Then, our likelihood function
    y ~ normal(coefficient*x + intercept, sigma);
}
generated quantities {
   real x_pos = x[categorical_rng(rep_vector(1,N) / N)];
   real y_pos; // posterior predictions
   y_pos = normal_rng(coefficient*x_pos+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)

#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
                mean se_mean    sd     2.5%      25%      50%      75%    97.5%
intercept      11.84    0.04  1.30     9.35    10.93    11.85    12.72    14.35
coefficient     2.98    0.00  0.02     2.94     2.97     2.98     3.00     3.03
sigma          14.76    0.01  0.49    13.83    14.42    14.75    15.08    15.74
x_pos          50.89    0.67 29.37     3.00    25.00    51.00    77.00    98.00
y_pos         164.20    2.02 89.09    15.22    87.91   165.14   242.90   308.71
lp__        -1595.10    0.05  1.25 -1598.47 -1595.65 -1594.77 -1594.20 -1593.70
            n_eff Rhat
intercept     831 1.00
coefficient   871 1.00
sigma        1399 1.00
x_pos        1897 1.00
y_pos        1940 1.00
lp__          636 1.01

Samples were drawn using NUTS(diag_e) at Fri May 10 14:05:37 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

Now we look at the generated y

#+begin_src R :results output graphics :file ./images/ex3_reworked_comparative_plot.png :exports both :width 600 :height 400 :session *R* 
extracted=rstan::extract(fit)
df_generated = data.frame(x=extracted$x_pos, y=extracted$y_pos, origin='generated')
df_generated = df_generated[sample(nrow(df_generated), nrow(df)), ]  # there are more points in df_generated, so to have a nicer plot we sample exactly the same number than in df
df$origin = 'initial'
tmp = rbind(df, df_generated)
ggplot(tmp, aes(x=x, y=y, color=origin)) + geom_point(alpha=0.5)
#+end_src

#+RESULTS:
[[file:./images/ex3_reworked_comparative_plot.png]]

This way we don't have to make a mean that erases the noise's effect,
and the generated y could definitely be used for further calculation. 

**** Tried to use Stan with more complex examples :STAN:R:
***** With a full polynomial model ([[Polynomial Model][Reworked in tuesday's entry]]):
We'll assume for this one that the noise follows a simple model, to
make it easier. We generate data, this time with three
parameters. We'll call them second_ degree, first_ degree and intercept.

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(rstan)

generate_dataset=function(intercept, second_degree, first_degree, N, min_x=0, max_x=100, sigma){
    x = sample(min_x:max_x,N,replace=T) 
    y = second_degree * x^2 + first_degree * x + intercept + rnorm(N, sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}

df=generate_dataset(100,2,6, 500, sigma=150)
head(df)
#+end_src

#+RESULTS:
#+begin_example
   x          y
1  9   419.2124
2 90 16933.1164
3 93 18001.0911
4 89 16522.1687
5 20  1013.0832
6  9   534.0954
#+end_example

#+begin_src R :results output graphics :file ./images/ex4_figure.png :exports both :width 600 :height 400 :session *R* 
ggplot(df, aes(x=x, y=y))+geom_point(alpha=0.3, color="darkblue")
#+end_src

#+RESULTS:
[[file:./images/ex4_figure.png]]

#+begin_src R :results output :session *R* :exports both
modelString = "data { // the observations
    int<lower=0> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real second_degree;
    real first_degree;
    real<lower=0> sigma; // indication: sigma cannot be negative
}

model {
     // We define our priors
    second_degree ~ normal(0, 10);
    first_degree ~ normal(0, 10);
    intercept   ~ normal(0, 10);
    sigma       ~ normal(0, 10);
    // Then, our likelihood function
    for (n in 1:(N)){
      y ~ normal(second_degree*(x[n]*x[n]) + first_degree*x[n] + intercept, sigma);
    }
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Warning messages:
1: There were 155 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
2: There were 8 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
3: Examine the pairs() plot to diagnose sampling problems

Inference for Stan model: e6c4a717223c83fc91c9c4018606219b.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                     mean se_mean      sd        2.5%         25%         50%
intercept          216.01  170.46  347.33        1.02        5.15       16.60
second_degree       -2.18    0.06    0.11       -2.26       -2.24       -2.24
first_degree       262.91    6.67   13.61      237.07      264.29      270.41
sigma             4715.81   10.28   21.48     4672.10     4713.75     4725.89
lp__          -2463195.40  827.57 1668.03 -2464315.63 -2464275.36 -2464166.68
                      75%       97.5% n_eff  Rhat
intercept          184.50      864.26     4  6.60
second_degree       -2.19       -1.96     4  5.75
first_degree       271.17      272.17     4  6.17
sigma             4729.56     4735.32     4  3.92
lp__          -2462778.64 -2460240.49     4 12.27

Samples were drawn using NUTS(diag_e) at Fri May 10 12:34:16 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

The sampling issued several warnings, which isn't surprising because
the parameters found by Stan don't match at all with the ones we chose
to generate the data.

#+begin_src R :results output graphics :file ./images/ex4_trace.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit, pars = c("intercept", "second_degree", "first_degree", "sigma"))+ scale_color_manual(values = blues9)
#+end_src

#+RESULTS:
[[file:./images/ex4_trace.png]]

If we look at the trace it becomes clear that our model must have been
wrong, because the chains don't converge towards each other at
all. It's apparent polynomial regression cannot be done with a
model based on linear regression and only slightly changed.

Stan's warnings advises us to look at the pairs plot.

#+begin_src R :results output graphics :file ./images/ex4_pairs.png :exports both :width 600 :height 400 :session *R* 
pairs(fit)
#+end_src

#+RESULTS:
[[file:./images/ex4_pairs.png]]

Apparently, there should be a negative relationship between lp__ and
energy__, however this plot is quite difficult to understand without
indication.

***** EDIT 2019-05-14 Changed the model following Tom's advice

Tom figured out a mistake in the declaration of the likelihood
function.

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,2,6, 500, sigma=150)

modelString = "data { // the observations
    int<lower=0> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma; // indication: sigma cannot be negative
}

model {
    second_degree ~ normal(0, 10);
    first_degree ~ normal(0, 10);
    intercept   ~ normal(0, 10);
    sigma       ~ normal(0, 10);
    // Then, our likelihood function
    for (n in 1:(N)){
      y[n] ~ normal(second_degree*(x[n]*x[n]) + first_degree*x[n] + intercept, sigma);
    }
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 10461bf4fda3fe871ad7e125904745bb.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean   sd     2.5%      25%      50%      75%
intercept        28.85    0.46 8.98    12.14    22.61    28.56    34.55
first_degree      9.43    0.02 0.55     8.36     9.07     9.44     9.80
second_degree     1.97    0.00 0.01     1.96     1.97     1.97     1.97
sigma           134.36    0.09 3.26   128.36   132.07   134.29   136.65
lp__          -2881.15    0.07 1.43 -2884.70 -2881.83 -2880.84 -2880.10
                 97.5% n_eff Rhat
intercept        47.97   381 1.01
first_degree     10.50   577 1.01
second_degree     1.98   740 1.00
sigma           140.60  1293 1.00
lp__          -2879.40   479 1.01

Samples were drawn using NUTS(diag_e) at Tue May 14 14:42:59 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

Surprisingly, despite the correction, the results are still wildly
off. We think the outcome might be influenced by faulty priors, so we
try removing them.

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,2,6, 500, sigma=150)

modelString = "data { // the observations
    int<lower=0> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma; // indication: sigma cannot be negative
}

model {
    // Then, our likelihood function
    for (n in 1:(N)){
      y[n] ~ normal(second_degree*(x[n]*x[n]) + first_degree*x[n] + intercept, sigma);
    }
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: d2b16cc8b2214d87ccd7963b321901c3.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean    sd     2.5%      25%      50%      75%
intercept       105.29    1.07 17.86    71.14    92.35   105.77   117.64
first_degree      5.66    0.05  0.88     4.01     5.05     5.64     6.29
second_degree     2.00    0.00  0.01     1.99     2.00     2.00     2.01
sigma           148.09    0.17  4.88   139.01   144.77   147.95   151.36
lp__          -2743.12    0.05  1.43 -2746.50 -2743.85 -2742.79 -2742.08
                 97.5% n_eff Rhat
intercept       139.21   278 1.02
first_degree      7.34   331 1.02
second_degree     2.02   476 1.01
sigma           158.16   805 1.01
lp__          -2741.28   680 1.01

Samples were drawn using NUTS(diag_e) at Tue May 14 14:49:24 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

This time the results are much closer to the truth, and very similar
to the results we found with the second method (on monday's 13th
entry). The conclusion is that we need to find more precise priors,
or give a very general one (or none) so it won't influence the outcome
in a way we don't want it to.

**** Continued to read the book, notes on May 3rd section
Surprisingly, the part on polynomial regression is very sparse and
doesn't really help us make a functional model.
*** 2019-05-13 monday
**** Continued to read the book, notes on May 3rd section
**** A second try with a polynomial model :STAN:R:
We still have a simple model for the noise, and three parameters:
second_ degree, first_ degree and intercept.

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(rstan)

generate_dataset=function(intercept, first_degree, second_degree, N, min_x=0, max_x=100, sigma){
    x = sample(min_x:max_x,N,replace=T) 
    y = second_degree * (x^2) + first_degree * x + intercept + rnorm(N, sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}

df=generate_dataset(100,6,2, 500, sigma=150)
x2=df$x^2 #Need to add it here because we cannot in the stan model
head(df)
#+end_src

#+RESULTS:
#+begin_example
   x          y
1  0   257.9807
2 45  4353.7896
3 65  8872.7711
4 91 17196.1885
5 86 15519.8198
6 42  3911.2678
#+end_example

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] x2;
    vector[N] y;
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}

model {
    // We use no prior, as the previous ones are too vague to be useful for computing the posterior
    y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,x2=x2,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:

#+begin_example
Inference for Stan model: 6001096c2a1f8ca4aec729de06bdbead.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean    sd     2.5%      25%      50%      75%
intercept        74.78    0.87 18.35    39.43    61.72    74.65    87.40
first_degree      6.94    0.04  0.88     5.18     6.36     6.94     7.53
second_degree     1.99    0.00  0.01     1.97     1.98     1.99     1.99
sigma           150.35    0.14  4.71   141.63   146.97   150.29   153.73
lp__          -2751.41    0.05  1.45 -2755.14 -2752.10 -2751.08 -2750.32
                 97.5% n_eff Rhat
intercept       111.46   448 1.01
first_degree      8.70   501 1.01
second_degree     2.01   597 1.01
sigma           159.24  1214 1.00
lp__          -2749.61   713 1.01

Samples were drawn using NUTS(diag_e) at Tue May 14 11:28:37 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

This time, the parameters found by Stan are quite close to the ones we
chose to generate the data, but it's still quite inaccurate,
especially for the intercept.

#+begin_src R :results output graphics :file ./images/ex4_reworked_trace.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)+ scale_color_manual(values = blues9)
#+end_src

#+RESULTS:
[[file:./images/ex4_reworked_trace.png]]

At least this time the chains converge towards each other, so we can
conclude this model is better than the previous but still needs to be
worked on to correct some inaccuracies (we probably need to add some
informative priors).

The surprising part is that depending on the value of sigma, the trace
can show a very different behavior. For example, with a small value of
sigma:

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,6,2, 500, sigma=4)
x2=df$x^2
modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] x2;
    vector[N] y;
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}

model {
    // We use no prior, as the previous ones are too vague to be useful for computing the posterior
    y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
data = list(N=nrow(df),x=df$x,x2=x2,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:

#+begin_example
Warning messages:
1: There were 229 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: There were 1 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
3: There were 4 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
4: Examine the pairs() plot to diagnose sampling problems

Inference for Stan model: 6001096c2a1f8ca4aec729de06bdbead.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean     sd     2.5%      25%     50%     75%   97.5%
intercept        93.74    7.03  15.26    57.08    89.96  100.45  100.97  103.14
first_degree      6.25    0.28   0.61     5.86     5.96    5.98    6.40    7.69
second_degree     2.00    0.00   0.01     1.99     2.00    2.00    2.00    2.00
sigma             6.33    1.70   3.89     3.68     3.87    4.81    7.13   15.05
lp__          -1056.63  103.39 215.81 -1592.99 -1110.91 -951.71 -921.93 -920.60
              n_eff Rhat
intercept         5 3.37
first_degree      5 3.28
second_degree     5 3.22
sigma             5 2.75
lp__              4 4.73

Samples were drawn using NUTS(diag_e) at Tue May 14 11:30:45 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/ex4_reworked_trace_small_sigma.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)+ scale_color_manual(values = blues9)
#+end_src

#+RESULTS:
[[file:./images/ex4_reworked_trace_small_sigma.png]]

This time the chains don't converge again, despite the fact that the
model is still the same.

If we try with a much higher value of sigma, the chains do converge
but the found parameters have a higher standard deviation for their
value (which is not surprising).

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,6,2, 500, sigma=986)
x2=df$x^2
modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] x2;
    vector[N] y;
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}

model {
    // We use no prior, as the previous ones are too vague to be useful for computing the posterior
    y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
data = list(N=nrow(df),x=df$x,x2=x2,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 6001096c2a1f8ca4aec729de06bdbead.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean     sd     2.5%      25%      50%      75%
intercept        54.93    5.63 132.23  -202.71   -34.30    49.64   143.42
first_degree      5.57    0.27   5.96    -6.25     1.39     5.94     9.69
second_degree     2.02    0.00   0.06     1.91     1.98     2.01     2.06
sigma          1006.73    1.16  32.68   945.09   984.52  1006.43  1028.69
lp__          -3700.35    0.06   1.50 -3704.02 -3701.05 -3700.00 -3699.23
                 97.5% n_eff Rhat
intercept       318.61   552 1.01
first_degree     16.65   501 1.01
second_degree     2.13   556 1.01
sigma          1071.30   792 1.01
lp__          -3698.51   672 1.00

Samples were drawn using NUTS(diag_e) at Tue May 14 11:32:12 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

As we can see here, the parameters are much too inaccurate to model
properly the situation.

#+begin_src R :results output graphics :file ./images/ex4_reworked_trace_high_sigma.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)+ scale_color_manual(values = blues9)
#+end_src

#+RESULTS:
[[file:./images/ex4_reworked_trace_high_sigma.png]]

**** A look at generated quantities with a polynomial model :STAN:R:

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,6,2, 500, sigma=125)
x2=df$x^2
modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] x2;
    vector[N] y;
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}

model {
    y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
}
generated quantities {
   real x_pos = x[categorical_rng(rep_vector(1,N) / N)];
   real y_pos; // posterior predictions
   y_pos = normal_rng(second_degree*(x_pos^2)+first_degree*x_pos+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
data = list(N=nrow(df),x=df$x,x2=x2,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
#+end_src

#+begin_src R :results output graphics :file ./images/ex4_reworked_comparative_plot.png :exports both :width 600 :height 400 :session *R* 
extracted=rstan::extract(fit)
df_generated = data.frame(x=extracted$x_pos, y=extracted$y_pos, origin='generated')
df_generated = df_generated[sample(nrow(df_generated), nrow(df)), ]
df$origin = 'initial'
tmp = rbind(df, df_generated)
ggplot(tmp, aes(x=x, y=y, color=origin)) + geom_point(alpha=0.6)
#+end_src

#+RESULTS:
[[file:./images/ex4_reworked_comparative_plot.png]]

Once again, the data generated from the parameters found by stan seems
quite precise and close enough to the data we generated at the
beginning.
*** 2019-05-14 tuesday
**** REWORK <<Polynomial Model>>
***** A third model
We already tried two different models for this one; the first ended up
with unprecise parameters found, and so did the second one which also
had the disadvantage to need the vector of x squared as data.

Following Tom's advice, we try to do this in the transformed data
block.

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,2,6, 500, sigma=150)

modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] y;
}
transformed data{
    vector[N] x2;
    for (n in 1:N)
      x2[n]=x[n]*x[n];
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}
model {
    y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 91966ecf1347dea6d1c3f249ec4a3d80.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean    sd     2.5%      25%      50%      75%
intercept        96.08    1.35 20.83    57.95    81.53    95.94   110.73
first_degree      5.73    0.05  0.97     3.81     5.07     5.72     6.38
second_degree     2.00    0.00  0.01     1.98     2.00     2.00     2.01
sigma           153.74    0.14  4.77   144.75   150.37   153.67   157.03
lp__          -2761.46    0.07  1.39 -2764.76 -2762.21 -2761.16 -2760.42
                 97.5% n_eff Rhat
intercept       135.65   239 1.01
first_degree      7.61   317 1.01
second_degree     2.02   420 1.01
sigma           163.07  1205 1.00
lp__          -2759.69   435 1.02

Samples were drawn using NUTS(diag_e) at Tue May 14 15:44:11 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

Once again, this new model doesn't change the precision of the results
but it is perhaps easier to understand and use.

***** DONE Figuring out the right priors
:LOGBOOK:  
- State "DONE"       from "STARTED" [2019-05-16 jeu. 10:46]
- State "STARTED"    from "TODO"       [2019-05-15 mer. 16:06]
:END:      

With the help of [[http://modernstatisticalworkflow.blogspot.com/2017/04/an-easy-way-to-simulate-fake-data-from.htm][An easy way to simulate fake data from your Stan
model]], and other articles by Jim Savage, as well as [[https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations ][Stan's wiki entry
on priors]], we took a closer look at our priors, which used to be:
variable ~ normal(0,10). However we ran into several issues with that
prior: not only is it not precise enough, it also can push the
posterior in a direction we don't want, and we would end up with wrong
results. Therefore, we made a model that let us look more closely at
the influence of priors.

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(rstan)

generate_dataset=function(intercept, second_degree, first_degree, N, min_x=0, max_x=100, sigma){
    x = sample(min_x:max_x,N,replace=T) 
    y = second_degree * x^2 + first_degree * x + intercept + rnorm(N, sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}
df=generate_dataset(100,2,6, 500, sigma=150)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] y;
    int<lower = 0, upper = 1> run_estimation; // a switch to evaluate the likelihood
    real it_mean; //the center of the intercept's distribution
    real it_sd; //the variance of the intercept's distribution
    real f_degree_mean; //the center of the first degree's distribution
    real s_degree_mean; //the center of the second degree's distribution
    real degree_sd;  //the variance of the first and second degrees's distribution
    real sigma_mean; //the center of the sigma's distribution
    real sigma_sd; //the variance of the sigma's distribution
}
transformed data{
    vector[N] x2;
    for (n in 1:N)
      x2[n]=x[n]*x[n];
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}
model {
    intercept ~ normal(it_mean, it_sd);
    first_degree ~ normal(f_degree_mean, degree_sd);
    second_degree ~ normal(s_degree_mean, degree_sd);
    sigma ~ normal(sigma_mean, sigma_sd);
    if(run_estimation==1){ //we have two simulation modes: if run_estimation=0 then the posterior is the prior, else the simulation is the same as before
      y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
    }
}

generated quantities {
  real x_pos = x[categorical_rng(rep_vector(1,N) / N)];
  real y_sim;
  y_sim = normal_rng(second_degree*(x_pos^2)+first_degree*x_pos+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y,run_estimation=0,it_mean=1,it_sd=10,f_degree_mean=1,s_degree_mean=1,degree_sd=10,sigma_mean=1,sigma_sd=10)
fit = sampling(sm,data=data, iter=500, chains=8)
#+end_src

#+RESULTS:
#+begin_example

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 8e-06 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 1: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 1: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 1: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 1: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 1: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 1: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 1: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 1: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 1: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 1: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 1: Iteration: 500 / 500 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.01738 seconds (Warm-up)
Chain 1:                0.005843 seconds (Sampling)
Chain 1:                0.023223 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 4e-06 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 2: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 2: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 2: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 2: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 2: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 2: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 2: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 2: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 2: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 2: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 2: Iteration: 500 / 500 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.015096 seconds (Warm-up)
Chain 2:                0.005844 seconds (Sampling)
Chain 2:                0.02094 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 4e-06 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 3: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 3: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 3: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 3: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 3: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 3: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 3: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 3: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 3: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 3: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 3: Iteration: 500 / 500 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.01803 seconds (Warm-up)
Chain 3:                0.007696 seconds (Sampling)
Chain 3:                0.025726 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 6e-06 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 4: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 4: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 4: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 4: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 4: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 4: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 4: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 4: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 4: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 4: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 4: Iteration: 500 / 500 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.017028 seconds (Warm-up)
Chain 4:                0.005188 seconds (Sampling)
Chain 4:                0.022216 seconds (Total)
Chain 4: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 5).
Chain 5: 
Chain 5: Gradient evaluation took 4e-06 seconds
Chain 5: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 5: Adjust your expectations accordingly!
Chain 5: 
Chain 5: 
Chain 5: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 5: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 5: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 5: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 5: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 5: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 5: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 5: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 5: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 5: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 5: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 5: Iteration: 500 / 500 [100%]  (Sampling)
Chain 5: 
Chain 5:  Elapsed Time: 0.017953 seconds (Warm-up)
Chain 5:                0.004923 seconds (Sampling)
Chain 5:                0.022876 seconds (Total)
Chain 5: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 6).
Chain 6: 
Chain 6: Gradient evaluation took 4e-06 seconds
Chain 6: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 6: Adjust your expectations accordingly!
Chain 6: 
Chain 6: 
Chain 6: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 6: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 6: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 6: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 6: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 6: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 6: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 6: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 6: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 6: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 6: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 6: Iteration: 500 / 500 [100%]  (Sampling)
Chain 6: 
Chain 6:  Elapsed Time: 0.017574 seconds (Warm-up)
Chain 6:                0.004908 seconds (Sampling)
Chain 6:                0.022482 seconds (Total)
Chain 6: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 7).
Chain 7: 
Chain 7: Gradient evaluation took 4e-06 seconds
Chain 7: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 7: Adjust your expectations accordingly!
Chain 7: 
Chain 7: 
Chain 7: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 7: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 7: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 7: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 7: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 7: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 7: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 7: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 7: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 7: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 7: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 7: Iteration: 500 / 500 [100%]  (Sampling)
Chain 7: 
Chain 7:  Elapsed Time: 0.017471 seconds (Warm-up)
Chain 7:                0.004945 seconds (Sampling)
Chain 7:                0.022416 seconds (Total)
Chain 7: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 8).
Chain 8: 
Chain 8: Gradient evaluation took 4e-06 seconds
Chain 8: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 8: Adjust your expectations accordingly!
Chain 8: 
Chain 8: 
Chain 8: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 8: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 8: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 8: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 8: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 8: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 8: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 8: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 8: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 8: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 8: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 8: Iteration: 500 / 500 [100%]  (Sampling)
Chain 8: 
Chain 8:  Elapsed Time: 0.01777 seconds (Warm-up)
Chain 8:                0.00615 seconds (Sampling)
Chain 8:                0.02392 seconds (Total)
Chain 8: 
Warning messages:
1: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: Examine the pairs() plot to diagnose sampling problems
#+end_example

#+begin_src R :results output :session *R* :exports both
print(fit, pars=c("intercept", "first_degree", "second_degree", "sigma"))
y_sim = extract(fit, pars = "y_sim")
mean(df$y)
mean(y_sim$y_sim)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: ba67be2a2b0f008b97473baf3d15d10d.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

              mean se_mean    sd   2.5%   25%  50%   75% 97.5% n_eff Rhat
intercept     0.75    0.24 10.04 -19.48 -6.00 0.89  7.50 20.32  1738    1
first_degree  1.21    0.24  9.88 -17.40 -5.76 1.08  7.73 21.74  1757    1
second_degree 0.99    0.26 10.01 -18.16 -5.82 0.80  7.93 19.88  1485    1
sigma         8.48    0.16  6.24   0.32  3.52 7.44 12.07 22.49  1462    1

Samples were drawn using NUTS(diag_e) at Thu May 16 13:50:33 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
 
[1] 7144.907

[1] 3530.052
#+end_example

#+begin_src R :results output graphics :file ./images/generated_data_vague_prior.png :exports both :width 600 :height 400 :session *R* 
extracted=rstan::extract(fit)
df_generated = data.frame(x=extracted$x_pos, y=extracted$y_sim, origin='generated')
df_generated = df_generated[sample(nrow(df_generated), nrow(df)), ]  # there are more points in df_generated, so to have a nicer plot we sample exactly the same number than in df
df$origin = 'initial'
tmp = rbind(df, df_generated)
ggplot(tmp, aes(x=x, y=y, color=origin)) + geom_point(alpha=0.5)
#+end_src

#+RESULTS:
[[file:./images/generated_data_vague_prior.png]]

This way we can see more easily the distribution of the previously
given priors, and how that makes the generated data
inaccurate. Indeed, the parameters are really far from their actual
value, and so the generated data y_ sim is completely off.

To figure out the right priors, the best way would be to approximate
the value of our parameters and give that approximation as the prior.

#+begin_src R :results output :session *R* :exports both
mean_0=0 #a vector of means of y values for x=0
mean_10=0 #a vector of means of y values for x=10

for(x in 1:1000){ #we run 1000 simulations to get a pretty accurate approximation
  df=generate_dataset(100,2,6, 500, sigma=150)
  #for each simulation we get the mean of y values for x=0 and x=10
  mean_0[x]=mean(df$y[which(df$x %in% 0)])
  mean_10[x]=mean(df$y[which(df$x %in% 10)])
}

sd_intercept=abs(mean(diff(mean_0),na.rm = T))
#we get the difference between each means and we makes a mean of it to get a sd for the intercept.
#we use the absolute value for it to be positive.

sd_intercept
abs(mean(diff(mean_10),na.rm = T))

mean_intercept=round(mean(mean_0,na.rm = T)) 
#we make a mean of 1000 observations of the values at x=0 (so y=intercept)

intercept=rnorm(500,mean_intercept,sd_intercept)
#we simulate the intercept to remove it from the y values at x=10

mean_10=mean_10-intercept
#we removed the value of the intercept so y=first_degree*10+second_degree*100 

mean_sec= mean(mean_10,na.rm = T)%/%100 
#we divide the mean by 100 and store only the integer part to get the value of the second_degree

mean_first=mean(mean_10,na.rm = T)%%100 %/%10 
#this time we get the remainder, and divide it by ten to get the value of the first_degree

#+end_src

#+RESULTS:
:  
: [1] 0.1241611
: 
: [1] 0.03740519

As we can see, the values of the sd are really small, and can be
replaced by a sd of 1. Now we run our model again but with the
approximated means of intercept, first-degree and second-degree.

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y,run_estimation=0,it_mean=mean_intercept,it_sd=1,f_degree_mean=mean_first,s_degree_mean=mean_sec,degree_sd=1,sigma_mean=0,sigma_sd=10)

fit = sampling(sm,data=data, iter=500, chains=8)
#+end_src

#+RESULTS:
#+begin_example
 
SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 5e-06 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 1: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 1: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 1: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 1: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 1: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 1: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 1: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 1: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 1: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 1: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 1: Iteration: 500 / 500 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.010822 seconds (Warm-up)
Chain 1:                0.005554 seconds (Sampling)
Chain 1:                0.016376 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 6e-06 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 2: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 2: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 2: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 2: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 2: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 2: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 2: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 2: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 2: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 2: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 2: Iteration: 500 / 500 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.009115 seconds (Warm-up)
Chain 2:                0.007346 seconds (Sampling)
Chain 2:                0.016461 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 6e-06 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 3: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 3: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 3: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 3: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 3: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 3: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 3: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 3: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 3: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 3: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 3: Iteration: 500 / 500 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.009098 seconds (Warm-up)
Chain 3:                0.007334 seconds (Sampling)
Chain 3:                0.016432 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 5e-06 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 4: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 4: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 4: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 4: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 4: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 4: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 4: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 4: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 4: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 4: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 4: Iteration: 500 / 500 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.012249 seconds (Warm-up)
Chain 4:                0.00712 seconds (Sampling)
Chain 4:                0.019369 seconds (Total)
Chain 4: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 5).
Chain 5: 
Chain 5: Gradient evaluation took 4e-06 seconds
Chain 5: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 5: Adjust your expectations accordingly!
Chain 5: 
Chain 5: 
Chain 5: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 5: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 5: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 5: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 5: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 5: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 5: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 5: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 5: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 5: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 5: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 5: Iteration: 500 / 500 [100%]  (Sampling)
Chain 5: 
Chain 5:  Elapsed Time: 0.006398 seconds (Warm-up)
Chain 5:                0.004923 seconds (Sampling)
Chain 5:                0.011321 seconds (Total)
Chain 5: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 6).
Chain 6: 
Chain 6: Gradient evaluation took 4e-06 seconds
Chain 6: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 6: Adjust your expectations accordingly!
Chain 6: 
Chain 6: 
Chain 6: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 6: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 6: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 6: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 6: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 6: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 6: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 6: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 6: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 6: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 6: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 6: Iteration: 500 / 500 [100%]  (Sampling)
Chain 6: 
Chain 6:  Elapsed Time: 0.00574 seconds (Warm-up)
Chain 6:                0.007349 seconds (Sampling)
Chain 6:                0.013089 seconds (Total)
Chain 6: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 7).
Chain 7: 
Chain 7: Gradient evaluation took 5e-06 seconds
Chain 7: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
Chain 7: Adjust your expectations accordingly!
Chain 7: 
Chain 7: 
Chain 7: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 7: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 7: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 7: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 7: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 7: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 7: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 7: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 7: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 7: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 7: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 7: Iteration: 500 / 500 [100%]  (Sampling)
Chain 7: 
Chain 7:  Elapsed Time: 0.009162 seconds (Warm-up)
Chain 7:                0.008283 seconds (Sampling)
Chain 7:                0.017445 seconds (Total)
Chain 7: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 8).
Chain 8: 
Chain 8: Gradient evaluation took 5e-06 seconds
Chain 8: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
Chain 8: Adjust your expectations accordingly!
Chain 8: 
Chain 8: 
Chain 8: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 8: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 8: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 8: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 8: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 8: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 8: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 8: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 8: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 8: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 8: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 8: Iteration: 500 / 500 [100%]  (Sampling)
Chain 8: 
Chain 8:  Elapsed Time: 0.008495 seconds (Warm-up)
Chain 8:                0.009114 seconds (Sampling)
Chain 8:                0.017609 seconds (Total)
Chain 8:
#+end_example

#+begin_src R :results output :session *R* :exports both
y_sim = extract(fit, pars = "y_sim")
mean(df$y)
mean(y_sim$y_sim)
print(fit,pars=c("intercept","first_degree","second_degree","sigma"))
stan_trace(fit,pars=c("intercept","first_degree","second_degree","sigma"))
#+end_src

#+RESULTS:
#+begin_example

[1] 6668.037

[1] 6612.556

Inference for Stan model: ba67be2a2b0f008b97473baf3d15d10d.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
intercept     102.01    0.03 1.00 100.09 101.33 101.98 102.64 104.04  1604    1
first_degree    5.99    0.02 0.99   4.16   5.27   5.97   6.70   7.89  1572    1
second_degree   2.00    0.03 1.01   0.00   1.32   2.03   2.70   3.88  1272    1
sigma           7.95    0.19 6.17   0.25   3.06   6.69  11.60  23.05  1108    1

Samples were drawn using NUTS(diag_e) at Thu May 16 13:51:42 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

Now this looks much better! The chains converge towards each other as
they should, and the generated data from the parameters looks quite
close to the actual data. Let's run our model for real this time.

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y,run_estimation=1,it_mean=mean_intercept,it_sd=1,f_degree_mean=mean_first,s_degree_mean=mean_sec,degree_sd=1,sigma_mean=150,sigma_sd=1)

fit = sampling(sm,data=data, iter=500, chains=8)
#+end_src

#+RESULTS:
#+begin_example
 
SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.00013 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.3 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 1: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 1: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 1: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 1: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 1: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 1: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 1: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 1: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 1: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 1: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 1: Iteration: 500 / 500 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.65015 seconds (Warm-up)
Chain 1:                0.240311 seconds (Sampling)
Chain 1:                0.890461 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 6.5e-05 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.65 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 2: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 2: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 2: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 2: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 2: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 2: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 2: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 2: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 2: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 2: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 2: Iteration: 500 / 500 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.730258 seconds (Warm-up)
Chain 2:                1.27852 seconds (Sampling)
Chain 2:                2.00877 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 8.7e-05 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.87 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 3: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 3: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 3: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 3: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 3: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 3: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 3: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 3: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 3: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 3: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 3: Iteration: 500 / 500 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.392354 seconds (Warm-up)
Chain 3:                0.529003 seconds (Sampling)
Chain 3:                0.921357 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 8.3e-05 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.83 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 4: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 4: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 4: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 4: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 4: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 4: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 4: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 4: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 4: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 4: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 4: Iteration: 500 / 500 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.409167 seconds (Warm-up)
Chain 4:                0.626242 seconds (Sampling)
Chain 4:                1.03541 seconds (Total)
Chain 4: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 5).
Chain 5: 
Chain 5: Gradient evaluation took 6.4e-05 seconds
Chain 5: 1000 transitions using 10 leapfrog steps per transition would take 0.64 seconds.
Chain 5: Adjust your expectations accordingly!
Chain 5: 
Chain 5: 
Chain 5: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 5: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 5: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 5: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 5: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 5: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 5: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 5: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 5: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 5: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 5: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 5: Iteration: 500 / 500 [100%]  (Sampling)
Chain 5: 
Chain 5:  Elapsed Time: 0.427934 seconds (Warm-up)
Chain 5:                0.283541 seconds (Sampling)
Chain 5:                0.711475 seconds (Total)
Chain 5: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 6).
Chain 6: 
Chain 6: Gradient evaluation took 8.5e-05 seconds
Chain 6: 1000 transitions using 10 leapfrog steps per transition would take 0.85 seconds.
Chain 6: Adjust your expectations accordingly!
Chain 6: 
Chain 6: 
Chain 6: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 6: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 6: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 6: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 6: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 6: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 6: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 6: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 6: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 6: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 6: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 6: Iteration: 500 / 500 [100%]  (Sampling)
Chain 6: 
Chain 6:  Elapsed Time: 0.870979 seconds (Warm-up)
Chain 6:                0.562292 seconds (Sampling)
Chain 6:                1.43327 seconds (Total)
Chain 6: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 7).
Chain 7: 
Chain 7: Gradient evaluation took 6.4e-05 seconds
Chain 7: 1000 transitions using 10 leapfrog steps per transition would take 0.64 seconds.
Chain 7: Adjust your expectations accordingly!
Chain 7: 
Chain 7: 
Chain 7: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 7: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 7: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 7: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 7: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 7: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 7: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 7: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 7: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 7: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 7: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 7: Iteration: 500 / 500 [100%]  (Sampling)
Chain 7: 
Chain 7:  Elapsed Time: 1.0137 seconds (Warm-up)
Chain 7:                0.291937 seconds (Sampling)
Chain 7:                1.30564 seconds (Total)
Chain 7: 

SAMPLING FOR MODEL 'ba67be2a2b0f008b97473baf3d15d10d' NOW (CHAIN 8).
Chain 8: 
Chain 8: Gradient evaluation took 6.5e-05 seconds
Chain 8: 1000 transitions using 10 leapfrog steps per transition would take 0.65 seconds.
Chain 8: Adjust your expectations accordingly!
Chain 8: 
Chain 8: 
Chain 8: Iteration:   1 / 500 [  0%]  (Warmup)
Chain 8: Iteration:  50 / 500 [ 10%]  (Warmup)
Chain 8: Iteration: 100 / 500 [ 20%]  (Warmup)
Chain 8: Iteration: 150 / 500 [ 30%]  (Warmup)
Chain 8: Iteration: 200 / 500 [ 40%]  (Warmup)
Chain 8: Iteration: 250 / 500 [ 50%]  (Warmup)
Chain 8: Iteration: 251 / 500 [ 50%]  (Sampling)
Chain 8: Iteration: 300 / 500 [ 60%]  (Sampling)
Chain 8: Iteration: 350 / 500 [ 70%]  (Sampling)
Chain 8: Iteration: 400 / 500 [ 80%]  (Sampling)
Chain 8: Iteration: 450 / 500 [ 90%]  (Sampling)
Chain 8: Iteration: 500 / 500 [100%]  (Sampling)
Chain 8: 
Chain 8:  Elapsed Time: 0.638479 seconds (Warm-up)
Chain 8:                0.317678 seconds (Sampling)
Chain 8:                0.956157 seconds (Total)
Chain 8: 
Warning messages:
1: There were 1 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
2: Examine the pairs() plot to diagnose sampling problems
#+end_example

#+begin_src R :results output :session *R* :exports both
y_sim = extract(fit, pars = "y_sim")
mean(df$y)
mean(y_sim$y_sim)
print(fit,pars=c("intercept","first_degree","second_degree","sigma"))
#+end_src

#+RESULTS:
#+begin_example

[1] 6668.037

[1] 6640.834

Inference for Stan model: ba67be2a2b0f008b97473baf3d15d10d.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
intercept     100.97    1.30 7.10  97.79 101.36 102.02 102.66 103.96    30 1.25
first_degree    5.43    0.02 0.43   4.57   5.12   5.43   5.73   6.32   534 1.00
second_degree   2.00    0.00 0.01   1.99   2.00   2.00   2.01   2.01   550 1.00
sigma         150.28    0.03 0.96 148.35 149.64 150.31 150.93 152.12  1197 1.00

Samples were drawn using NUTS(diag_e) at Thu May 16 13:52:14 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

And now with the data taken in account, the found parameters are much
more accurate than before, and this shows in the generated data y-sim.

#+begin_src R :results output graphics :file ./images/generated_data_precise_prior.png :exports both :width 600 :height 400 :session *R* 
extracted=rstan::extract(fit)
df_generated = data.frame(x=extracted$x_pos, y=extracted$y_sim, origin='generated')
df_generated = df_generated[sample(nrow(df_generated), nrow(df)), ]  # there are more points in df_generated, so to have a nicer plot we sample exactly the same number than in df
df$origin = 'initial'
tmp = rbind(df, df_generated)
ggplot(tmp, aes(x=x, y=y, color=origin)) + geom_point(alpha=0.5)
#+end_src

#+RESULTS:
[[file:./images/generated_data_precise_prior.png]]

*** 2019-05-16 thursday
**** Attended the seminar about how to use Big Data Solutions to Improve HPC systems by Thomas Ropars :SEMINAR:
***** Introduction
Supercomputers are producing large amount of data that need to be
analyzed. They produce mostly two kinds of data: scientific data and
monitoring data. Scientific data are the results of the execution of
numerical simulations and need to be analyzed to extract knowledge. 
Monitoring data are produced by all kinds of sensors and software
components, and can be analyzed to detect, among other things,
reliability and performance issues. Considering the scale of such
systems, the amount of data to process is huge and analyzing these
data with short response time is often necessary. Using techniques and
algorithms coming from the Big Data community seems appealing in this
context.

***** Notes
- This talk presented how to apply Big Data and Machine Learning
  techniques in a High Performance Computing context using Apache
  Spark Streaming as a tool for in-situ data analysis. Apache Spark
  Streaming is a scalable fault-tolerant streaming processing
  system, that allows real-time processing of data, which means that
  the data can come in a continuous flow. Data will be accumulated
  during a certain duration (every N period of time), and Spark
  Streaming will produce a Resilient Distributed Dataset from these
  accumulated data.

- CPU overheating issues can cause damage to hardware so it's
  important to prevent them as much as possible. To do so, the idea
  would be to predict them in advance, so to reduce the frequency of
  use of the CPU precisely one minute before the upcoming overheat. In
  many cases, this would prevent overheating, and for false positive
  cases (cases where the CPU was not going to overheat), it's not an
  important loss in performance so it's worth reducing the frequency
  just in case.
*** 2019-05-17 friday
**** Started to look at the model of dgemm with the provided data :STAN:R:
Read [[https://github.com/Ezibenroc/calibration_analysis/blob/master/dahu/blas/dgemm_heterogeneous_model.ipynb ][Tom's analysis on the heterogenous model of dgemm]].

First we import the data from the dgemm.csv file.

#+begin_src R :results output :session *R* :exports both
library(rstan)
library(dplyr)

data=read.csv(file="~/Documents/org/dgemm.csv", header=TRUE, sep=",")

data=data.frame(as.numeric(data$m),as.numeric(data$n),as.numeric(data$k),data$duration,data$cpu)
#we change the type of the data from int to numeric to avoid int overflow when we compute m*n*k

names(data)=c("m","n","k","duration","cpu")

data$mnk=data$m * data$n * data$k
data$mn = data$m * data$n
data$mk = data$m * data$k
data$nk = data$m * data$k

head(data)
#+end_src

#+RESULTS:
#+begin_example
Le chargement a nécessité le package : ggplot2
Registered S3 methods overwritten by 'ggplot2':
  method         from 
  [.quosures     rlang
  c.quosures     rlang
  print.quosures rlang
Le chargement a nécessité le package : StanHeaders
rstan (Version 2.18.2, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)

Attachement du package : ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union
 
     m    n    k  duration cpu        mnk      mn       mk       nk
1  378 7640 2427 0.4859466  20 7008981840 2887920   917406   917406
2  378 7640 2427 0.4861293  20 7008981840 2887920   917406   917406
3  378 7640 2427 0.4868529  20 7008981840 2887920   917406   917406
4 9441  640 1160 0.4551385  20 7008998400 6042240 10951560 10951560
5 9441  640 1160 0.4535278  20 7008998400 6042240 10951560 10951560
6 9441  640 1160 0.4544535  20 7008998400 6042240 10951560 10951560
#+end_example

So we have our dataframe with the coefficients mnk, mn, mk and nk as
well as the dgemm's run duration depending on the coefficients and the
cpu.

#+begin_src R :results output :session *R* :exports both
ggplot(sample_n(data, 10000), aes(x=mnk, y=duration, color='factor(cpu)')) + geom_point(alpha=0.3) +geom_smooth(method='lm')
#+end_src

We can see on this graph that the majority of the data is between the
values 0 and 2e+10 of mnk. We sample our data frame to keep only the
data between these values.

#+begin_src R :results output graphics :file ./images/dgemm_duration_on_mnk.png :exports both :width 600 :height 400 :session *R* 
tmp=select(filter(data, data$mnk < 2e10),everything())

ggplot(sample_n(tmp, 10000), aes(x=mnk, y=duration, color=factor(cpu))) + geom_point(alpha=0.3) +geom_smooth(method='lm')
#+end_src

#+RESULTS:
[[file:./images/dgemm_duration_on_mnk.png]]

Now we can easily find back the heterogeneity that Tom observed.
Indeed, the duration of a dgemm run depends on a combinaison of the m,
n and k parameters (the coefficients mn, mk and nk play a part too but
we started with an easier model with only mnk), but also on the cpu.

We'll start with a M_H-1/N_H-2 model, so a linear model with noise
modeled by a polynomial function, with per-host estimations.

*** 2019-05-20 monday
**** Continued trying to model dgemm :STAN:R:

#+begin_src R :results output :session *R* :exports both
library(rstan)
library(dplyr)

data=read.csv(file="~/Documents/org/dgemm.csv", header=TRUE, sep=",")
data=data.frame(as.numeric(data$m),as.numeric(data$n),as.numeric(data$k),data$duration,data$cpu)
names(data)=c("m","n","k","duration","cpu")

data$mnk=data$m * data$n * data$k
tmp=select(filter(data, data$mnk < 2e10),everything())
#+end_src

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] mnk; //m*n*k
    vector<lower=0>[N] duration; //the duration of dgemm
    real it_mean; //the center of the intercept's distribution
    real it_sd; //the variance of the intercept's distribution
    real mu_mean; //the center of the mnk coefficient's distribution
    real mu_sd; //the variance of the mnk coefficient's distribution
    real sigma_mean; //the center of the noise's distribution
    real sigma_sd;  //the variance of the noise's distribution
}
parameters {
    real intercept;
    real mu;
    real<lower=0> sigma;
}
model {
    intercept ~ normal(it_mean, it_sd);
    mu ~ normal(mu_mean, mu_sd);
    sigma ~ normal(sigma_mean, sigma_sd);
    duration ~ normal(mu*mnk+intercept, sigma*mnk); //duration=mu*mnk+intercept +- sigma*mnk
}
generated quantities {
  real x_pos = mnk[categorical_rng(rep_vector(1,N) / N)];
  real y_sim = normal_rng(mu*x_pos+intercept, sigma*x_pos);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
t=sample_n(data,500) #We use only a sample of 500 values
reg = lm(data=t, duration~mnk) 
#We conduct a linear regression on the sample, to get an approximation of our coefficients
out=summary(reg)
out
#+end_src

#+RESULTS:
#+begin_example
 
Call:
lm(formula = duration ~ mnk, data = t)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.07742 -0.01496 -0.00813  0.00475  0.33583 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 1.462e-02  2.588e-03    5.65 2.71e-08 ***
mnk         6.571e-11  3.866e-13  169.96  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.0353 on 498 degrees of freedom
Multiple R-squared:  0.9831,	Adjusted R-squared:  0.983 
F-statistic: 2.889e+04 on 1 and 498 DF,  p-value: < 2.2e-16
#+end_example

#+begin_src R :results output :session *R* :exports both
dt = list(N=500,mnk=t$mnk,duration=t$duration,run_estimation=1,it_mean=out$coefficients[1,1],it_sd=out$coefficients[1,2],mu_mean=out$coefficients[2,1],mu_sd=out$coefficients[2,2],sigma_mean=0,sigma_sd=0.1)
fit = sampling(sm,data=dt, iter=500, chains=8)
#+end_src

#+RESULTS:
#+begin_example 
Warning messages:
1: There were 1717 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
2: Examine the pairs() plot to diagnose sampling problems
#+end_example

#+begin_src R :results output :session *R* :exports both
print(fit, digits=15)
extracted=rstan::extract(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 026d3a101ad7cfe8c65a4e6d7a90fbdf.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                   mean      se_mean           sd          2.5%           25%
intercept  1.504111e-01 4.062348e-01 8.132832e-01 -9.823013e-01 -3.357801e-01
mu         6.571400e-11 8.000000e-15 3.750000e-13  6.495500e-11  6.546400e-11
sigma      1.227907e+00 6.971982e-01 1.395793e+00  2.296408e-01  3.116581e-01
x_pos      5.338288e+09 1.026550e+08 4.567926e+09  6.676320e+08  2.325543e+09
y_sim     -4.223652e+08 2.621214e+08 1.124476e+10 -2.937809e+10 -1.841169e+09
lp__      -6.178564e+04 3.215719e+04 6.437879e+04 -1.971862e+05 -9.520017e+04
                    50%           75%         97.5% n_eff         Rhat
intercept -3.122313e-02  4.276527e-01  1.594935e+00     4 8.724664e+08
mu         6.572500e-11  6.597000e-11  6.644800e-11  2247 9.994211e-01
sigma      4.271844e-01  1.526354e+00  3.936863e+00     4 8.608280e+08
x_pos      5.000026e+09  7.666454e+09  9.991562e+09  1980 1.001690e+00
y_sim     -5.799446e+07  1.418074e+09  2.340147e+10  1840 9.998499e-01
lp__      -2.279149e+04 -1.491921e+04 -1.177313e+04     4 1.017793e+05

Samples were drawn using NUTS(diag_e) at Fri May 24 15:01:42 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

The result of our model isn't very conclusive, the mu coefficient is always equal
to zero with no variance, which seems very surprising and probably
incorrect. Likewise, there is obviously an issue with the generated
data. The mean of y_sim should be between 0 and 1, since most of the
values of duration are.

EDIT: The reason why the mu coefficient (and on later tests the other
coefficients as well) was equal to zero was because of a rounding of
its value. To counter it, we have to add the option digit to the
print.

#+begin_src R :results output graphics :file ./images/trace_test_dgemm.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
[[file:./images/trace_test_dgemm.png]]

The trace shows as well that the fit didn't work properly, since the
only parameter for which the chains converge is mu, which appears to
have an extremely low value, but different from zero. We assume that
the value shown in the summary of the fit must be truncated.

For the generated data, let's look at what happens if we try to
generate data ourselves with the found coefficients.

#+begin_src R :results output :session *R* :exports both
y_gen=(extracted$x_pos*extracted$mu+extracted$intercept)+(extracted$sigma*extracted$x_pos)-(extracted$sigma*extracted$x_pos)

mean(t$duration)
mean(y_gen)
mean(extracted$y_sim)
#+end_src

#+RESULTS:
:  
: [1] 0.3719175
: 
: [1] -0.4725649
: 
: [1] -149795273

If we generate data from the coefficients we found, we notice that
it could be more accurate (since the fit didn't work properly) but
it's already much closer to the real data than the data generated by
stan. So for some reason, there's an issue with generating data with
stan on this model.

*** 2019-05-21 tuesday
**** Several tests to correct the model :STAN:R:

Our model for dgemm was far from optimal, and its failure to find a
precise value towards which the chains would converge is at the root
of our issues with the generated values. The priority is to correct
this model.

***** First run with Tom's default values as priors

#+begin_src R :results output :session *R* :exports both
library(rstan)
library(dplyr)

data=read.csv(file="~/Documents/org/dgemm.csv", header=TRUE, sep=",")
data=data.frame(as.numeric(data$m),as.numeric(data$n),as.numeric(data$k),data$duration,data$cpu)
names(data)=c("m","n","k","duration","cpu")

data$mnk=data$m * data$n * data$k
tmp=select(filter(data, data$mnk < 2e10),everything())
#+end_src

#+RESULTS:
#+begin_example
Le chargement a nécessité le package : ggplot2
Registered S3 methods overwritten by 'ggplot2':
  method         from 
  [.quosures     rlang
  c.quosures     rlang
  print.quosures rlang
Le chargement a nécessité le package : StanHeaders
rstan (Version 2.18.2, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)

Attachement du package : ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union
#+end_example

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] mnk;
    vector<lower=0>[N] duration;
    int<lower = 0, upper = 1> run_estimation;
    real it_mean;
    real it_sd;
    real mu_mean;
    real mu_sd;
    real sigma_mean;
    real sigma_sd;
    real n_mean;
    real n_sd;
}
parameters {
    real intercept;
    real mu;
    real<lower=0> sigma;
    real<lower=0> noise;
}
model {
    intercept ~ normal(it_mean, it_sd);
    mu ~ normal(mu_mean, mu_sd);
    sigma ~ normal(sigma_mean, sigma_sd);
    noise ~ normal(n_mean, n_sd);
    if(run_estimation==1){
      duration ~ normal(mu*mnk+intercept, sigma*mnk+noise);
    }
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
t=sample_n(tmp,500)
reg = lm(data=t, duration~mnk)
out=summary(reg)
dt = list(N=500,mnk=t$mnk,duration=t$duration,run_estimation=1,it_mean=7.661272e-07,it_sd=7.661272e-08,mu_mean=out$coefficients[2,1],mu_sd=out$coefficients[2,2],sigma_mean=1.895320e-12,sigma_sd=1.895320e-13,n_mean=7.462142e-07, n_sd=7.462142e-08)
fit = sampling(sm,data=dt, iter=500, chains=8,control=list(adapt_delta=0.9, max_treedepth=20))
#+end_src

#+RESULTS:
#+begin_example
Warning messages:
1: There were 8 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
2: Examine the pairs() plot to diagnose sampling problems
#+end_example

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 61a0efffa5767e43bbea608c3c35d0c3.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                   mean      se_mean           sd          2.5%           25%
intercept  3.531664e-01 3.892800e-01 7.793398e-01 -8.924872e-01 -2.539452e-01
mu        -2.262480e-10 2.628420e-10 1.030852e-08  6.597700e-11  6.730800e-11
sigma      5.229433e-05 1.072298e-05 4.419211e-05  1.310142e-05  2.219570e-05
noise      4.083433e-01 1.118904e-01 2.240049e-01  1.679360e-01  2.247335e-01
lp__      -6.531501e+16 2.435243e+16 1.398765e+17 -4.607655e+17 -6.626595e+16
                    50%           75%         97.5% n_eff         Rhat
intercept  3.624562e-01  9.344805e-01  1.574308e+00     4 3.994786e+06
mu         6.772400e-11  6.819900e-11  6.924000e-11  1538 1.000987e+00
sigma      4.229748e-05  6.898724e-05  1.819297e-04    17 1.502134e+00
noise      3.644352e-01  4.851525e-01  8.737234e-01     4 2.810154e+06
lp__      -2.491757e+16 -6.910786e+15 -2.613278e+15    33 1.209887e+00

Samples were drawn using NUTS(diag_e) at Fri May 24 15:37:59 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_test_dgemm_priors.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)
#+end_src

#+RESULTS:
[[file:./trace_test_dgemm_priors.png]]

***** A test without any priors

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] mnk;
    vector<lower=0>[N] duration;
    int<lower = 0, upper = 1> run_estimation;
}
parameters {
    real intercept;
    real mu;
    real<lower=0> sigma;
    real<lower=0> noise;
}
model {
    if(run_estimation==1){
      duration ~ normal(mu*mnk+intercept, sigma*mnk+noise);
    }
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
t=sample_n(tmp,500)
reg = lm(data=t, duration~mnk)
out=summary(reg)
dt = list(N=500,mnk=t$mnk,duration=t$duration,run_estimation=1)
fit = sampling(sm,data=dt, iter=500, chains=8,control=list(adapt_delta=0.99, max_treedepth=15))
#+end_src

#+RESULTS:
#+begin_example
Warning messages:
1: There were 1671 divergent transitions after warmup. Increasing adapt_delta above 0.99 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: There were 34 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 15. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
3: There were 6 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
4: Examine the pairs() plot to diagnose sampling problems
#+end_example

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: eab2140253d776bf0d3d3b28a210bbf6.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                   mean      se_mean           sd          2.5%           25%
intercept -2.569696e-01 6.598232e-01 1.320969e+00 -1.974464e+00 -1.253528e+00
mu         6.832950e-10 1.286599e-09 4.545803e-08 -1.090251e-07 -1.035903e-08
sigma      7.888101e-07 3.646713e-07 7.378537e-07  4.241160e-08  2.098605e-07
noise      2.230022e+00 1.351280e+00 2.705279e+00  1.984302e-01  3.994403e-01
lp__      -3.749007e+03 3.103105e+02 6.221434e+02 -4.462446e+03 -4.439525e+03
                    50%           75%         97.5% n_eff        Rhat
intercept -5.394311e-01  8.099665e-01  1.751192e+00     4 782.9242302
mu        -3.226750e-10  1.197858e-08  1.095395e-07  1248   0.9996885
sigma      4.633372e-07  1.726487e-06  1.785682e-06     4   7.2742745
noise      6.909477e-01  3.061833e+00  8.232229e+00     4 404.1248208
lp__      -3.823721e+03 -3.392409e+03 -2.645889e+03     4  19.9918769

Samples were drawn using NUTS(diag_e) at Fri May 24 15:52:18 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_dgemm_test_no_prior.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)
#+end_src

#+RESULTS:
[[file:./images/trace_dgemm_test_no_prior.png]]

Without priors, the chains converge (around zero) for the mu coefficient, but not
for the others. This is still not what needs to be done to have
accurate results.

***** DONE A test with a simplified model
:LOGBOOK:  
- State "DONE"       from "TODO" [2019-05-22 mer. 10:56]
:END:      

#+begin_src R :results output :session *R* :exports both
library(rstan)
library(dplyr)

data=read.csv(file="~/Documents/org/dgemm.csv", header=TRUE, sep=",")
data=data.frame(as.numeric(data$m),as.numeric(data$n),as.numeric(data$k),data$duration,data$cpu)
names(data)=c("m","n","k","duration","cpu")

data$mnk=data$m * data$n * data$k
tmp=select(filter(data, data$mnk < 2e10),everything())
#+end_src

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] mnk;
    vector<lower=0>[N] duration;
    real mu_mean;
    real mu_sd;
    real sigma_mean;
    real sigma_sd;
}
parameters {
    real mu;
    real<lower=0> sigma;
}
model {
    mu ~ normal(mu_mean, mu_sd);
    sigma ~ normal(sigma_mean, sigma_sd);
    duration ~ normal(mu*mnk, sigma*mnk);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
t=sample_n(tmp,500)
reg = lm(data=t, duration~mnk)
out=summary(reg)
dt = list(N=500,mnk=t$mnk,duration=t$duration,run_estimation=1,mu_mean=out$coefficients[2,1],mu_sd=out$coefficients[2,2],sigma_mean=1.895320e-12,sigma_sd=1.895320e-13)
fit = sampling(sm,data=dt, iter=5100, warmup=1000, chains=8,control=list(adapt_delta=0.9, max_treedepth=20))
#+end_src

#+RESULTS:
#+begin_example
Warning messages:
1: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: There were 8 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
3: Examine the pairs() plot to diagnose sampling problems
#+end_example

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 55845b1a4a35f6aadfd4f4f5a12c737c.
8 chains, each with iter=5100; warmup=1000; thin=1; 
post-warmup draws per chain=4100, total post-warmup draws=32800.

               mean      se_mean           sd          2.5%           25%
mu     6.700800e-11 3.400000e-14 6.489000e-12  6.567900e-11  6.654800e-11
sigma  1.274686e-06 2.558771e-07 1.400233e-06  1.795740e-07  5.344737e-07
lp__  -4.990499e+13 1.888952e+13 1.759958e+14 -2.961368e+14 -3.819235e+13
                50%           75%         97.5% n_eff     Rhat
mu     6.699800e-11  6.742900e-11  6.832100e-11 36035 0.999984
sigma  6.479972e-07  1.656480e-06  4.612580e-06    30 1.324777
lp__  -5.844517e+12 -3.976075e+12 -4.488306e+11    87 1.072626

Samples were drawn using NUTS(diag_e) at Fri May 24 15:56:42 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_dgemm_simple.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)
#+end_src

#+RESULTS:
[[file:./trace_dgemm_simple.png]]

Even with a simpler model, without intercept, and a longer fit with
5000 iterations, the results aren't accurate at all. The best thing to
do would be to make more tests with generated data, to see if we could
find a model that could fit.

*** 2019-05-22 wednesday
**** Several tests with generated data to correct the model :STAN:R:
***** A test with generated data close to dgemm's data
Going back to our generated data, we'll try to see if we can find a
correct fit of our data with a simple, linear model

****** With priors
We'll start by generating data with the coefficients we gave as
priors.

#+begin_src R :results output :session *R* :exports both
library(rstan)

generate_dataset=function(intercept, mu, N, min_x=1, max_x=1e10, sigma){
    mnk = sample(min_x:max_x,N,replace=T) 
    duration = mu * mnk + intercept + rnorm(N, sd=sigma*mnk)
    df = data.frame(mnk,duration)
    return(df)
}
df=generate_dataset(7.661272e-07,6.862693e-11, 500, sigma=1.895320e-12)
#+end_src

We generate data between 1 and 1e10 which is approximately the values
of mnk we used before.

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] mnk;
    vector<lower=0>[N] duration;
    real it_mean;
    real it_sd;
    real mu_mean;
    real mu_sd;
    real sigma_mean;
    real sigma_sd;
}
parameters {
    real intercept;
    real mu;
    real<lower=0> sigma;
}
model {
    intercept ~ normal(it_mean, it_sd);
    mu ~ normal(mu_mean, mu_sd);
    sigma ~ normal(sigma_mean, sigma_sd);
    duration ~ normal(mu*mnk+intercept, sigma*mnk);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),mnk=df$mnk,duration=df$duration,it_mean=7.661272e-07,it_sd=7.661272e-08,mu_mean=6.862693e-11,mu_sd=6.862693e-12,sigma_mean=1.895320e-12,sigma_sd=1.895320e-13)

fit = sampling(sm,data=data, iter=5100, warmup=1000, chains=8, control=list(adapt_delta=0.99, max_treedepth=15))
#+end_src

#+RESULTS:
#+begin_example
Warning messages:
1: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.99 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: There were 8 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
3: Examine the pairs() plot to diagnose sampling problems
#+end_example

We gave the same priors as before, and followed stan's warnings to
increase adapt-delta, max-treedepth and the number of iteration to get
more information. Still, we get the same warnings.

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: bd64ccd38cfd75adfa383e1d11cf788d.
8 chains, each with iter=5100; warmup=1000; thin=1; 
post-warmup draws per chain=4100, total post-warmup draws=32800.

                   mean      se_mean           sd          2.5%           25%
intercept -6.141436e-01 2.791332e-01 5.633156e-01 -1.795303e+00 -1.014738e+00
mu         7.529700e-11 8.050000e-12 2.162900e-11  5.500600e-11  6.455700e-11
sigma      2.494379e-07 1.167940e-07 2.692559e-07  5.413230e-10  4.657374e-08
lp__      -6.103574e+13 3.382827e+13 7.032100e+13 -2.756563e+14 -9.292872e+13
                    50%           75%         97.5% n_eff      Rhat
intercept -4.353619e-01 -2.959864e-01  3.180765e-01     4 10.836042
mu         6.982400e-11  7.563500e-11  1.441850e-10     7  1.917048
sigma      1.740066e-07  3.390582e-07  8.747806e-07     5  3.248281
lp__      -2.375979e+13 -8.925277e+12 -6.984745e+12     4  5.472502

Samples were drawn using NUTS(diag_e) at Fri May 24 16:13:43 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_dgemm_simulated_data_priors.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)
#+end_src

#+RESULTS:
[[file:./images/trace_dgemm_simulated_data_priors.png]]

With priors, the results aren't that much different from the ones with
the real data.

****** Without priors

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] mnk;
    vector<lower=0>[N] duration;
}
parameters {
    real intercept;
    real mu;
    real<lower=0> sigma;
}
model {
    duration ~ normal(mu*mnk+intercept, sigma*mnk);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),mnk=df$mnk,duration=df$duration)

fit = sampling(sm,data=data, iter=500, chains=8, control=list(adapt_delta=0.9, max_treedepth=15))
#+end_src

#+RESULTS:
#+begin_example
Warning messages:
1: There were 624 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: There were 7 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
3: Examine the pairs() plot to diagnose sampling problems
#+end_example

We had to make a fit with only 500 iterations this time, otherwise it
takes too long.

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 6aecb669cd764d58320b4a59fdaa7f61.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                   mean      se_mean           sd          2.5%           25%
intercept  3.102190e-01 2.983665e-01 5.973307e-01 -7.105154e-01 -3.898865e-02
mu        -4.236798e-09 6.341455e-08 2.341009e-06 -5.081258e-06 -8.691229e-07
sigma      4.030542e-05 1.490636e-05 2.996813e-05  1.094829e-05  1.902506e-05
lp__      -5.816910e+03 1.733226e+02 3.485917e+02 -6.384650e+03 -6.087584e+03
                    50%           75%         97.5% n_eff        Rhat
intercept  3.449164e-01  5.428088e-01  1.479953e+00     4 3825.218823
mu        -4.182785e-08  8.355895e-07  5.608310e-06  1363    1.002042
sigma      2.896082e-05  5.571756e-05  9.762570e-05     4   14.749600
lp__      -5.777482e+03 -5.568095e+03 -5.293072e+03     4   12.192077

Samples were drawn using NUTS(diag_e) at Fri May 24 16:20:05 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_dgemm_simulated_data_no_priors.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)
#+end_src

#+RESULTS:
[[file:./images/trace_dgemm_simulated_data_no_priors.png]]

And without priors, the results are not better, though the chains
converge more on the mu coefficient (but on a value that doesn't seem
right). So surprisingly, even with generated data for which we used to
find accurate results, the results are still wildly off. It appears
that stan is unable to correctly find coefficients when their value is
really low.

***** A test with data reduced to a small interval
Since the previous test wasn't conclusive, we'll try again with
generated data, but this time reducing mnk between 1 and 100 (like our
previous models for generated data).

#+begin_src R :results output :session *R* :exports both
library(rstan)

generate_dataset=function(intercept, mu, N, min_x=1, max_x=100, sigma){
    mnk = sample(min_x:max_x,N,replace=T) 
    duration = mu * mnk + intercept + rnorm(N, sd=sigma*mnk)
    df = data.frame(mnk,duration)
    return(df)
}
df=generate_dataset(7.661272e-07,6.862693e-11, 500, sigma=1.895320e-12)
#+end_src

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] mnk;
    vector<lower=0>[N] duration;
    real it_mean;
    real it_sd;
    real mu_mean;
    real mu_sd;
    real sigma_mean;
    real sigma_sd;
}
parameters {
    real intercept;
    real mu;
    real<lower=0> sigma;
}
model {
    intercept ~ normal(it_mean, it_sd);
    mu ~ normal(mu_mean, mu_sd);
    sigma ~ normal(sigma_mean, sigma_sd);
    duration ~ normal(mu*mnk+intercept, sigma*mnk);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),mnk=df$mnk,duration=df$duration,it_mean=7.661272e-07,it_sd=7.661272e-08,mu_mean=6.862693e-11,mu_sd=6.862693e-12,sigma_mean=1.895320e-12,sigma_sd=1.895320e-13)

fit = sampling(sm,data=data, iter=5100, warmup=1000, chains=8, control=list(adapt_delta=0.99, max_treedepth=15))
#+end_src

#+RESULTS:
#+begin_example
Warning messages:
1: There were 3 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 15. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
2: There were 8 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
3: Examine the pairs() plot to diagnose sampling problems
#+end_example

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: bd64ccd38cfd75adfa383e1d11cf788d.
8 chains, each with iter=5100; warmup=1000; thin=1; 
post-warmup draws per chain=4100, total post-warmup draws=32800.

                   mean      se_mean           sd          2.5%           25%
intercept -2.267105e-01 4.687930e-01 9.506106e-01 -1.917152e+00 -8.403034e-01
mu         2.460500e-10 1.057454e-09 2.130387e-09 -2.234471e-09 -2.203943e-09
sigma      7.125636e-07 9.323223e-08 2.113287e-07  3.112603e-07  5.568434e-07
lp__      -9.540276e+13 4.951747e+13 1.026864e+14 -3.440622e+14 -9.243007e+13
                    50%           75%         97.5% n_eff      Rhat
intercept -1.261219e-01  7.176045e-01  9.511613e-01     4  9.902207
mu         6.989710e-10  2.359326e-09  2.375085e-09     4 18.033746
sigma      7.227470e-07  8.342450e-07  1.054829e-06     5  3.661025
lp__      -6.035752e+13 -3.286637e+13 -5.001282e+12     4  6.982544

Samples were drawn using NUTS(diag_e) at Fri May 24 16:23:16 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_dgemm_simulated_data_small_interval.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)
#+end_src

#+RESULTS:
[[file:./images/trace_dgemm_simulated_data_small_interval.png]]

Even with a reduced interval, the results are not better. Our
theory that stan is unable to precisely find coefficients with low
values even with simple models seems more and more plausible.
***** A test with the first linear model and small coefficients
****** Without priors

#+begin_src R :results output :session *R* :exports both
library(rstan)
generate_dataset=function(intercept, mu, N, min_x=1, max_x=100, sigma){
    mnk = sample(min_x:max_x,N,replace=T) 
    duration = mu * mnk + intercept + rnorm(N, sd=sigma*mnk)
    df = data.frame(mnk,duration)
    return(df)
}
df=generate_dataset(7.661272,6.862693, 500, sigma=1.895320)
#+end_src

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] mnk;
    vector<lower=0>[N] duration;
}
parameters {
    real intercept;
    real mu;
    real<lower=0> sigma;
}
model {
    duration ~ normal(mu*mnk+intercept, sigma*mnk);
}
generated quantities {
 real x_pos = mnk[categorical_rng(rep_vector(1,N) / N)];
 real y_sim= normal_rng(mu*x_pos+intercept, sigma*x_pos);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),mnk=df$mnk,duration=df$duration)

fit = sampling(sm,data=data, iter=1000, chains=8, control=list(adapt_delta=0.9, max_treedepth=15))
#+end_src

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 6c53f653112405a28b1addf711377071.
8 chains, each with iter=1000; warmup=500; thin=1; 
post-warmup draws per chain=500, total post-warmup draws=4000.

              mean     se_mean         sd     2.5%      25%      50%      75%
intercept 7.647600 0.014967366 0.79401695 6.117863 7.091200 7.641664 8.186877
mu        7.036958 0.001692130 0.09073839 6.856957 6.976358 7.036628 7.098632
sigma     1.853676 0.001064808 0.05973399 1.742868 1.812748 1.853186 1.892427
             97.5% n_eff      Rhat
intercept 9.223198  2814 0.9994819
mu        7.214872  2876 0.9994882
sigma     1.978509  3147 1.0027485

Samples were drawn using NUTS(diag_e) at Fri May 24 16:25:54 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_linear_model_normal_values_priors.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
[[file:./images/trace_linear_model_normal_values_priors.png]]

With these coefficients, the results are quite accurate. Let's see what
happens if we reduce the coefficients.

#+begin_src R :results output :session *R* :exports both
generate_dataset=function(intercept, mu, N, min_x=1, max_x=100, sigma){
    mnk = sample(min_x:max_x,N,replace=T) 
    duration = mu * mnk + intercept + rnorm(N, sd=sigma*mnk)
    df = data.frame(mnk,duration)
    return(df)
}
df=generate_dataset(7.661272e-04,6.862693e-06, 500, sigma=1.895320e-06)
#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),mnk=df$mnk,duration=df$duration)

fit = sampling(sm,data=data, iter=1000, chains=8, control=list(adapt_delta=0.9, max_treedepth=15))
#+end_src

#+RESULTS:
#+begin_example
Warning messages:
1: There were 971 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: There were 7 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
3: Examine the pairs() plot to diagnose sampling problems
#+end_example

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 6c53f653112405a28b1addf711377071.
8 chains, each with iter=1000; warmup=500; thin=1; 
post-warmup draws per chain=500, total post-warmup draws=4000.

                  mean      se_mean           sd         2.5%          25%
intercept 7.667357e-04 6.504060e-08 1.934812e-06 7.629936e-04 7.659316e-04
mu        6.844724e-06 4.583065e-09 2.314504e-07 6.369520e-06 6.748218e-06
sigma     3.868036e-06 1.291103e-06 3.109356e-06 1.908412e-06 2.135792e-06
                   50%          75%        97.5% n_eff      Rhat
intercept 7.667782e-04 7.675560e-04 7.703698e-04   885 1.0009571
mu        6.842989e-06 6.941767e-06 7.317097e-06  2550 0.9994663
sigma     2.515340e-06 4.275463e-06 1.123641e-05     6 2.3392721

Samples were drawn using NUTS(diag_e) at Fri May 24 16:27:42 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_linear_model_small_values_priors.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
[[file:./images/trace_linear_model_small_values_priors.png]]

This time, the chains no longer converge for sigma, even though the
value found seems close enough to the truth. Now we try by taking the
real coefficients: 

#+begin_src R :results output :session *R* :exports both
generate_dataset=function(intercept, mu, N, min_x=1, max_x=100, sigma){
    mnk = sample(min_x:max_x,N,replace=T) 
    duration = mu * mnk + intercept + rnorm(N, sd=sigma*mnk)
    df = data.frame(mnk,duration)
    return(df)
}
df=generate_dataset(7.661272e-07,6.862693e-11, 500, sigma=1.895320e-12)
#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),mnk=df$mnk,duration=df$duration)

fit = sampling(sm,data=data, iter=1000, chains=8, control=list(adapt_delta=0.9, max_treedepth=15))
#+end_src

#+RESULTS:
#+begin_example
Warning messages:
1: There were 1186 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: There were 8 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
3: Examine the pairs() plot to diagnose sampling problems
#+end_example

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 6c53f653112405a28b1addf711377071.
8 chains, each with iter=1000; warmup=500; thin=1; 
post-warmup draws per chain=500, total post-warmup draws=4000.

                  mean      se_mean           sd          2.5%           25%
intercept 7.400348e-07 8.844799e-08 2.129813e-06 -3.267757e-06  4.382780e-08
mu        4.219793e-09 6.283186e-09 2.615570e-07 -4.642814e-07 -9.867937e-08
sigma     3.968054e-06 1.455374e-06 3.502562e-06  4.796358e-07  2.085801e-06
                   50%          75%        97.5% n_eff     Rhat
intercept 7.616282e-07 1.518633e-06 4.440404e-06   580 1.013834
mu        6.300120e-10 9.383928e-08 5.091366e-07  1733 1.001520
sigma     3.356702e-06 4.682584e-06 1.537264e-05     6 2.470264

Samples were drawn using NUTS(diag_e) at Fri May 24 16:28:54 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_linear_model_smallest_values_priors.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
[[file:./images/trace_linear_model_smallest_values_priors.png]]

We can conclude that the chains converge less and less and the results
get more and more inaccurate the smaller our coefficients are, in an
analysis without priors.

****** With priors

#+begin_src R :results output :session *R* :exports both
library(rstan)
generate_dataset=function(intercept, mu, N, min_x=1, max_x=100, sigma){
    mnk = sample(min_x:max_x,N,replace=T) 
    duration = mu * mnk + intercept + rnorm(N, sd=sigma*mnk)
    df = data.frame(mnk,duration)
    return(df)
}
df=generate_dataset(7.661272,6.862693, 500, sigma=1.895320)
#+end_src

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] mnk;
    vector<lower=0>[N] duration;
    real it_mean;
    real it_sd;
    real mu_mean;
    real mu_sd;
    real sigma_mean;
    real sigma_sd;
}
parameters {
    real intercept;
    real mu;
    real<lower=0> sigma;
}
model {
    intercept ~ normal(it_mean, it_sd);
    mu ~ normal(mu_mean, mu_sd);
    sigma ~ normal(sigma_mean, sigma_sd);
    duration ~ normal(mu*mnk+intercept, sigma*mnk);
}
generated quantities {
 real x_pos = mnk[categorical_rng(rep_vector(1,N) / N)];
 real y_sim= normal_rng(mu*x_pos+intercept, sigma*x_pos);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),mnk=df$mnk,duration=df$duration,it_mean=7.661272,it_sd=7.661272e-01,mu_mean=6.862693,mu_sd=6.862693e-01,sigma_mean=1.895320,sigma_sd=1.895320e-01)
fit = sampling(sm,data=data, iter=1000, chains=8, control=list(adapt_delta=0.9, max_treedepth=15))
#+end_src

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 4288276d79fabf2e3c6527abfc683c05.
8 chains, each with iter=1000; warmup=500; thin=1; 
post-warmup draws per chain=500, total post-warmup draws=4000.

              mean      se_mean         sd     2.5%      25%      50%      75%
intercept 7.652102 0.0083744530 0.44089146 6.805078 7.349009 7.648880 7.941100
mu        6.784125 0.0015773987 0.08542805 6.617932 6.726332 6.784802 6.840460
sigma     1.803864 0.0009319497 0.05526949 1.696784 1.767061 1.803639 1.839666
             97.5% n_eff     Rhat
intercept 8.531703  2772 1.000523
mu        6.950301  2933 1.000465
sigma     1.916008  3517 1.000373

Samples were drawn using NUTS(diag_e) at Fri May 24 16:34:13 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_linear_model_normal_values_priors.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
[[file:./images/trace_linear_model_normal_values_priors.png]]

As before, with these values the results are accurate.

#+begin_src R :results output :session *R* :exports both
generate_dataset=function(intercept, mu, N, min_x=1, max_x=100, sigma){
    mnk = sample(min_x:max_x,N,replace=T) 
    duration = mu * mnk + intercept + rnorm(N, sd=sigma*mnk)
    df = data.frame(mnk,duration)
    return(df)
}
df=generate_dataset(7.661272e-04,6.862693e-06, 500, sigma=1.895320e-06)
#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),mnk=df$mnk,duration=df$duration,it_mean=7.661272e-04,it_sd=7.661272e-05,mu_mean=6.862693e-06,mu_sd=6.862693e-07,sigma_mean=1.895320e-06,sigma_sd=1.895320e-07)
fit = sampling(sm,data=data, iter=1000, chains=8, control=list(adapt_delta=0.9, max_treedepth=15))
#+end_src

#+RESULTS:
#+begin_example
Warning messages:
1: There were 266 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: There were 8 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
3: Examine the pairs() plot to diagnose sampling problems
#+end_example

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 4288276d79fabf2e3c6527abfc683c05.
8 chains, each with iter=1000; warmup=500; thin=1; 
post-warmup draws per chain=500, total post-warmup draws=4000.

                  mean      se_mean           sd         2.5%          25%
intercept 7.639708e-04 1.595381e-06 1.878662e-05 7.144829e-04 7.612581e-04
mu        6.869342e-06 9.534174e-09 4.971330e-07 5.855977e-06 6.575348e-06
sigma     3.059329e-05 1.799649e-05 3.742901e-05 3.812280e-06 7.213205e-06
                   50%          75%        97.5% n_eff     Rhat
intercept 7.650721e-04 7.690010e-04 8.023561e-04   139 1.093105
mu        6.871389e-06 7.166638e-06 7.872845e-06  2719 1.004060
sigma     1.583788e-05 2.918923e-05 1.272126e-04     4 7.638276

Samples were drawn using NUTS(diag_e) at Fri May 24 16:38:58 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_linear_model_small_values_priors.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
[[file:./images/trace_linear_model_small_values_priors.png]]

Adding a prior doesn't seem to help stan's fitting when the
coefficients are getting small. The trace is still precise for mu and
for the intercept, but not for sigma.

#+begin_src R :results output :session *R* :exports both
generate_dataset=function(intercept, mu, N, min_x=1, max_x=100, sigma){
    mnk = sample(min_x:max_x,N,replace=T) 
    duration = mu * mnk + intercept + rnorm(N, sd=sigma*mnk)
    df = data.frame(mnk,duration)
    return(df)
}
df=generate_dataset(7.661272e-07,6.862693e-11, 500, sigma=1.895320e-12)
#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),mnk=df$mnk,duration=df$duration,it_mean=7.661272e-07,it_sd=7.661272e-08,mu_mean=6.862693e-11,mu_sd=6.862693e-12,sigma_mean=1.895320e-12,sigma_sd=1.895320e-13)
fit = sampling(sm,data=data, iter=1000, chains=8, control=list(adapt_delta=0.9, max_treedepth=15))
#+end_src

#+RESULTS:
#+begin_example
Warning messages:
1: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: There were 8 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
3: Examine the pairs() plot to diagnose sampling problems
#+end_example

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 4288276d79fabf2e3c6527abfc683c05.
8 chains, each with iter=1000; warmup=500; thin=1; 
post-warmup draws per chain=500, total post-warmup draws=4000.

                   mean      se_mean           sd          2.5%           25%
intercept  4.813463e-01 5.856872e-01 1.178049e+00 -1.283666e+00 -5.717404e-01
mu        -8.205100e-11 8.178180e-10 1.730782e-09 -2.293373e-09 -1.302557e-09
sigma      1.106099e-06 1.619282e-07 5.537407e-07  4.153059e-07  7.820781e-07
                    50%          75%        97.5% n_eff      Rhat
intercept  9.067481e-01 1.265058e+00 1.845933e+00     4 11.781661
mu        -7.152190e-10 1.779412e-09 2.426026e-09     4  4.923957
sigma      9.568483e-07 1.255968e-06 2.213947e-06    12  1.858659

Samples were drawn using NUTS(diag_e) at Fri May 24 16:40:26 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_linear_model_smallest_values_priors.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
[[file:./images/trace_linear_model_smallest_values_priors.png]]

The surprising part here is that adding a prior when the coefficients
are really small seems to make the convergence of the chains harder,
and to induce results farther from their actual value, despite the
accuracy of the prior. Thus, the problem we face is the following: how
to make precise analysis in stan with small values? Considering the
problem doesn't seem linked to the complexity of the model, as we've
tried also with a noise of normal distribution and it didn't change
anything.

***** A test with the first linear model and normally distributed noise
This test will be without priors as we've seen they don't necessarily
help but instead tend to worsen the divergence of the chains in the
case of small coefficients. We'll only test with the low values, as
we've seen that for bigger values the stan fit is okay.

#+begin_src R :results output :session *R* :exports both
library(rstan)

generate_dataset=function(intercept, mu, N, min_x=1, max_x=100, sigma){
    mnk = sample(min_x:max_x,N,replace=T) 
    duration = mu * mnk + intercept + rnorm(N, sd=sigma)
    df = data.frame(mnk,duration)
    return(df)
}
df=generate_dataset(7.661272e-04,6.862693e-06, 500, sigma=1.895320e-06)
#+end_src

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] mnk;
    vector<lower=0>[N] duration;
}
parameters {
    real intercept;
    real mu;
    real<lower=0> sigma;
}
model {
    duration ~ normal(mu*mnk+intercept, sigma*mnk);
}
generated quantities {
 real x_pos = mnk[categorical_rng(rep_vector(1,N) / N)];
 real y_sim= normal_rng(mu*x_pos+intercept, sigma*x_pos);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),mnk=df$mnk,duration=df$duration)

fit = sampling(sm,data=data, iter=1000, chains=8, control=list(adapt_delta=0.999, max_treedepth=15))
#+end_src

#+RESULTS:
#+begin_example 
Warning messages:
1: There were 744 divergent transitions after warmup. Increasing adapt_delta above 0.999 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: There were 85 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 15. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
3: There were 8 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
4: Examine the pairs() plot to diagnose sampling problems
#+end_example

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 6c53f653112405a28b1addf711377071.
8 chains, each with iter=1000; warmup=500; thin=1; 
post-warmup draws per chain=500, total post-warmup draws=4000.

                  mean      se_mean           sd         2.5%          25%
intercept 7.652839e-04 1.676923e-08 6.671097e-07 7.641304e-04 7.650775e-04
mu        6.897411e-06 1.160137e-09 5.954334e-08 6.777433e-06 6.875426e-06
sigma     9.459901e-07 2.238896e-07 7.849776e-07 1.778151e-07 4.290312e-07
                   50%          75%        97.5% n_eff     Rhat
intercept 7.652870e-04 7.654885e-04 7.665026e-04  1583 1.005037
mu        6.898233e-06 6.921563e-06 7.012984e-06  2634 1.000710
sigma     7.963216e-07 1.111225e-06 2.712883e-06    12 1.465876

Samples were drawn using NUTS(diag_e) at Fri May 24 16:52:47 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_linear_normal_small.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
[[file:./images/trace_linear_normal_small.png]]

Exactly as before, the chains don't converge for sigma, despite having
a simpler model.

#+begin_src R :results output :session *R* :exports both
generate_dataset=function(intercept, mu, N, min_x=1, max_x=100, sigma){
    mnk = sample(min_x:max_x,N,replace=T) 
    duration = mu * mnk + intercept + rnorm(N, sd=sigma)
    df = data.frame(mnk,duration)
    return(df)
}
df=generate_dataset(7.661272e-07,6.862693e-11, 500, sigma=1.895320e-12)
#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),mnk=df$mnk,duration=df$duration)

fit = sampling(sm,data=data, iter=1000, chains=8, control=list(adapt_delta=0.9, max_treedepth=15))
#+end_src

#+RESULTS:
#+begin_example
Warning messages:
1: There were 1248 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: There were 8 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
3: Examine the pairs() plot to diagnose sampling problems
#+end_example

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 6c53f653112405a28b1addf711377071.
8 chains, each with iter=1000; warmup=500; thin=1; 
post-warmup draws per chain=500, total post-warmup draws=4000.

                  mean      se_mean           sd          2.5%           25%
intercept 6.334967e-07 9.374760e-08 1.948634e-06 -3.511208e-06 -2.780692e-07
mu        4.279249e-09 5.213525e-09 2.269370e-07 -4.428789e-07 -1.103004e-07
sigma     4.106003e-06 1.012670e-06 2.249853e-06  9.290305e-07  2.617086e-06
                   50%          75%        97.5% n_eff     Rhat
intercept 6.842971e-07 1.579134e-06 4.764106e-06   432 1.023165
mu        5.220629e-09 1.080350e-07 4.888979e-07  1895 1.003868
sigma     4.374194e-06 4.682763e-06 8.107355e-06     5 2.954315

Samples were drawn using NUTS(diag_e) at Fri May 24 16:55:12 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_linear_normal_smallest.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
[[file:./images/trace_linear_normal_smallest.png]]

With normally distributed noise, the fit seems better as the chains
still converge more or less for the intercept and the mu
coefficient. So the quality of the fit depends slightly on the
simplicity of the model, but still the small values are clearly an
issue for stan.
*** 2019-05-24 friday
**** Some solutions to correct the loss of precision with small parameters
A first solution would be to make a non centered parameterisation, so
the sampler would find more accurate results.

#+begin_src R :results output :session *R* :exports both
library(rstan)

generate_dataset=function(intercept, mu, N, min_x=1, max_x=100, sigma){
    mnk = sample(min_x:max_x,N,replace=T) 
    duration = mu * mnk + intercept + rnorm(N, sd=sigma*mnk)
    df = data.frame(mnk,duration)
    return(df)
}
df=generate_dataset(7.661272e-07,6.862693e-11, 1000, sigma=1.895320e-12)
#+end_src

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] mnk;
    vector<lower=0>[N] duration;
    real it_mean;
    real it_sd;
    real mu_mean;
    real mu_sd;
    real sigma_mean;
    real sigma_sd;
}
parameters {
    real intercept_raw;
    real mu_raw;
    real<lower=0> sigma_raw;
}
transformed parameters {
    real intercept = it_mean + intercept_raw * it_sd;
    real mu = mu_mean + mu_raw * mu_sd;
    real sigma = sigma_mean + sigma_raw * sigma_sd;
}
model {
   intercept_raw ~ normal(0,1);
   mu_raw ~ normal(0,1);
   sigma_raw ~ normal(0,1);
   duration ~ normal(mu*mnk+intercept, sigma);
}"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),mnk=df$mnk,duration=df$duration,it_mean=7.661272e-07,it_sd=7.661272e-08,mu_mean=6.862693e-11,mu_sd=6.862693e-12,sigma_mean=1.895320e-12,sigma_sd=1.895320e-13)

fit = sampling(sm,data=data, iter=5000, warmup=1000, chains=8, control=list(adapt_delta=0.99, max_treedepth=15))
#+end_src

#+RESULTS:
#+begin_example
Warning messages:
1: There were 192 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 15. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
2: There were 7 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
3: Examine the pairs() plot to diagnose sampling problems
#+end_example

We gave the same priors as before, and followed stan's warnings to
increase adapt-delta, max-treedepth and the number of iteration to get
more information. Still, we get the same warnings.

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15,pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 3137d51cccd536c33529682acca3446d.
8 chains, each with iter=5000; warmup=1000; thin=1; 
post-warmup draws per chain=4000, total post-warmup draws=32000.

                  mean   se_mean         sd         2.5%          25%
intercept 7.661317e-07 8.783e-12 3.3081e-11 7.661211e-07 7.661234e-07
mu        6.861700e-11 1.700e-13 6.4200e-13 6.573100e-11 6.875200e-11
sigma     1.720600e-11 3.849e-12 1.0970e-11 1.994000e-12 3.697000e-12
                  50%          75%        97.5% n_eff     Rhat
intercept 7.66124e-07 7.661249e-07 7.662804e-07    14 2.321179
mu        6.87670e-11 6.877700e-11 6.881700e-11    14 2.322790
sigma     2.61820e-11 2.630600e-11 2.645800e-11     8 2.924352

Samples were drawn using NUTS(diag_e) at Fri May 24 11:49:50 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_non_centered_parameterisation.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit, pars=c("intercept","mu","sigma"))
#+end_src

#+RESULTS:
[[file:./images/trace_non_centered_parameterisation.png]]

Indeed, the results are much closer to the real parameters and appear
reasonable. Also the traces still don't show a full convergence, but a
much better one than before.

**** Remade all the previous tests without the rounding on the fit's printing
*** 2019-05-27 monday
**** STARTED Read Bayesian Data Analysis by Andrew Gelman, John B. Carlin and four other authors :PAPER:STATISTICS:
**** STARTED Read the slides of the Polaris Bootcamp talks
**** Remade dgemm's M-1 N-2 model with a non centered parameterisation
As per advices from the stan forum, we rewrote the dgemm's model in a
different way, with transformed parameters, to help with the sampling.

#+begin_src R :results output :session *R* :exports both
library(rstan)
library(dplyr)

data=read.csv(file="~/Documents/org/dgemm.csv", header=TRUE, sep=",")
data=data.frame(as.numeric(data$m),as.numeric(data$n),as.numeric(data$k),data$duration,data$cpu)
names(data)=c("m","n","k","duration","cpu")

data$mnk=data$m * data$n * data$k
tmp=select(filter(data, data$mnk < 2e10),everything())
#+end_src

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] mnk;
    vector[N] duration;
    real it_mean;
    real it_sd;
    real mu_mean;
    real mu_sd;
    real sigma_mean;
    real sigma_sd;
    real n_mean;
    real n_sd;
}
parameters {
    real intercept_raw;
    real mu_raw;
    real<lower=0> sigma_raw;
    real<lower=0> noise_raw;
}
transformed parameters {
    real intercept = it_mean + intercept_raw * it_sd;
    real mu = mu_mean + mu_raw * mu_sd;
    real sigma = sigma_mean + sigma_raw * sigma_sd;
    real noise = n_mean + noise_raw * n_sd;
}
model {
    intercept_raw ~ normal(0,1);
    mu_raw ~ normal(0,1);
    sigma_raw ~ normal(0,1);
    noise_raw ~ normal(0,1);
    duration ~ normal(mu*mnk+intercept, sigma*mnk+noise);
}

generated quantities {
  real x_pos = mnk[categorical_rng(rep_vector(1,N) / N)];
  real y_sim= normal_rng(mu*x_pos+intercept, sigma*x_pos+noise);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
t=sample_n(tmp,500)
dt = list(N=500,mnk=t$mnk,duration=t$duration,it_mean=7.661272e-07,it_sd=7.661272e-08,mu_mean=6.862693e-11,mu_sd=6.862693e-12,sigma_mean=1.895320e-12,sigma_sd=1.895320e-13,n_mean=7.462142e-07, n_sd=7.462142e-08)
#As usual, we use the values of the default case in Tom's analysis as prior

fit = sampling(sm,data=dt, iter=500, chains=8)
#+end_src

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15,pars=c("intercept","mu","sigma","noise"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 2052d8b2e3aa5f2928b46819f13fa8ff.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean      se_mean           sd         2.5%          25%
intercept 7.647913e-07 1.665034e-09 7.499101e-08 6.217157e-07 7.140399e-07
mu        6.982500e-11 6.000000e-15 2.460000e-13 6.932000e-11 6.966600e-11
sigma     5.349000e-12 2.000000e-15 9.700000e-14 5.166000e-12 5.284000e-12
noise     8.059263e-07 1.181056e-09 4.510582e-08 7.484906e-07 7.709575e-07
                   50%          75%        97.5% n_eff      Rhat
intercept 7.658860e-07 8.167262e-07 9.135431e-07  2028 0.9989287
mu        6.983400e-11 6.998700e-11 7.030700e-11  1841 1.0034719
sigma     5.348000e-12 5.408000e-12 5.542000e-12  2028 1.0016055
noise     7.966443e-07 8.344683e-07 9.117214e-07  1459 1.0013814

Samples were drawn using NUTS(diag_e) at Tue May 28 09:24:05 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_dgemm_corrected.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit,pars=c("intercept","mu","sigma","noise"))
#+end_src

#+RESULTS:
[[file:./images/trace_dgemm_corrected.png]]

The results are really close to the priors, and the chains converge in
the expected way.

#+begin_src R :results output :session *R* :exports both
extracted=rstan::extract(fit)

t_generated = data.frame(mnk=extracted$x_pos, duration=extracted$y_sim, origin='generated')
t_generated = t_generated[sample(nrow(t_generated), nrow(t)), ]
t=select(t,c("mnk","duration"))
t$origin = 'initial'
new_t = rbind(t, t_generated)
#+end_src

#+begin_src R :results output graphics :file ./images/generated_quantities_dgemm_corrected.png :exports both :width 600 :height 400 :session *R* 
ggplot(new_t, aes(mnk,duration,color=origin)) + geom_point(alpha=0.6)
#+end_src

#+RESULTS:
[[file:./images/generated_quantities_dgemm_corrected.png]]

The generated quantities are very close to our data. Some points are
missing, but that is because of the simplicity of our model (we need
to consider the mn, mk and nk coefficients).
*** 2019-05-28 tuesday
**** Made a M-2 N-2 model for dgemm
***** A first test with the model

The idea of this model is to take in account the other coefficients
that might be important: mn, mk and nk.

#+begin_src R :results output :session *R* :exports both
library(rstan)
library(dplyr)

data=read.csv(file="~/Documents/org/dgemm.csv", header=TRUE, sep=",")
data=data.frame(as.numeric(data$m),as.numeric(data$n),as.numeric(data$k),data$duration,data$cpu)
names(data)=c("m","n","k","duration","cpu")

data$mnk=data$m * data$n * data$k
tmp=select(filter(data, data$mnk < 2e10),everything())
#+end_src

For this model, we no longer give directly mnk, but instead the three
variables m, n and k and we compute the coefficients in the stan
model, with the transformed data block. To avoid using a lot of
variables, we now have a vector of 5 values for mu and sigma,
corresponding to the coefficients in the following order: the mnk
coefficient, the mn one, the mk one, the nk one and finally the
intercept (or "noise" as we called it before for sigma).

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] m;
    vector[N] n;
    vector[N] k;
    vector[N] duration;
    vector[5] mu_mean;
    vector[5] mu_sd;
    vector[5] sigma_mean;
    vector[5] sigma_sd;
}
transformed data {
    vector[N] mnk;
    vector[N] mn;
    vector[N] mk;
    vector[N] nk;
    for(i in 1:N){
      mnk[i] = m[i]*n[i]*k[i];
      mn[i] = m[i]*n[i];
      mk[i] = m[i]*k[i];
      nk[i] = n[i]*k[i];
    }
}
parameters {
    real mu_raw;
    real<lower=0> sigma_raw;
}
transformed parameters {
    vector[5] mu;
    vector[5] sigma;
    for(j in 1:5){
      mu[j] = mu_mean[j] + mu_raw * mu_sd[j]; //consider testing with mu_raw and sigma_raw as vectors
      sigma[j] = sigma_mean[j] + sigma_raw * sigma_sd[j];
    }
}
model {
    mu_raw ~ normal(0,1);
    sigma_raw ~ normal(0,1);
    duration ~ normal(mu[1]*mnk + mu[2]*mn + mu[3]*mk + mu[4]*nk + mu[5], sigma[1]*mnk + sigma[2]*mn + sigma[3]*mk + sigma[4]*nk + sigma[5]);
}

generated quantities {
  real mnk_pos = mnk[categorical_rng(rep_vector(1,N) / N)];
  real mn_pos = mn[categorical_rng(rep_vector(1,N) / N)];
  real mk_pos = mk[categorical_rng(rep_vector(1,N) / N)];
  real nk_pos = nk[categorical_rng(rep_vector(1,N) / N)];
  real y_sim = normal_rng(mu[1]*mnk_pos + mu[2]*mn_pos + mu[3]*mk_pos + mu[4]*nk_pos + mu[5], sigma[1]*mnk_pos + sigma[2]*mn_pos + sigma[3]*mk_pos + sigma[4]*nk_pos + sigma[5]);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
t=sample_n(tmp,500)
dt = list(N=500,m=t$m,n=t$n,k=t$k,duration=t$duration,mu_mean=c(6.814095e-11,3.416642e-11,1.648690e-09,3.067383e-09,7.661272e-07),mu_sd=c(6.814095e-12,3.416642e-12,1.648690e-10,3.067383e-10,7.661272e-08),sigma_mean=c(1.224054e-12,1.185692e-11,2.846888e-11,3.073464e-11,7.462142e-07),sigma_sd=c(1.224054e-13,1.185692e-12,2.846888e-12,3.073464e-12,7.462142e-08))
#As usual, we use the values of the default case in Tom's analysis as prior

fit = sampling(sm,data=dt, iter=500, chains=8)
#+end_src

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15,pars=c("mu[1]","mu[2]","mu[3]","mu[4]","mu[5]","sigma[1]","sigma[2]","sigma[3]","sigma[4]","sigma[5]"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 3373ca2cb4c480c671f2ce5f776dab86.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                 mean     se_mean           sd         2.5%          25%
mu[1]    6.487400e-11 3.00000e-15 1.460000e-13 6.457900e-11 6.478000e-11
mu[2]    3.252800e-11 2.00000e-15 7.300000e-14 3.238100e-11 3.248100e-11
mu[3]    1.569639e-09 8.30000e-14 3.541000e-12 1.562518e-09 1.567368e-09
mu[4]    2.920309e-09 1.54000e-13 6.589000e-12 2.907061e-09 2.916084e-09
mu[5]    7.293932e-07 3.84070e-11 1.645633e-09 7.260841e-07 7.283379e-07
sigma[1] 3.150000e-12 1.00000e-15 6.000000e-14 3.035000e-12 3.111000e-12
sigma[2] 3.051100e-11 1.40000e-14 5.790000e-13 2.939900e-11 3.013200e-11
sigma[3] 7.325700e-11 3.30000e-14 1.390000e-12 7.058900e-11 7.234900e-11
sigma[4] 7.908700e-11 3.60000e-14 1.501000e-12 7.620700e-11 7.810700e-11
sigma[5] 1.920181e-06 8.62853e-10 3.644106e-08 1.850240e-06 1.896372e-06
                  50%          75%        97.5% n_eff      Rhat
mu[1]    6.487400e-11 6.496900e-11 6.515900e-11  1836 1.0017786
mu[2]    3.252800e-11 3.257600e-11 3.267100e-11  1836 1.0017786
mu[3]    1.569640e-09 1.571934e-09 1.576538e-09  1836 1.0017786
mu[4]    2.920311e-09 2.924578e-09 2.933145e-09  1836 1.0017786
mu[5]    7.293938e-07 7.304594e-07 7.325992e-07  1836 1.0017786
sigma[1] 3.149000e-12 3.188000e-12 3.272000e-12  1784 0.9996186
sigma[2] 3.050400e-11 3.088100e-11 3.169300e-11  1784 0.9996186
sigma[3] 7.324200e-11 7.414500e-11 7.609600e-11  1784 0.9996186
sigma[4] 7.907100e-11 8.004600e-11 8.215300e-11  1784 0.9996186
sigma[5] 1.919785e-06 1.943469e-06 1.994603e-06  1784 0.9996186

Samples were drawn using NUTS(diag_e) at Tue May 28 14:29:08 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_dgemm_m-2.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit,pars=c("mu[1]","mu[2]","mu[3]","mu[4]","mu[5]","sigma[1]","sigma[2]","sigma[3]","sigma[4]","sigma[5]"))
#+end_src

#+RESULTS:
[[file:./images/trace_dgemm_m-2.png]]

Once again, all the chains converge in the expected way.

#+begin_src R :results output :session *R* :exports both
extracted=rstan::extract(fit)

t_generated = data.frame(mnk=extracted$mnk_pos, duration=extracted$y_sim, origin='generated')
t_generated = t_generated[sample(nrow(t_generated), nrow(t)), ]
t=select(t,c("mnk","duration"))
t$origin = 'initial'
new_t = rbind(t, t_generated)
#+end_src

#+begin_src R :results output graphics :file ./images/generated_quantities_dgemm_m-2.png :exports both :width 600 :height 400 :session *R* 
ggplot(new_t, aes(mnk,duration,color=origin)) + geom_point(alpha=0.6)
#+end_src

#+RESULTS:
[[file:./images/generated_quantities_dgemm_m-2.png]]

Despite the addition of the other coefficients, it seems as if there
might still be a missing factor. This time the generated data
considers more points, and some farther from the mean, but there are
still some places where the points are not represented, like the ones
corresponding to a mnk of 4e+09. This might be simply caused by the
sampling though.

***** A second test with as many observations of mu_raw and sigma_raw as coefficients

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] m;
    vector[N] n;
    vector[N] k;
    vector[N] duration;
    vector[5] mu_mean;
    vector[5] mu_sd;
    vector[5] sigma_mean;
    vector[5] sigma_sd;
}
transformed data {
    vector[N] mnk;
    vector[N] mn;
    vector[N] mk;
    vector[N] nk;
    for(i in 1:N){
      mnk[i] = m[i]*n[i]*k[i];
      mn[i] = m[i]*n[i];
      mk[i] = m[i]*k[i];
      nk[i] = n[i]*k[i];
    }
}
parameters {
    vector[5] mu_raw;
    vector[5] sigma_raw;
}
transformed parameters {
    vector[5] mu;
    vector[5] sigma;
    for(j in 1:5){
      mu[j] = mu_mean[j] + mu_raw[j] * mu_sd[j]; //a test with mu_raw and sigma_raw as vectors
      sigma[j] = sigma_mean[j] + sigma_raw[j] * sigma_sd[j];
    }
}
model {
    mu_raw ~ normal(0,1);
    sigma_raw ~ normal(0,1);
    duration ~ normal(mu[1]*mnk + mu[2]*mn + mu[3]*mk + mu[4]*nk + mu[5], sigma[1]*mnk + sigma[2]*mn + sigma[3]*mk + sigma[4]*nk + sigma[5]);
}

generated quantities {
  real mnk_pos = mnk[categorical_rng(rep_vector(1,N) / N)];
  real mn_pos = mn[categorical_rng(rep_vector(1,N) / N)];
  real mk_pos = mk[categorical_rng(rep_vector(1,N) / N)];
  real nk_pos = nk[categorical_rng(rep_vector(1,N) / N)];
  real y_sim = normal_rng(mu[1]*mnk_pos + mu[2]*mn_pos + mu[3]*mk_pos + mu[4]*nk_pos + mu[5], sigma[1]*mnk_pos + sigma[2]*mn_pos + sigma[3]*mk_pos + sigma[4]*nk_pos + sigma[5]);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
t=sample_n(tmp,500)
dt = list(N=500,m=t$m,n=t$n,k=t$k,duration=t$duration,mu_mean=c(6.814095e-11,3.416642e-11,1.648690e-09,3.067383e-09,7.661272e-07),mu_sd=c(6.814095e-12,3.416642e-12,1.648690e-10,3.067383e-10,7.661272e-08),sigma_mean=c(1.224054e-12,1.185692e-11,2.846888e-11,3.073464e-11,7.462142e-07),sigma_sd=c(1.224054e-13,1.185692e-12,2.846888e-12,3.073464e-12,7.462142e-08))
#As usual, we use the values of the default case in Tom's analysis as prior

fit = sampling(sm,data=dt, iter=500, chains=8)
#+end_src

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15,pars=c("mu[1]","mu[2]","mu[3]","mu[4]","mu[5]","sigma[1]","sigma[2]","sigma[3]","sigma[4]","sigma[5]"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: ec98b735ff133e67a71659791e2e2fed.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                 mean      se_mean           sd         2.5%          25%
mu[1]    6.446800e-11 4.000000e-15 2.000000e-13 6.405400e-11 6.433900e-11
mu[2]    3.483800e-11 5.600000e-14 3.417000e-12 2.815500e-11 3.254500e-11
mu[3]    1.898298e-09 2.046000e-12 1.022110e-10 1.700526e-09 1.828444e-09
mu[4]    3.124828e-09 1.109000e-12 6.508000e-11 2.998773e-09 3.078611e-09
mu[5]    7.838633e-07 1.214177e-09 7.448801e-08 6.387858e-07 7.324592e-07
sigma[1] 3.404000e-12 1.000000e-15 6.100000e-14 3.289000e-12 3.361000e-12
sigma[2] 1.223500e-11 1.800000e-14 1.182000e-12 1.002000e-11 1.142100e-11
sigma[3] 3.001600e-11 4.800000e-14 2.839000e-12 2.460100e-11 2.811300e-11
sigma[4] 3.148200e-11 5.100000e-14 3.239000e-12 2.524000e-11 2.917900e-11
sigma[5] 7.850231e-07 1.083913e-09 7.040377e-08 6.517484e-07 7.352331e-07
                  50%          75%        97.5% n_eff      Rhat
mu[1]    6.447600e-11 6.460100e-11 6.486400e-11  2288 0.9992614
mu[2]    3.482700e-11 3.717600e-11 4.167300e-11  3759 0.9993119
mu[3]    1.898794e-09 1.967225e-09 2.095195e-09  2495 0.9994625
mu[4]    3.127444e-09 3.167501e-09 3.250138e-09  3445 0.9972609
mu[5]    7.841385e-07 8.367408e-07 9.249504e-07  3764 0.9980433
sigma[1] 3.404000e-12 3.446000e-12 3.524000e-12  3193 0.9976414
sigma[2] 1.221800e-11 1.298100e-11 1.469000e-11  4514 0.9970022
sigma[3] 3.005000e-11 3.197300e-11 3.559300e-11  3455 0.9978838
sigma[4] 3.146900e-11 3.365600e-11 3.778200e-11  4068 0.9984935
sigma[5] 7.843223e-07 8.331268e-07 9.298525e-07  4219 0.9988116

Samples were drawn using NUTS(diag_e) at Tue May 28 16:43:59 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/trace_dgemm_m-2_second_test.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit,pars=c("mu[1]","mu[2]","mu[3]","mu[4]","mu[5]","sigma[1]","sigma[2]","sigma[3]","sigma[4]","sigma[5]"))
#+end_src

#+RESULTS:
[[file:./images/trace_dgemm_m-2_second_test.png]]

#+begin_src R :results output :session *R* :exports both
extracted=rstan::extract(fit)

t_generated = data.frame(mnk=extracted$mnk_pos, duration=extracted$y_sim, origin='generated')
t_generated = t_generated[sample(nrow(t_generated), nrow(t)), ]
t=select(t,c("mnk","duration"))
t$origin = 'initial'
new_t = rbind(t, t_generated)
#+end_src

#+begin_src R :results output graphics :file ./images/generated_quantities_dgemm_m-2_second_test.png :exports both :width 600 :height 400 :session *R* 
ggplot(new_t, aes(mnk,duration,color=origin)) + geom_point(alpha=0.6)
#+end_src

#+RESULTS:
[[file:./images/generated_quantities_dgemm_m-2_second_test.png]]

There doesn't seem to be much difference between the two, but it might
be more correct to use different observations of mu_raw and sigma_raw
for the different coefficients.

EDIT: A talk with Tom confirmed this model is more correct, not only
because there shouldn't be the same mu_raw and sigma_raw for every
coefficient, but also because visually, this model covers more initial
data, and generates less observations where there weren't any.
*** 2019-05-29 wednesday
**** DONE Made a M_H-1 N_H-2 model for dgemm
:LOGBOOK:  
- State "DONE"       from "STARTED"    [2019-06-03 lun. 15:55]
:END:      
***** The model, a test with two CPU
****** With the same prior for everyone

#+begin_src R :results output :session *R* :exports both
library(rstan)
library(dplyr)

data=read.csv(file="~/Documents/org/dgemm.csv", header=TRUE, sep=",")
data=data.frame(as.numeric(data$m),as.numeric(data$n),as.numeric(data$k),data$duration,data$cpu)
names(data)=c("m","n","k","duration","cpu")

data$mnk=data$m * data$n * data$k
tmp=select(filter(data, data$mnk < 2e10),everything())
#+end_src

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    int hosts; //the number of hosts/CPUs
    matrix[hosts,N] mnk; //we have the same number of mnk and duration observations for each host
    matrix[hosts,N] duration;
    real it_mean; //for now we consider using the same prior for every host
    real it_sd;
    real mu_mean;
    real mu_sd;
    real sigma_mean;
    real sigma_sd;
    real n_mean;
    real n_sd;
}
parameters {
    vector[hosts] intercept_raw;
    vector[hosts] mu_raw;
    vector[hosts] sigma_raw;
    vector[hosts] noise_raw;
}
transformed parameters {
    vector[hosts] intercept;
    vector[hosts] mu;
    vector[hosts] sigma;
    vector[hosts] noise;
    for(i in 1:hosts){
      intercept[i]=it_mean + intercept_raw[i] * it_sd;
      mu[i]=mu_mean + mu_raw[i] * mu_sd;
      sigma[i]=sigma_mean + sigma_raw[i] * sigma_sd;
      noise[i]= n_mean + noise_raw[i] * n_sd;
    }
}
model {
    for(i in 1:hosts){
      intercept_raw[i] ~ normal(0,1);
      mu_raw[i] ~ normal(0,1);
      sigma_raw[i] ~ normal(0,1);
      noise_raw[i] ~ normal(0,1);
      duration[i] ~ normal(mu[i]*mnk[i]+intercept[i], sigma[i]*mnk[i]+noise[i]);
    }
}

generated quantities {
  vector[hosts] mnk_pos;
  vector[hosts] y_sim;
  for(i in 1:hosts){
     mnk_pos[i] = mnk[i][categorical_rng(rep_vector(1,N) / N)];
     y_sim[i] = normal_rng(mu[i]*mnk_pos[i]+intercept[i], sigma[i]*mnk_pos[i]+noise[i]);
  }
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
t2=sample_n(select(filter(tmp, tmp$cpu==2), everything()),500)
t4=sample_n(select(filter(tmp, tmp$cpu==4), everything()),500)

dt = list(N=500,hosts=2,mnk=rbind(t2$mnk,t4$mnk),duration=rbind(t2$duration,t4$duration),it_mean=7.661272e-07,it_sd=7.661272e-08,mu_mean=6.862693e-11,mu_sd=6.862693e-12,sigma_mean=1.895320e-12,sigma_sd=1.895320e-13,n_mean=7.462142e-07, n_sd=7.462142e-08)

fit = sampling(sm,data=dt, iter=500, chains=8)
#+end_src

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15, pars=c("intercept","mu","sigma","noise"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: e2a3c492674675bfaf628141eab8240d.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                     mean      se_mean           sd         2.5%          25%
intercept[1] 7.677801e-07 1.455665e-09 7.648153e-08 6.207045e-07 7.144779e-07
intercept[2] 7.577541e-07 1.291884e-09 7.530550e-08 6.123789e-07 7.074350e-07
mu[1]        7.027000e-11 4.000000e-15 2.130000e-13 6.986600e-11 7.013200e-11
mu[2]        6.887200e-11 4.000000e-15 2.330000e-13 6.839400e-11 6.872200e-11
sigma[1]     4.634000e-12 1.000000e-15 9.300000e-14 4.446000e-12 4.571000e-12
sigma[2]     5.317000e-12 1.000000e-15 9.300000e-14 5.143000e-12 5.253000e-12
noise[1]     7.424895e-07 1.283709e-09 7.598378e-08 5.971960e-07 6.907532e-07
noise[2]     7.359063e-07 1.223931e-09 7.261507e-08 5.941192e-07 6.876684e-07
                      50%          75%        97.5% n_eff      Rhat
intercept[1] 7.644885e-07 8.199637e-07 9.162530e-07  2761 0.9979879
intercept[2] 7.565275e-07 8.068684e-07 9.064296e-07  3398 0.9980183
mu[1]        7.026700e-11 7.041200e-11 7.068900e-11  2927 0.9997397
mu[2]        6.887400e-11 6.902800e-11 6.932600e-11  3054 0.9994879
sigma[1]     4.635000e-12 4.697000e-12 4.811000e-12  4423 0.9974519
sigma[2]     5.316000e-12 5.382000e-12 5.492000e-12  3951 1.0002668
noise[1]     7.431057e-07 7.922236e-07 8.954540e-07  3504 0.9979978
noise[2]     7.353928e-07 7.830687e-07 8.770024e-07  3520 0.9979922

Samples were drawn using NUTS(diag_e) at Wed May 29 14:13:13 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/dgemm_per_host_two_nodes_one_prior.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit, pars=c("intercept","mu","sigma","noise"))
#+end_src

#+RESULTS:
[[file:./images/dgemm_per_host_two_nodes_one_prior.png]]

#+begin_src R :results output :session *R* :exports both
extracted=rstan::extract(fit)

t_g_2 = data.frame(mnk=extracted$mnk_pos[,1], duration=extracted$y_sim[,1], origin="generated")
t_g_4 = data.frame(mnk=extracted$mnk_pos[,2], duration=extracted$y_sim[,2], origin="generated")
t_g_2 = t_g_2[sample(nrow(t_g_2), nrow(t2)), ]
t_g_4 = t_g_4[sample(nrow(t_g_4), nrow(t4)), ]
t2=select(t2,c("mnk","duration"))
t2$origin = 'initial'
t4=select(t4,c("mnk","duration"))
t4$origin = 'initial'

new_t2 = rbind(t2, t_g_2)
new_t4 = rbind(t4, t_g_4)
#+end_src

#+begin_src R :results output graphics :file ./images/generated_cpu2_one_prior.png :exports both :width 600 :height 400 :session *R* 
ggplot(new_t2, aes(mnk,duration,color=origin)) + geom_point(alpha=0.6)
#+end_src

#+RESULTS:
[[file:./images/generated_cpu2_one_prior.png]]

#+begin_src R :results output graphics :file ./images/generated_cpu4_one_prior.png :exports both :width 600 :height 400 :session *R* 
ggplot(new_t4, aes(mnk,duration,color=origin)) + geom_point(alpha=0.6)
#+end_src

#+RESULTS:
[[file:./images/generated_cpu4_one_prior.png]]

Once again we have some points that are not covered but that's because
we went back to a linear model. Otherwise, the results seem
reasonable, they are some differences between the two nodes but not
outstanding ones. We would now like to compare this with a case where
we give different priors.
****** With a different prior for each host
#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    int hosts;
    matrix[hosts,N] mnk;
    matrix[hosts,N] duration;
    vector[hosts] it_mean;
    vector[hosts] it_sd;
    vector[hosts] mu_mean;
    vector[hosts] mu_sd;
    vector[hosts] sigma_mean;
    vector[hosts] sigma_sd;
    vector[hosts] n_mean;
    vector[hosts] n_sd;
}
parameters {
    vector[hosts] intercept_raw;
    vector[hosts] mu_raw;
    vector[hosts] sigma_raw;
    vector[hosts] noise_raw;
}
transformed parameters {
    vector[hosts] intercept;
    vector[hosts] mu;
    vector[hosts] sigma;
    vector[hosts] noise;
    for(i in 1:hosts){
      intercept[i]=it_mean[i] + intercept_raw[i] * it_sd[i];
      mu[i]=mu_mean[i] + mu_raw[i] * mu_sd[i];
      sigma[i]=sigma_mean[i] + sigma_raw[i] * sigma_sd[i];
      noise[i]= n_mean[i] + noise_raw[i] * n_sd[i];
    }
}
model {
    for(i in 1:hosts){
      intercept_raw[i] ~ normal(0,1);
      mu_raw[i] ~ normal(0,1);
      sigma_raw[i] ~ normal(0,1);
      noise_raw[i] ~ normal(0,1);
      duration[i] ~ normal(mu[i]*mnk[i]+intercept[i], sigma[i]*mnk[i]+noise[i]);
    }
}

generated quantities {
  vector[hosts] mnk_pos;
  vector[hosts] y_sim;
  for(i in 1:hosts){
     mnk_pos[i] = mnk[i][categorical_rng(rep_vector(1,N) / N)];
     y_sim[i] = normal_rng(mu[i]*mnk_pos[i]+intercept[i], sigma[i]*mnk_pos[i]+noise[i]);
  }
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
t2=sample_n(select(filter(tmp, tmp$cpu==2), everything()),500)
t4=sample_n(select(filter(tmp, tmp$cpu==4), everything()),500)
dt = list(N=500,hosts=2,mnk=rbind(t2$mnk,t4$mnk),duration=rbind(t2$duration,t4$duration),it_mean=c(7.853125e-07,7.452431e-07),it_sd=c(7.853125e-08,7.452431e-08),mu_mean=c(6.973063e-11,6.792463e-11),mu_sd=c(6.973063e-12,6.792463e-12), sigma_mean=c(2.223536e-12,2.109653e-12),sigma_sd=c(2.223536e-13,2.109653e-13),n_mean=c(7.830495e-07,7.203726e-07), n_sd=c(7.830495e-08,7.203726e-08))

fit = sampling(sm,data=dt, iter=500, chains=8)
#+end_src

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15, pars=c("intercept","mu","sigma","noise"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 409a86ee4851a360e58fc04c0e8a5c5a.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                     mean      se_mean           sd         2.5%          25%
intercept[1] 7.880431e-07 1.276733e-09 7.692609e-08 6.381251e-07 7.355875e-07
intercept[2] 7.380009e-07 1.285357e-09 7.454907e-08 5.915043e-07 6.880249e-07
mu[1]        7.027800e-11 4.000000e-15 2.210000e-13 6.985700e-11 7.012600e-11
mu[2]        6.887400e-11 4.000000e-15 2.500000e-13 6.838900e-11 6.870700e-11
sigma[1]     4.895000e-12 2.000000e-15 1.010000e-13 4.699000e-12 4.825000e-12
sigma[2]     5.542000e-12 2.000000e-15 1.060000e-13 5.326000e-12 5.471000e-12
noise[1]     7.781871e-07 1.489335e-09 7.552529e-08 6.326489e-07 7.259371e-07
noise[2]     7.123988e-07 1.172191e-09 6.983055e-08 5.793920e-07 6.635591e-07
                      50%          75%        97.5% n_eff      Rhat
intercept[1] 7.878554e-07 8.414993e-07 9.344399e-07  3630 0.9974140
intercept[2] 7.379273e-07 7.872983e-07 8.811370e-07  3364 0.9999059
mu[1]        7.027400e-11 7.042800e-11 7.071700e-11  3094 0.9977677
mu[2]        6.887700e-11 6.904500e-11 6.935000e-11  3798 0.9974621
sigma[1]     4.892000e-12 4.964000e-12 5.097000e-12  3343 0.9969910
sigma[2]     5.542000e-12 5.612000e-12 5.753000e-12  3121 0.9983362
noise[1]     7.780691e-07 8.308313e-07 9.248879e-07  2572 1.0004661
noise[2]     7.129377e-07 7.612410e-07 8.463209e-07  3549 0.9987890

Samples were drawn using NUTS(diag_e) at Wed May 29 15:51:08 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/dgemm_per_host_two_nodes_multiple_prior.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit, pars=c("intercept","mu","sigma","noise"))
#+end_src

#+RESULTS:
[[file:./images/dgemm_per_host_two_nodes_multiple_prior.png]]

#+begin_src R :results output :session *R* :exports both
extracted=rstan::extract(fit)

t_g_2 = data.frame(mnk=extracted$mnk_pos[,1], duration=extracted$y_sim[,1], origin="generated")
t_g_4 = data.frame(mnk=extracted$mnk_pos[,2], duration=extracted$y_sim[,2], origin="generated")
t_g_2 = t_g_2[sample(nrow(t_g_2), nrow(t2)), ]
t_g_4 = t_g_4[sample(nrow(t_g_4), nrow(t4)), ]
t2=select(t2,c("mnk","duration"))
t2$origin = 'initial'
t4=select(t4,c("mnk","duration"))
t4$origin = 'initial'

new_t2 = rbind(t2, t_g_2)
new_t4 = rbind(t4, t_g_4)
#+end_src

#+begin_src R :results output graphics :file ./images/generated_cpu2_multiple_prior.png :exports both :width 600 :height 400 :session *R* 
ggplot(new_t2, aes(mnk,duration,color=origin)) + geom_point(alpha=0.6)
#+end_src

#+RESULTS:
[[file:./images/generated_cpu2_multiple_prior.png]]

#+begin_src R :results output graphics :file ./images/generated_cpu4_multiple_prior.png :exports both :width 600 :height 400 :session *R* 
ggplot(new_t4, aes(mnk,duration,color=origin)) + geom_point(alpha=0.6)
#+end_src

#+RESULTS:
[[file:./images/generated_cpu4_multiple_prior.png]]

There is no visible difference between giving the same prior and
giving a prior for each host with only two hosts. We'll make a test
with all of them.
***** A test with a slow CPU and a normal one
#+begin_src R :results output :session *R* :exports both
library(rstan)
library(dplyr)

data=read.csv(file="~/Documents/org/dgemm.csv", header=TRUE, sep=",")
data=data.frame(as.numeric(data$m),as.numeric(data$n),as.numeric(data$k),data$duration,data$cpu)
names(data)=c("m","n","k","duration","cpu")

data$mnk=data$m * data$n * data$k
tmp=select(filter(data, data$mnk < 2e10),everything())
#+end_src

For this test we use the same prior for every host since we
established no difference is visible with two nodes.
 
#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    int hosts;
    matrix[hosts,N] mnk;
    matrix[hosts,N] duration;
    real it_mean; 
    real it_sd;
    real mu_mean;
    real mu_sd;
    real sigma_mean;
    real sigma_sd;
    real n_mean;
    real n_sd;
}
parameters {
    vector[hosts] intercept_raw;
    vector[hosts] mu_raw;
    vector[hosts] sigma_raw;
    vector[hosts] noise_raw;
}
transformed parameters {
    vector[hosts] intercept;
    vector[hosts] mu;
    vector[hosts] sigma;
    vector[hosts] noise;
    for(i in 1:hosts){
      intercept[i]=it_mean + intercept_raw[i] * it_sd;
      mu[i]=mu_mean + mu_raw[i] * mu_sd;
      sigma[i]=sigma_mean + sigma_raw[i] * sigma_sd;
      noise[i]= n_mean + noise_raw[i] * n_sd;
    }
}
model {
    for(i in 1:hosts){
      intercept_raw[i] ~ normal(0,1);
      mu_raw[i] ~ normal(0,1);
      sigma_raw[i] ~ normal(0,1);
      noise_raw[i] ~ normal(0,1);
      duration[i] ~ normal(mu[i]*mnk[i]+intercept[i], sigma[i]*mnk[i]+noise[i]);
    }
}

generated quantities {
  vector[hosts] mnk_pos;
  vector[hosts] y_sim;
  for(i in 1:hosts){
     mnk_pos[i] = mnk[i][categorical_rng(rep_vector(1,N) / N)];
     y_sim[i] = normal_rng(mu[i]*mnk_pos[i]+intercept[i], sigma[i]*mnk_pos[i]+noise[i]);
  }
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
t2=sample_n(select(filter(tmp, tmp$cpu==2), everything()),500)
t26=sample_n(select(filter(tmp, tmp$cpu==26), everything()),500)

dt = list(N=500,hosts=2,mnk=rbind(t2$mnk,t26$mnk),duration=rbind(t2$duration,t26$duration),it_mean=7.661272e-07,it_sd=7.661272e-08,mu_mean=6.862693e-11,mu_sd=6.862693e-12,sigma_mean=1.895320e-12,sigma_sd=1.895320e-13,n_mean=7.462142e-07, n_sd=7.462142e-08)

fit = sampling(sm,data=dt, iter=500, chains=8)
#+end_src

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15, pars=c("intercept","mu","sigma","noise"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 9b2bdd7c71ff8d6f3f2897893ad7fcbd.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                     mean      se_mean           sd         2.5%          25%
intercept[1] 7.725405e-07 1.299108e-09 7.439195e-08 6.321099e-07 7.217062e-07
intercept[2] 7.602818e-07 1.532695e-09 8.106713e-08 6.033053e-07 7.042289e-07
mu[1]        7.025300e-11 4.000000e-15 2.110000e-13 6.986000e-11 7.010600e-11
mu[2]        8.996900e-11 5.000000e-15 2.810000e-13 8.942900e-11 8.978400e-11
sigma[1]     4.554000e-12 2.000000e-15 9.200000e-14 4.377000e-12 4.492000e-12
sigma[2]     6.293000e-12 2.000000e-15 9.300000e-14 6.114000e-12 6.232000e-12
noise[1]     7.512104e-07 1.109580e-09 7.128249e-08 6.138021e-07 7.034565e-07
noise[2]     7.430552e-07 1.323083e-09 7.445439e-08 5.998008e-07 6.908911e-07
                      50%          75%        97.5% n_eff      Rhat
intercept[1] 7.725553e-07 8.227731e-07 9.144704e-07  3279 0.9995215
intercept[2] 7.587789e-07 8.147785e-07 9.148914e-07  2798 0.9995793
mu[1]        7.025000e-11 7.039000e-11 7.068500e-11  3097 0.9986593
mu[2]        8.997200e-11 9.015400e-11 9.050100e-11  2795 0.9976523
sigma[1]     4.553000e-12 4.616000e-12 4.738000e-12  2906 0.9997997
sigma[2]     6.293000e-12 6.355000e-12 6.476000e-12  2692 0.9994789
noise[1]     7.495206e-07 7.990853e-07 8.934637e-07  4127 0.9974535
noise[2]     7.442758e-07 7.956390e-07 8.872298e-07  3167 0.9991897

Samples were drawn using NUTS(diag_e) at Wed May 29 16:57:52 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/dgemm_per_host_slow_node.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit, pars=c("intercept","mu","sigma","noise"))
#+end_src

#+RESULTS:
[[file:./images/dgemm_per_host_slow_node.png]]

#+begin_src R :results output :session *R* :exports both
extracted=rstan::extract(fit)

t_g_2 = data.frame(mnk=extracted$mnk_pos[,1], duration=extracted$y_sim[,1], origin="generated")
t_g_26 = data.frame(mnk=extracted$mnk_pos[,2], duration=extracted$y_sim[,2], origin="generated")
t_g_2 = t_g_2[sample(nrow(t_g_2), nrow(t2)), ]
t_g_26 = t_g_26[sample(nrow(t_g_26), nrow(t26)), ]
t2=select(t2,c("mnk","duration"))
t2$origin = 'initial'
t26=select(t26,c("mnk","duration"))
t26$origin = 'initial'

new_t2 = rbind(t2, t_g_2)
new_t26 = rbind(t26, t_g_26)
#+end_src

#+begin_src R :results output graphics :file ./images/generated_cpu2_normal_node.png :exports both :width 600 :height 400 :session *R* 
ggplot(new_t2, aes(mnk,duration,color=origin)) + geom_point(alpha=0.6)
#+end_src

#+RESULTS:
[[file:./images/generated_cpu2_normal_node.png]]

#+begin_src R :results output graphics :file ./images/generated_cpu26_one_prior.png :exports both :width 600 :height 400 :session *R* 
ggplot(new_t26, aes(mnk,duration,color=origin)) + geom_point(alpha=0.6)
#+end_src

#+RESULTS:
[[file:./images/generated_cpu26_one_prior.png]]

With a normal node and a slower one, the results are once again very
accurate and the generated data is very close from the initial one,
with the exception of some points, for which we need to consider the
mn, mk and nk coefficients.

***** DONE A test with all the CPUs
:LOGBOOK: 
- State "DONE"       from "STARTED"    [2019-06-03 lun. 15:55]
- State "STARTED"    from "TODO"       [2019-06-03 lun. 14:42]
:END:      
****** With Tom's data

For this test, we put all of Tom's data for each node in a csv file,
and then import it to use it as priors (with a prior for each CPU).

#+begin_src R :results output :session *R* :exports both
library(rstan)
library(dplyr)

data=read.csv(file="~/Documents/org/dgemm.csv", header=TRUE, sep=",")
data=data.frame(as.numeric(data$m),as.numeric(data$n),as.numeric(data$k),data$duration,data$cpu)
names(data)=c("m","n","k","duration","cpu")

data$mnk=data$m * data$n * data$k
tmp=select(filter(data, data$mnk < 2e10),everything())
#+end_src

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    int hosts;
    matrix[hosts,N] mnk;
    matrix[hosts,N] duration;
    vector[hosts] it_mean;
    vector[hosts] it_sd;
    vector[hosts] mu_mean;
    vector[hosts] mu_sd;
    vector[hosts] sigma_mean;
    vector[hosts] sigma_sd;
    vector[hosts] n_mean;
    vector[hosts] n_sd;
}
parameters {
    vector[hosts] intercept_raw;
    vector[hosts] mu_raw;
    vector[hosts] sigma_raw;
    vector[hosts] noise_raw;
}
transformed parameters {
    vector[hosts] intercept;
    vector[hosts] mu;
    vector[hosts] sigma;
    vector[hosts] noise;
    for(i in 1:hosts){
      intercept[i]=it_mean[i] + intercept_raw[i] * it_sd[i];
      mu[i]=mu_mean[i] + mu_raw[i] * mu_sd[i];
      sigma[i]=sigma_mean[i] + sigma_raw[i] * sigma_sd[i];
      noise[i]= n_mean[i] + noise_raw[i] * n_sd[i];
    }
}
model {
    for(i in 1:hosts){
      intercept_raw[i] ~ normal(0,1);
      mu_raw[i] ~ normal(0,1);
      sigma_raw[i] ~ normal(0,1);
      noise_raw[i] ~ normal(0,1);
      duration[i] ~ normal(mu[i]*mnk[i]+intercept[i], sigma[i]*mnk[i]+noise[i]);
    }
}

generated quantities {
  vector[hosts] mnk_pos;
  vector[hosts] y_sim;
  for(i in 1:hosts){
     mnk_pos[i] = mnk[i][categorical_rng(rep_vector(1,N) / N)];
     y_sim[i] = normal_rng(mu[i]*mnk_pos[i]+intercept[i], sigma[i]*mnk_pos[i]+noise[i]);
  }
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
priors=read.csv(file="~/Documents/org/priors.csv", header=TRUE, sep=",")

t_i=list()
for(i in 1:nrow(priors)){
  t_i[[i]]=sample_n(select(filter(tmp, tmp$cpu==i+1), c("mnk","duration")),500)
}
mnk=lapply(t_i, '[[', 1)
duration=lapply(t_i, '[[', 2)
#+end_src

This time we make the fit with only 4 chains because the time needed
would be too long otherwise, due to the amount of data.

#+begin_src R :results output :session *R* :exports both
dt = list(N=500,hosts=nrow(priors),mnk=mnk,duration=duration,it_mean=priors$intercept,it_sd=priors$intercept_sd,mu_mean=priors$mu,mu_sd=priors$mu_sd,sigma_mean=priors$sigma,sigma_sd=priors$sigma_sd,n_mean=priors$noise,n_sd=priors$noise_sd)

fit = sampling(sm,data=dt, iter=500, chains=4)
#+end_src

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15,pars="mu")
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 409a86ee4851a360e58fc04c0e8a5c5a.
4 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=1000.

             mean se_mean       sd       2.5%        25%        50%        75%
mu[1]  7.0673e-11   5e-15 2.36e-13 7.0188e-11 7.0526e-11 7.0667e-11 7.0820e-11
mu[2]  6.7959e-11   3e-15 1.67e-13 6.7638e-11 6.7842e-11 6.7957e-11 6.8071e-11
mu[3]  6.9190e-11   6e-15 2.69e-13 6.8653e-11 6.9007e-11 6.9197e-11 6.9373e-11
mu[4]  6.7980e-11   4e-15 2.53e-13 6.7502e-11 6.7809e-11 6.7975e-11 6.8153e-11
mu[5]  6.8638e-11   4e-15 2.04e-13 6.8207e-11 6.8514e-11 6.8642e-11 6.8771e-11
mu[6]  6.8578e-11   5e-15 2.25e-13 6.8127e-11 6.8421e-11 6.8579e-11 6.8734e-11
mu[7]  6.7977e-11   3e-15 1.97e-13 6.7581e-11 6.7849e-11 6.7980e-11 6.8106e-11
mu[8]  6.7894e-11   4e-15 2.19e-13 6.7439e-11 6.7743e-11 6.7893e-11 6.8036e-11
mu[9]  6.9169e-11   5e-15 2.53e-13 6.8663e-11 6.9002e-11 6.9171e-11 6.9342e-11
mu[10] 6.7949e-11   4e-15 2.20e-13 6.7521e-11 6.7802e-11 6.7946e-11 6.8099e-11
mu[11] 7.0292e-11   5e-15 2.11e-13 6.9891e-11 7.0147e-11 7.0297e-11 7.0437e-11
mu[12] 6.7671e-11   4e-15 2.18e-13 6.7257e-11 6.7515e-11 6.7669e-11 6.7820e-11
mu[13] 6.8262e-11   4e-15 2.28e-13 6.7828e-11 6.8107e-11 6.8252e-11 6.8412e-11
mu[14] 6.7642e-11   4e-15 2.17e-13 6.7237e-11 6.7491e-11 6.7651e-11 6.7789e-11
mu[15] 7.0022e-11   6e-15 3.06e-13 6.9403e-11 6.9815e-11 7.0034e-11 7.0232e-11
mu[16] 6.8124e-11   5e-15 2.34e-13 6.7667e-11 6.7963e-11 6.8124e-11 6.8292e-11
mu[17] 6.7828e-11   5e-15 2.10e-13 6.7428e-11 6.7682e-11 6.7829e-11 6.7969e-11
mu[18] 6.7143e-11   3e-15 1.45e-13 6.6847e-11 6.7050e-11 6.7143e-11 6.7234e-11
mu[19] 6.8206e-11   7e-15 2.41e-13 6.7711e-11 6.8051e-11 6.8219e-11 6.8362e-11
mu[20] 6.7919e-11   4e-15 2.16e-13 6.7502e-11 6.7773e-11 6.7924e-11 6.8059e-11
mu[21] 6.7767e-11   4e-15 2.28e-13 6.7349e-11 6.7607e-11 6.7759e-11 6.7935e-11
mu[22] 6.8677e-11   6e-15 2.64e-13 6.8150e-11 6.8498e-11 6.8690e-11 6.8862e-11
mu[23] 6.8016e-11   4e-15 2.11e-13 6.7603e-11 6.7870e-11 6.8015e-11 6.8162e-11
mu[24] 6.8079e-11   4e-15 2.29e-13 6.7636e-11 6.7925e-11 6.8074e-11 6.8229e-11
mu[25] 8.9455e-11   7e-15 3.73e-13 8.8719e-11 8.9206e-11 8.9456e-11 8.9711e-11
mu[26] 7.3637e-11   4e-15 1.82e-13 7.3287e-11 7.3522e-11 7.3637e-11 7.3762e-11
mu[27] 7.6223e-11   6e-15 2.64e-13 7.5710e-11 7.6039e-11 7.6227e-11 7.6405e-11
mu[28] 7.6354e-11   5e-15 3.06e-13 7.5738e-11 7.6163e-11 7.6356e-11 7.6552e-11
mu[29] 8.3533e-11   5e-15 2.79e-13 8.3006e-11 8.3355e-11 8.3529e-11 8.3712e-11
mu[30] 8.2998e-11   7e-15 3.31e-13 8.2325e-11 8.2802e-11 8.2998e-11 8.3216e-11
mu[31] 7.7764e-11   5e-15 2.62e-13 7.7228e-11 7.7609e-11 7.7767e-11 7.7931e-11
mu[32] 7.8100e-11   7e-15 3.34e-13 7.7451e-11 7.7881e-11 7.8103e-11 7.8316e-11
mu[33] 7.2543e-11   6e-15 2.31e-13 7.2091e-11 7.2390e-11 7.2538e-11 7.2695e-11
mu[34] 6.8715e-11   5e-15 2.29e-13 6.8268e-11 6.8553e-11 6.8712e-11 6.8884e-11
mu[35] 7.1886e-11   4e-15 2.25e-13 7.1437e-11 7.1737e-11 7.1885e-11 7.2031e-11
mu[36] 6.7543e-11   3e-15 1.89e-13 6.7167e-11 6.7423e-11 6.7541e-11 6.7653e-11
mu[37] 6.7951e-11   4e-15 2.08e-13 6.7562e-11 6.7804e-11 6.7951e-11 6.8097e-11
mu[38] 6.8406e-11   4e-15 2.42e-13 6.7902e-11 6.8259e-11 6.8408e-11 6.8558e-11
mu[39] 6.8673e-11   4e-15 2.36e-13 6.8237e-11 6.8506e-11 6.8674e-11 6.8826e-11
mu[40] 6.8126e-11   4e-15 2.08e-13 6.7731e-11 6.7990e-11 6.8127e-11 6.8260e-11
mu[41] 6.8210e-11   4e-15 2.22e-13 6.7775e-11 6.8062e-11 6.8212e-11 6.8358e-11
mu[42] 6.8206e-11   4e-15 2.30e-13 6.7780e-11 6.8053e-11 6.8194e-11 6.8369e-11
mu[43] 6.8082e-11   5e-15 2.29e-13 6.7622e-11 6.7936e-11 6.8083e-11 6.8238e-11
mu[44] 6.8564e-11   5e-15 2.30e-13 6.8105e-11 6.8413e-11 6.8568e-11 6.8714e-11
mu[45] 6.8487e-11   4e-15 2.27e-13 6.8032e-11 6.8334e-11 6.8486e-11 6.8647e-11
mu[46] 6.7606e-11   5e-15 2.06e-13 6.7212e-11 6.7464e-11 6.7606e-11 6.7747e-11
mu[47] 6.8158e-11   4e-15 2.03e-13 6.7763e-11 6.8015e-11 6.8160e-11 6.8288e-11
mu[48] 6.7818e-11   4e-15 2.06e-13 6.7413e-11 6.7681e-11 6.7818e-11 6.7950e-11
mu[49] 7.2391e-11   3e-15 1.92e-13 7.2025e-11 7.2259e-11 7.2391e-11 7.2523e-11
mu[50] 6.7120e-11   4e-15 2.07e-13 6.6726e-11 6.6973e-11 6.7116e-11 6.7261e-11
mu[51] 6.8489e-11   5e-15 2.24e-13 6.8058e-11 6.8323e-11 6.8489e-11 6.8652e-11
mu[52] 6.8530e-11   5e-15 2.30e-13 6.8073e-11 6.8375e-11 6.8525e-11 6.8691e-11
mu[53] 6.8417e-11   4e-15 2.13e-13 6.8007e-11 6.8267e-11 6.8417e-11 6.8570e-11
mu[54] 6.8666e-11   5e-15 2.36e-13 6.8217e-11 6.8500e-11 6.8671e-11 6.8835e-11
mu[55] 6.9781e-11   5e-15 2.48e-13 6.9316e-11 6.9620e-11 6.9765e-11 6.9947e-11
mu[56] 6.8374e-11   5e-15 2.33e-13 6.7911e-11 6.8221e-11 6.8374e-11 6.8531e-11
mu[57] 6.8422e-11   4e-15 2.28e-13 6.7994e-11 6.8262e-11 6.8429e-11 6.8580e-11
mu[58] 6.8058e-11   4e-15 2.19e-13 6.7633e-11 6.7921e-11 6.8062e-11 6.8197e-11
mu[59] 6.8228e-11   4e-15 2.04e-13 6.7836e-11 6.8095e-11 6.8235e-11 6.8358e-11
mu[60] 6.7719e-11   4e-15 1.94e-13 6.7342e-11 6.7579e-11 6.7725e-11 6.7856e-11
mu[61] 6.8334e-11   4e-15 2.50e-13 6.7862e-11 6.8163e-11 6.8328e-11 6.8503e-11
mu[62] 6.8226e-11   5e-15 2.38e-13 6.7756e-11 6.8072e-11 6.8230e-11 6.8391e-11
mu[63] 6.7717e-11   4e-15 1.94e-13 6.7320e-11 6.7599e-11 6.7716e-11 6.7842e-11
mu[64] 6.7795e-11   4e-15 2.13e-13 6.7373e-11 6.7651e-11 6.7788e-11 6.7944e-11
            97.5% n_eff      Rhat
mu[1]  7.1142e-11  1951 0.9973934
mu[2]  6.8280e-11  3050 0.9970512
mu[3]  6.9716e-11  2266 0.9972631
mu[4]  6.8481e-11  3809 0.9974811
mu[5]  6.9034e-11  2462 0.9988577
mu[6]  6.9035e-11  2025 0.9994664
mu[7]  6.8379e-11  3498 0.9974239
mu[8]  6.8346e-11  3204 0.9961857
mu[9]  6.9660e-11  2647 0.9986371
mu[10] 6.8379e-11  2563 0.9978885
mu[11] 7.0694e-11  2028 0.9992098
mu[12] 6.8110e-11  3143 0.9975753
mu[13] 6.8721e-11  2740 0.9971963
mu[14] 6.8059e-11  3037 0.9971414
mu[15] 7.0596e-11  2639 0.9977692
mu[16] 6.8560e-11  2473 0.9976742
mu[17] 6.8236e-11  2017 0.9970223
mu[18] 6.7436e-11  2503 0.9979427
mu[19] 6.8646e-11  1368 1.0009566
mu[20] 6.8339e-11  2773 0.9968883
mu[21] 6.8189e-11  2851 0.9968978
mu[22] 6.9174e-11  2046 0.9984675
mu[23] 6.8411e-11  2403 0.9976797
mu[24] 6.8522e-11  2634 0.9975007
mu[25] 9.0174e-11  3243 0.9972513
mu[26] 7.4005e-11  2222 0.9988651
mu[27] 7.6739e-11  2201 0.9981781
mu[28] 7.6959e-11  3123 0.9970003
mu[29] 8.4097e-11  3490 0.9967337
mu[30] 8.3627e-11  2333 0.9974177
mu[31] 7.8280e-11  3153 0.9987105
mu[32] 7.8737e-11  2623 0.9979889
mu[33] 7.2983e-11  1736 0.9987888
mu[34] 6.9150e-11  2346 0.9975152
mu[35] 7.2322e-11  2695 0.9977149
mu[36] 6.7918e-11  3319 0.9969071
mu[37] 6.8342e-11  2482 0.9963745
mu[38] 6.8891e-11  3924 0.9972818
mu[39] 6.9128e-11  3504 0.9970264
mu[40] 6.8563e-11  2212 0.9992765
mu[41] 6.8624e-11  2492 0.9970435
mu[42] 6.8651e-11  2735 0.9975134
mu[43] 6.8526e-11  2320 0.9971229
mu[44] 6.9023e-11  2483 0.9971691
mu[45] 6.8931e-11  3040 0.9974746
mu[46] 6.8005e-11  2000 0.9979467
mu[47] 6.8560e-11  2579 0.9979061
mu[48] 6.8210e-11  2311 0.9992954
mu[49] 7.2751e-11  3234 0.9971189
mu[50] 6.7520e-11  2959 0.9973144
mu[51] 6.8934e-11  2424 0.9972709
mu[52] 6.8983e-11  2383 0.9981854
mu[53] 6.8807e-11  3547 0.9968613
mu[54] 6.9106e-11  2537 0.9973662
mu[55] 7.0290e-11  2683 0.9970602
mu[56] 6.8822e-11  2367 0.9983651
mu[57] 6.8862e-11  3462 0.9976889
mu[58] 6.8490e-11  2652 0.9965017
mu[59] 6.8623e-11  2642 0.9965458
mu[60] 6.8094e-11  2920 0.9968991
mu[61] 6.8814e-11  4781 0.9974511
mu[62] 6.8679e-11  2442 0.9968791
mu[63] 6.8098e-11  2868 0.9976047
mu[64] 6.8214e-11  3372 0.9981437

Samples were drawn using NUTS(diag_e) at Mon Jun  3 13:42:49 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

Due to the amount of found parameters, we won't bother showing the
traces, especially since the fitting returned no warnings, so we can
assume the traces must be correct. We won't print all of the values
either for the same reason, especially since we've seen our model
seems correct enough with less hosts, so we can assume if the values
for one parameter are correct, the same goes for the other parameters.

#+begin_src R :results output :session *R* :exports both
extracted=rstan::extract(fit)

t_g_64 = data.frame(mnk=extracted$mnk_pos[,64], duration=extracted$y_sim[,64], origin="generated")
t_g_64 = t_g_64[sample(nrow(t_g_64), nrow(t_i[[1]])), ]
t64=t_i[[64]]
t64$origin = 'initial'
new_t64 = rbind(t64, t_g_64)
#+end_src

As before, we won't show the difference between the real data and the
one generated by stan for each host because that wouldn't be very
useful.

#+begin_src R :results output graphics :file ./images/generated_per_host_node_64_multiple_priors.png :exports both :width 600 :height 400 :session *R* 
ggplot(new_t64, aes(mnk,duration,color=origin)) + geom_point(alpha=0.6)
#+end_src

#+RESULTS:
[[file:./images/generated_per_host_node_64_multiple_priors.png]]

Now we'll compare this result with the a model where we give only give
one prior for all the hosts.

****** With the same prior for every node

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    int hosts;
    matrix[hosts,N] mnk;
    matrix[hosts,N] duration;
    real it_mean;
    real it_sd;
    real mu_mean;
    real mu_sd;
    real sigma_mean;
    real sigma_sd;
    real n_mean;
    real n_sd;
}
parameters {
    vector[hosts] intercept_raw;
    vector[hosts] mu_raw;
    vector[hosts] sigma_raw;
    vector[hosts] noise_raw;
}
transformed parameters {
    vector[hosts] intercept;
    vector[hosts] mu;
    vector[hosts] sigma;
    vector[hosts] noise;
    for(i in 1:hosts){
      intercept[i]=it_mean + intercept_raw[i] * it_sd;
      mu[i]=mu_mean + mu_raw[i] * mu_sd;
      sigma[i]=sigma_mean + sigma_raw[i] * sigma_sd;
      noise[i]= n_mean + noise_raw[i] * n_sd;
    }
}
model {
    for(i in 1:hosts){
      intercept_raw[i] ~ normal(0,1);
      mu_raw[i] ~ normal(0,1);
      sigma_raw[i] ~ normal(0,1);
      noise_raw[i] ~ normal(0,1);
      duration[i] ~ normal(mu[i]*mnk[i]+intercept[i], sigma[i]*mnk[i]+noise[i]);
    }
}

generated quantities {
  vector[hosts] mnk_pos;
  vector[hosts] y_sim;
  for(i in 1:hosts){
     mnk_pos[i] = mnk[i][categorical_rng(rep_vector(1,N) / N)];
     y_sim[i] = normal_rng(mu[i]*mnk_pos[i]+intercept[i], sigma[i]*mnk_pos[i]+noise[i]);
  }
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+begin_src R :results output :session *R* :exports both
t_i=list()
for(i in 1:nrow(priors)){
  t_i[[i]]=sample_n(select(filter(tmp, tmp$cpu==i+1), c("mnk","duration")),500)
}
mnk=lapply(t_i, '[[', 1)
duration=lapply(t_i, '[[', 2)
#+end_src

Once again we use the default values as priors.

#+begin_src R :results output :session *R* :exports both
dt = list(N=500,hosts=nrow(priors),mnk=mnk,duration=duration,it_mean=7.661272e-07,it_sd=7.661272e-08,mu_mean=6.862693e-11,mu_sd=6.862693e-12,sigma_mean=1.895320e-12,sigma_sd=1.895320e-13,n_mean=7.462142e-07,n_sd=7.462142e-08)

fit = sampling(sm,data=dt, iter=500, chains=4)
#+end_src

#+begin_src R :results output :session *R* :exports both
extracted=rstan::extract(fit)

t_g_64 = data.frame(mnk=extracted$mnk_pos[,64], duration=extracted$y_sim[,64], origin="generated")
t_g_64 = t_g_64[sample(nrow(t_g_64), nrow(t_i[[1]])), ]
t64=t_i[[64]]
t64$origin = 'initial'
new_t64 = rbind(t64, t_g_64)
#+end_src

#+begin_src R :results output graphics :file ./images/generated_per_host_node_64_one_prior.png :exports both :width 600 :height 400 :session *R* 
ggplot(new_t64, aes(mnk,duration,color=origin)) + geom_point(alpha=0.6)
#+end_src

#+RESULTS:
[[file:./images/generated_per_host_node_64_one_prior.png]]
There's not much difference (at the scale of one CPU) between a general
prior and a more precise one.

** 2019-06 june
*** 2019-06-03 monday
**** DONE Adapt the model to a M_H-2 N_H-2 model
:LOGBOOK:  
- State "DONE"       from "STARTED" [2019-06-04 mar. 11:26]
- State "STARTED"    from "TODO"       [2019-06-03 lun. 16:13]
:END:      
#+begin_src R :results output :session *R* :exports both
library(rstan)
library(dplyr)

data=read.csv(file="~/Documents/org/dgemm.csv", header=TRUE, sep=",")
data=data.frame(as.numeric(data$m),as.numeric(data$n),as.numeric(data$k),data$duration,data$cpu)
names(data)=c("m","n","k","duration","cpu")

data$mnk=data$m * data$n * data$k
tmp=select(filter(data, data$mnk < 2e10),everything())
#+end_src

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    int hosts;
    matrix[hosts,N] m;
    matrix[hosts,N] n;
    matrix[hosts,N] k;
    matrix[hosts,N] duration;
    vector[5] mu_mean; //mu[1]=mnk, mu[2]=mn, mu[3]=mk, mu[4]=nk, mu[5]=intercept
    vector[5] mu_sd;
    vector[5] sigma_mean;
    vector[5] sigma_sd;
}
transformed data {
    matrix[hosts,N] mnk;
    matrix[hosts,N] mn;
    matrix[hosts,N] mk;
    matrix[hosts,N] nk;
    for(i in 1:hosts){
      for(j in 1:N){
        mnk[i,j] = m[i,j]*n[i,j]*k[i,j];
        mn[i,j] = m[i,j]*n[i,j];
        mk[i,j] = m[i,j]*k[i,j];
        nk[i,j] = n[i,j]*k[i,j];
      }
    }
}
parameters {
    matrix[hosts,5] mu_raw;
    matrix[hosts,5] sigma_raw;
}
transformed parameters {
    matrix[hosts,5] mu;
    matrix[hosts,5] sigma;
    for(i in 1:hosts){
      for(j in 1:5){
        mu[i,j] = mu_mean[j] + mu_raw[i,j] * mu_sd[j];
        sigma[i,j] = sigma_mean[j] + sigma_raw[i,j] * sigma_sd[j];
      }
    }
}
model {
    for(i in 1:hosts){
      mu_raw[i] ~ normal(0,1);
      sigma_raw[i] ~ normal(0,1);
      duration[i] ~ normal(mu[i,1]*mnk[i]+mu[i,2]*mn[i]+mu[i,3]*mk[i]+mu[i,4]*nk[i]+mu[i,5], sigma[i,1]*mnk[i]+sigma[i,2]*mn[i]+sigma[i,3]*mk[i]+sigma[i,4]*nk[i]+sigma[i,5]);
    }
}

generated quantities {
  vector[hosts] mnk_pos;
  vector[hosts] mn_pos;
  vector[hosts] mk_pos;
  vector[hosts] nk_pos;
  vector[hosts] y_sim;
  for(i in 1:hosts){
     mnk_pos[i] = mnk[i][categorical_rng(rep_vector(1,N) / N)];
     mn_pos[i] = mn[i][categorical_rng(rep_vector(1,N) / N)];
     mk_pos[i] = mk[i][categorical_rng(rep_vector(1,N) / N)];
     nk_pos[i] = nk[i][categorical_rng(rep_vector(1,N) / N)];
     y_sim[i] = normal_rng(mu[i,1]*mnk_pos[i]+mu[i,2]*mn_pos[i]+mu[i,3]*mk_pos[i]+mu[i,4]*nk_pos[i]+mu[i,5], sigma[i,1]*mnk_pos[i]+sigma[i,2]*mn_pos[i]+sigma[i,3]*mk_pos[i]+sigma[i,4]*nk_pos[i]+sigma[i,5]);
  }
}
"
sm = stan_model(model_code = modelString)
#+end_src

Since we now know there isn't much difference when giving a different
prior for the parameters of each host (compared to one general prior),
we'll only use one prior for this example. We'll also only use two
nodes, otherwise the fit takes ages.

#+begin_src R :results output :session *R* :exports both
t_i=list()
t_i[[1]]=sample_n(select(filter(tmp, tmp$cpu==26), c("m","n","k", "mnk", "duration")),500)
t_i[[2]]=sample_n(select(filter(tmp, tmp$cpu==64), c("m","n","k", "mnk", "duration")),500)

m=lapply(t_i, '[[', 1)
n=lapply(t_i, '[[', 2)
k=lapply(t_i, '[[', 3)
duration=lapply(t_i, '[[', 5)
#+end_src

#+begin_src R :results output :session *R* :exports both
dt = list(N=500,hosts=2,m=m,n=n,k=k,duration=duration,mu_mean=c(6.814095e-11,3.416642e-11,1.648690e-09,3.067383e-09,7.661272e-07),mu_sd=c(6.814095e-12,3.416642e-12,1.648690e-10,3.067383e-10,7.661272e-08),sigma_mean=c(1.224054e-12,1.185692e-11,2.846888e-11,3.073464e-11,7.462142e-07),sigma_sd=c(1.224054e-13,1.185692e-12,2.846888e-12,3.073464e-12,7.462142e-08))

fit = sampling(sm,data=dt, iter=500, chains=4)
#+end_src

#+begin_src R :results output :session *R* :exports both
print(fit,digits=15, pars=c("mu[1,1]","mu[2,1]"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 5b1503bbc291305d951fdfd893f403bd.
4 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=1000.

              mean se_mean       sd       2.5%        25%        50%        75%
mu[1,1] 8.3366e-11   6e-15 1.94e-13 8.2960e-11 8.3239e-11 8.3372e-11 8.3502e-11
mu[2,1] 6.2206e-11   2e-15 6.70e-14 6.2073e-11 6.2159e-11 6.2207e-11 6.2251e-11
             97.5% n_eff      Rhat
mu[1,1] 8.3718e-11   970 0.9973274
mu[2,1] 6.2340e-11   966 1.0019734

Samples were drawn using NUTS(diag_e) at Tue Jun  4 11:07:47 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

As expected the mnk coefficient is higher on the CPU number 26, which
was known as slower.

#+begin_src R :results output graphics :file ./images/traces_mh2_nh2.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit, pars=c("mu","sigma"))
#+end_src

#+RESULTS:
[[file:./images/traces_mh2_nh2.png]]

#+begin_src R :results output :session *R* :exports both
extracted=rstan::extract(fit)

t_g_26 = data.frame(mnk=extracted$mnk_pos[,1], duration=extracted$y_sim[,1], origin="generated")
t_g_26 = t_g_26[sample(nrow(t_g_26), nrow(t_i[[1]])), ]
t26=t_i[[1]]
t26=select(t26,c("mnk","duration"))
t26$origin = 'initial'
new_t26 = rbind(t26, t_g_26)

t_g_64 = data.frame(mnk=extracted$mnk_pos[,2], duration=extracted$y_sim[,2], origin="generated")
t_g_64 = t_g_64[sample(nrow(t_g_64), nrow(t_i[[2]])), ]
t64=t_i[[2]]
t64=select(t64,c("mnk","duration"))
t64$origin = 'initial'
new_t64 = rbind(t64, t_g_64)
#+end_src

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R* 
ggplot(new_t26, aes(mnk,duration,color=origin)) + geom_point(alpha=0.6)
#+end_src

#+RESULTS:
[[file:/tmp/babel-5316MYa/figure5316Zig.png]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R* 
ggplot(new_t64, aes(mnk,duration,color=origin)) + geom_point(alpha=0.6)
#+end_src

#+RESULTS:
[[file:/tmp/babel-5316MYa/figure5316msm.png]]

