# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages logdrawer
#+TITLE:       Journal
#+AUTHOR:      Hoël Jalmin
#+LANGUAGE:    en
#+TAGS: [ PROGRAMMING : R(r) STAN(S) ]
#+TAGS: [ TOOLS : ORGMODE(o) GIT(g)  ]
#+TAGS: PAPER(P) VIDEO(V) MEETING(m) SEMINAR(s)
#+SEQ_TODO: TODO(t!) STARTED(s!) INTERRUPTED(i!) DONE(d!) 
#+SEQ_TODO: REWORK(r!)
#+SEQ_TODO: EDIT(e!)

* Introduction
This file is the report for my internship, about automating
calculation kernels' modellings. I'll explain more about this later on.

* 2019
** 2019-04 april
*** 2019-04-30 monday
**** DONE Read [[https://hal.inria.fr/hal-02096571/document ][Faithful and Efficient Simulation of High Performance Linpack]] :PAPER:
:LOGBOOK:  
- State "DONE"       from "STARTED"    [2019-05-02 thursday 16:53]
:END:      
***** Introduction
With a power consumption of several MW per hour on a TOP500 machine,
running applications on supercomputers at scale solely to optimize
their performance is extremely expensive. Likewise, High-Performance
Linpack (HPL), the benchmark used to ran supercomputers in the TOP500,
requires a careful tuning of many parameters (problem size, grid
arrangement, granularity, collective operation algorithms, etc.) and
supports exploration of the most common and fundamental performance
issues and their solutions. In this article, we explain how we both
extended the SimGrid's SMPI simulator and slightly modified the
open source version of HPL to allow a fast emulation on a single
commodity server at the scale of a supercomputer. We explain how to
model the different components (network, BLAS, ...) and show that a
careful modeling of both spatial and temporal node variability allows us
to obtain predictions within a few percents of real experiments.

***** Notes 
The paper explains how to extend the Simgrid simulator to simulate the
HPL benchmark and get the rough performance of it, as it is quite
representative of usual performance issues, without actually running
it. The simulation reduces considerably memory consumption compared to
a normal run of HPL, so it can be ran on a smaller cluster with less
time. Other simulation models for HPL already existed, but were quite
inaccurate as they did not take all the parameters in account.

****** About HPL
HPL is a parallel implementation of a benchmark which measures how
fast a computer can solve a system of linear equations. HPL uses the
BLAS libraries to make matrix operations such as multiplication (using
dgemm). The sequential complexity depends on the matrix size: 
2/3N³ + 2N² + O(N), but the calculation time cannot be determined
without running HPL or a simulation of it (it depends on network
capacity and a lot of parameters). Even while running HPL there's a
difference between the theoretical peak performance and the actual one
reached depending on how MPI communications are done.


****** About SimGrid and MPI simulation
There are two approaches to MPI simulation: offline and online. The
first one gives a previously obtained trace of the application to a
simulator, which makes predictions depending on the performance
models. The issue is that a first run is necessary to get the trace,
which is only valid at the moment of the run, since it depicts a
precise behavior. Thus, predictions are extrapolated which can be an
issue if the execution is non deterministic. This simulation type is
not proper for HPL, whereas online simulation (used by SimGrid) means
the simulator directs the execution; it decides which process to run
at which time. SimGrid provides accurate performance models for
application with heavy network use, as it considers network topology
and heterogeneity. SMPI, based on SimGrid, runs MPI ranks on mutually
exclusive threads, so whenever a thread enters an MPI call, SMPI moves
its clock ahead of the time spent computing since the last MPI call.


****** DONE About how to emulate HPL
:LOGBOOK:  
- State "DONE"       from "STARTED"    [2019-05-02 thursday 10:51]
- State "STARTED"    from "TODO"       [2019-05-02 thursday 09:39]
:END:      
Most of the calculation time of HPL is spent in the dgemm and dtrsm
(A.x = b equations) kernels, so replacing these two by models greatly
reduces simulation time: it was only be necessary to make a SMPI call
when one of these kernels were used (this makes results incorrect). 
Likewise, other functions were replaced since they used incorrect data
because of dgemm and dstrsm. Initialization of pseudo-random matrices
and correctness verification were also skipped, changing the idamax
function to return random values, making the simulation deterministic.

Memory consumption was scaled down by sharing the input matrix A
between all MPI processes and indicating that its data can be
overwritten. For the panel matrix, containing matrix indexes, data
corruption cannot be risked so a partial shared malloc was introduced,
to only share the range of values that doesn't have matrix indexes. 
The number of allocations and page faults was also greatly reduced by
reusing the allocated panels instead of getting more of them. Also, a
lot of calls to memcopy were avoided by making SMPI aware of which
memory areas were private or not. Calls to mmap (used to remap the
data segment to the private copy of the MPI rank when context
switching) were also avoided by loading several times the data segment
into memory. Finally, huge pages were used so the page table wouldn't
be too large.

All of these changes allowed a reduced complexity by removing the
O(N³) part.


****** DONE About the chosen models for kernels and communications
:LOGBOOK:  
- State "DONE"       from "STARTED"    [2019-05-02 thursday 16:05]
- State "STARTED"    from "TODO"       [2019-05-02 thursday 10:51]
:END:      
Several modeling notations were chosen. M-0 means the duration is
roughly constant and independant of the input parameters. M-1 means
the performance depends on a combination of the parameters. M-2
is used when a polynomial model is required (for complex behavior). 
M_H means the platform has spatial variability and modeling should
be done for each host, and M' is used when the duration is linear for
specific parameters values. The same was done for the noise: N-0 means
no noise, N-1 means the noise has a normal distribution, N-2 means the
noise has to be modelised by a polynomial function. N_H means noise
estimations are made per-host and N' is when the noise is modelised by
several normal distributions.

Communications between MPI nodes are mostly linear in message size but
vary depending on the protocol used. The chosen model was a M'-1
(linear within each host but discontinuous) N'-1 (complex distribution
of linear noise) model, with its paramaters estimated by pytree: the
message size range and the 2-4 modes of the normal distribution mixture.

For dgemm, a polynomial model (M_H-2) was required because of the spatial
variability: depending on the value of M*N*K (so on the matrix size),
some durations will be higher than others regardless of the node used
(which means dgemm doesn't have a linear behavior). There is also some
temporal variability, modelized here by a random call. For other BLAS
and HPL kernels, a linear model M-1 is close enough to reality; but the
noise needs to follow a N-2 model because the variability it provides
increases with the value of the parameters (which indicates a
polynomial model).


****** DONE The simulation at scale
:LOGBOOK:  
- State "DONE"       from "STARTED"    [2019-05-02 thursday 16:53]
- State "STARTED"    from "TODO"       [2019-05-02 thursday 16:20]
:END:      
After working on these optimizations, an emulation was done at scale
using the Dahu cluster, with a high number of iterations, complex
communication patterns and more MPI processes than usual. The
emulation was stopped after five iterations, to compare to real runs:
the communication durations were a bit too optimistic, and it was
noted that using a complex model makes more realistic traces. Several
simulations were then done to figure out which model would be more
accurate, and the models that are the closest to reality are: M-1 N-2
for the kernels, M_H-2 N_H-2 for dgemm and M'-1 N-0 for the network. 
Adding a linear noise for the network doesn't have any visible effect.

This was compared to the run made on the Stampede cluster. Considering
the input parameters for this run and the result, an optimistic model
(M'-1 N-0) was chosen for the dgemm and dtrsm functions, as well as
for the MPI communications, while ignoring the other functions. 
Although usually the simulations are within of few percent of reality,
the performance of this one was much lower than the performance of the
Stamepde run, which used a modified version of HPL and different
parameters than the ones printed by HPL. It was also found that the
communications had been optimized for the run, which explains the
difference between the simulation and the reality.

*** 2019-04-30 tuesday
**** Installed Emacs and Org-mode and all required dependencies. :ORGMODE:
Followed the given MOOC to understand how Org-mode functions, and how
to take efficient notes. Learned the basics of org-mode, and of the
keyboard shortcuts introduced by the provided emacs initialization.

**** Began writing the journal
** 2019-05 may
*** 2019-05-02 thursday
**** Attended the keynote speech about contrasting artifical and human intelligence by Jean-Louis Dessales :SEMINAR:
***** Introduction
Some artificial intelligence techniques were recently able to scale
up, provoking what many consider as a technical revolution. However,
the type of AI that proved so successful in the past decade relies on
the exploitation of massive data, and is limited to narrow domains of
expertise. By contrast, human intelligence is very efficient at making
broad inferences from limited evidence. I will highlight a few
qualitative differences between artificial intelligence and human
intelligence. These differences are mainly due to a small set of
cognitive operations, such as contrast or simplicity detection, that
human beings perform on the fly. I will also suggest that attempting
to bridge the gap between these two forms of intelligence might be the
best way to improve artificial systems in the future.

***** Notes
- It is often said that artificial intelligence will eventually replace
 mankind, but Jean-Louis Dessales clearly doesn't think so. It is
 foolish for him to believe there is a loss function that can be
 applied for everything.

- There are specific characteristics of artifical intelligence that
  makes them too different from human beings to be able to solve every
  problem humans are able to.
 
  + For instance, neuronal networks work best when they can analyse a
    lot of data, to then be able to recognize it. This can work for
    fields such as image recognition, but not for particular fields
    such as criminal investigation where every case is different and
    the possible similarity between cases is only an average and
    cannot be exact. Also artificial intelligence work without biases,
    unlike human beings.

  + Artificial intelligence function in an isotropic way, they can
    learn how to recognize language even if the order of the words in
    the sentences is mixed up; whereas this will just confuse human
    beings. However, even though artificial intelligences recognize
    language they do not understand it, and if they try to speak they
    often make no sense because they look at semantics similarity and
    not semantics itself. Any idoms are lost to them, because they
    will not recognize it.

  + Neuronal networks are also unable to recognize a pattern if they
    were not introduced to it, and have very narrow expertise. For
    example, if given a sequence of numbers, they will be unable to
    find the next ones (ex: 1223334444). As they work with recognition
    instead of understanding, trying to broaden their expertise will
    sometimes fail. For example a neuronal network made to recognize
    skin cancer cannot recognize psoriasis, and trying to change it
    will reduce their ability to see skin cancer.

- When attempting to solve problems, human beings look for simplicity:
  the least complexity is the best. How humans see complexity and
  unexpectedness is by a difference between what they expect and what
  they see.

  + Unexpectedness can be defined as a complexity drop between what was
    expected and what happened. For example, seeing a "simple" (well
    known) person in a "complex" (rarely visited) place is unexpected,
    because of the difference in complexity.

- Because of the way humans learn by making connections (for example
  when learning a new word the context is immediately connected with
  it; if a child hears their parent talk about "chopping the fish"
  they are able to connect it to the place they heard it, to the food
  associated with it...), artificial intelligence cannot replace
  humans in the sense that this process of learning is completely
  foreign to them. They do not process the relations between objects,
  they focus on pattern recognition.
*** 2019-05-03 friday
**** STARTED Read [[http://xcelab.net/rm/statistical-rethinking ][A Bayesian Course with examples in R and Stan]] by Richard McElreath :PAPER:STAN:
:LOGBOOK:  
- State "STARTED"    from "INTERRUPTED" [2019-05-09 jeu. 16:42]
- State "INTERRUPTED" from "STARTED"    [2019-05-06 lun. 18:52]
:END:      
***** About models
- Models are like golems, powerful but easy to misuse.
- Use of models is wide spread and necessary but giving too many
  models is confusing, you need to find the right one. statistical
  tools are not diverse enough.
- Falsifying null hypothesis isn't enough. it would be more logical to
  falsify an explanatory model because falsifying a null model bring
  very little knowledge.
- Hypotheses correspond to several process models, and statistical
  models correspond to several process models
- Bayesian statistics are able to process small samples and don't need
  a prior hypothesis to confirm or reject. In bayesian statistics,
  probabilities are interpreted as uncertainty, models' parameters are
  modelled by probability laws and parameters are approximated by execution.
- The priors are observations/deductions that we know before the
  analysis and that helps guide the outcome.

***** About priors, likelihood and posteriors
- Small worlds are the logical worlds of the models. There are no
  surprises, and it's possible to verify the models's logic and if it
  works properly. Whereas in large worlds there might be unpredicted
  events, and the models never completely encompass large worlds.
- Bayesian analysis are garden of forking data: multiple paths exist,
  with each one branching more possibilities as we explore the
  paths. By getting more observations, we prune some path so to keep
  only the ones consistant to our data.
- Prior information can help us find the plausability of each path.
- The plausability of x knowing y is proportional to the number of
  ways x can produce y * the prior plausability of x.
- Models are built on the way, updated each time we get data that
  confirms or denies it's prior assumptions. They learn with each set
  of plausabilities.
- A model is a mix of a likelihood, which represents the plausability
  of the data given a fixed value as parameter, several parameters and
  a prior (the plausability of each value of the parameters).
- Bayes theorem: P(A∩B) = P(A|B)Pr(B) = P(B|A)Pr(A) 
  aka Posterior = Likelihood*Prior / Average Likelihood
  Pr(A|B) = Pr(B|A)*Pr(A)/(Pr(B|A)*Pr(A))+(Pr(B|A¯)*Pr(A¯))
- Grid approximation uses a finite grid of parameters values and
  scales poorly. When we have several parameters, we use a quadratic
  approximation (to describe the normal shape of the posterior).

***** About samples
- The false positive test: It's a counterintuitive result to a simple
  test. Knowing the amount of people afflicted in the general
  population, the probability for the test to correctly detect the
  illness, and the probability for it to have a false positive result,
  what is the probability, given that the test is positive, that the
  test subject has the illness? It's often lower than expected.
- Working on samples of parameters' values is easier and more
  intuitive, especially when models become complicated.
- Sampling is useful to summarize and understand the posterior
  distribution
- Sampling can also help simulate the model, to check it, or to
  simulate new predictions.

***** About linear and polynomial models
- Normal distributions are extremely used because they are common in
  nature and easy to calculate with.
- Two perspectives: ontological and epistemological. The first sees
  fluctuations, that when summed up make a symmetrical Gaussian
  distribution. For the second, we only know the mean and variance and
  considers Gaussian distribution because it is the most common and
  least surprising (maximu entropy).
- To build a model, we need to know the outcomes, the likelihood, the
  predictors and how they relate to the likelihood and the priors.
- Priors need to be at least a little precise, otherwise they bring
  nothing and the posterior will only depend on the data. Bad priors
  are relatively harmless, only useless.
- Polynomial regression is not advised

***** About multivariate linear models
- Multivariate linear models are much like linear models but with
  several variables. For example, the likelihood y follows a normal
  distribution based on alpha + beta_r x + beta_s z with beta_r and beta_s
  coefficients that measure the association between x and y.
- Sometimes, the association between two variables is quite hidden,
  and so a regression with both is needed to see it. In this case,
  both need to be considered or their relationship with the posterior
  will be hidden.
- Sometimes, adding variables is neither useful nor desirable: it may
  falsify the result of the regression because two predictor variables
  will be highly corrolated (multicollinearity). And including
  post-treatment variables will hide the relationship between the
  outcome and the first variable if there are unobserved confounders
  between the outcome and the post-treatment variable. 

***** About overfitting and regularization
**** STARTED [[https://www.youtube.com/watch?v=BWEtS3HuU5A&list=PLDcUM9US4XdM9_N6XUUFrhghGJ4K25bFc&index=10 ][Listened to several lectures by Richard McElreath]] :VIDEO:STAN:
:LOGBOOK:  
- State "STARTED"    from "INTERRUPTED" [2019-05-09 jeu. 17:08]
- State "INTERRUPTED" from "STARTED"    [2019-05-06 lun. 18:52]
:END:      
These lectures are available on his youtube channel, and explain in
more detail what his book is about.

*** 2019-05-06 monday
**** Continued to read the book, notes on May 3rd section
**** Talked to Tom about a simple Stan example and notebooks
**** Started to use Stan with simple cases, following Tom's example :STAN:R:

We start by generating some data following a relatively straight line,
with some noise.

#+begin_src R :results output :session *R* :exports both
generate_dataset=function(intercept, coefficient, N, min_x=0, max_x=100, sigma=1){
    x = sample(min_x:max_x,N,replace=T) 
    y = coefficient * x + intercept + rnorm(N,sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}
df=generate_dataset(50, -2, 500, sigma=15)
head(df)
#+end_src

#+RESULTS:
:  
:    x          y
: 1 53 -50.228060
: 2 60 -66.521457
: 3 26   4.949108
: 4 18   2.624106
: 5 15  24.501256
: 6 24  -9.271574

#+begin_src R :results output graphics :file ./images/ex1_figure.png :exports both :width 600 :height 400 :session *R* 
library(ggplot2)
ggplot(df, aes(x=x, y=y))+geom_point(alpha=0.3)
#+end_src

#+RESULTS:
[[file:./images/figure.png]]

Then, we define a stan model to find our parameters (intercept,
coefficient and sigma) that we now assume unknown 

#+begin_src R :results output :session *R* :exports both
library(rstan)

modelString = "data { // the observations
    int<lower=1> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real coefficient;
    real<lower=0> sigma; // indication: sigma cannot be negative
} 
model {
    // We define our priors
    intercept   ~ normal(0, 10); // We know that all the parameters follow a normal distribution
    coefficient ~ normal(0, 10);
    sigma       ~ normal(0, 10);

    // Then, our likelihood function
    y ~ normal(coefficient*x + intercept, sigma);
}
"
sm = stan_model(model_code = modelString)

#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: ea4b5a288cf5f1d87215860103a9026e.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                mean se_mean   sd     2.5%      25%      50%      75%    97.5%
intercept      49.74    0.05 1.36    47.04    48.85    49.74    50.63    52.53
coefficient    -2.00    0.00 0.02    -2.04    -2.01    -1.99    -1.98    -1.95
sigma          15.19    0.01 0.47    14.31    14.88    15.18    15.49    16.13
lp__        -1621.74    0.04 1.18 -1624.81 -1622.26 -1621.43 -1620.86 -1620.41
            n_eff Rhat
intercept     896 1.01
coefficient   953 1.01
sigma        1134 1.01
lp__          763 1.00

Samples were drawn using NUTS(diag_e) at Tue May  7 10:50:03 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

As we can see, Stan can find back the parameters given a close enough
model.

#+begin_src R :results output graphics :file ./images/ex1_stan_plot.png :exports both :width 600 :height 400 :session *R* 
stan_plot(fit)
#+end_src

#+RESULTS:
[[file:./images/stan_plot.png]]

#+begin_src R :results output graphics :file ./images/ex1_stan_trace.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)
#+end_src

#+RESULTS:
[[file:./images/stan_trace.png]]

#+begin_src R :results output graphics :file ./images/ex1_stan_hist.png :exports both :width 600 :height 400 :session *R* 
stan_hist(fit)
#+end_src

#+RESULTS:
[[file:./images/stan_hist.png]]

#+begin_src R :results output graphics :file ./images/ex1_stan_dens.png :exports both :width 600 :height 400 :session *R* 
stan_dens(fit) #attention à la densité, elle n'est pas toujours pertinente
#+end_src

#+RESULTS:
[[file:./images/stan_dens.png]]

*** 2019-05-07 tuesday
**** Search for a way to generate data once Stan has found the parameters :STAN:
- Looked at different packages such as rstanarm but they are too restricted
- Looked at Stan's program blocks
  + The data block lists the data we'll give to Stan. It's possible to
    use restrictions between < > (the data won't be negative...). 
    Within a block, anything declared, can then be used subsequently.
  + The transformed data block is used to create new data based on the
    input data. It has no particular use for this case.
  + The parameters block indicates the parameters that will be estimated
    by Stan.
  + The transformed parameters block includes optional parameters that
    are dependent on the previous parameters.
  + The model block specifies the priors and likelihood
  + Finally, the generated quantities block calculates any data based
    on the model's results, so it might be useful in this case.
**** Tried to use Stan with more complex examples :STAN:R:
***** With noise depending on x

We generate data, this time with noise depending on x. This time,
we're careful not to have any x=0 since that would produce bogus data
for sigma.

#+begin_src R :results output :session *R* :exports both
generate_dataset=function(intercept, coefficient, N, min_x=1, max_x=100, sigma=1){
    x = sample(min_x:max_x,N,replace=T) 
    y = coefficient * x + intercept + rnorm(N,sd=sigma*x)
    df = data.frame(x=x,y=y)
    return(df)
}
df=generate_dataset(12, 50, 500, sigma=15)
head(df)
#+end_src

#+RESULTS:
:  
:    x        y
: 1 17 1200.497
: 2 52 2663.708
: 3 85 4234.701
: 4 42 2494.610
: 5 68 4240.304
: 6 86 2746.118

#+begin_src R :results output graphics :file ./images/ex2_figure.png :exports both :width 600 :height 400 :session *R* 
library(ggplot2)
ggplot(df, aes(x=x, y=y))+geom_point(alpha=0.3)
#+end_src

#+RESULTS:
[[file:./images/ex2_figure.png]]

Then, we define a stan model to find our parameters

#+begin_src R :results output :session *R* :exports both
library(rstan)

modelString = "data { // the observations
    int<lower=1> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real coefficient;
    real<lower=0> sigma; // indication: sigma cannot be negative
} 
model {
    // We define our priors
    intercept   ~ normal(0, 10); // We know that all the parameters follow a normal distribution
    coefficient ~ normal(0, 10);
    sigma       ~ normal(0, 10);

    // Then, our likelihood function
    y ~ normal(coefficient*x + intercept, sigma*x);
}
"
sm = stan_model(model_code = modelString)

#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example

Inference for Stan model: 12d920cec7fd90b16640e34f6f5a767a.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                mean se_mean   sd     2.5%      25%      50%      75%    97.5%
intercept       9.42    0.13 4.87    -0.17     6.16     9.28    12.77    19.16
coefficient    50.18    0.02 0.68    48.86    49.73    50.20    50.65    51.48
sigma          14.42    0.01 0.45    13.60    14.10    14.41    14.71    15.32
lp__        -3438.43    0.04 1.20 -3441.45 -3438.98 -3438.13 -3437.57 -3437.08
            n_eff Rhat
intercept    1364 1.00
coefficient  1629 1.00
sigma        1901 1.00
lp__          933 1.01

Samples were drawn using NUTS(diag_e) at Tue May  7 11:03:57 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

This time, the intercept is farther from its actual value, because of
a high standard deviation in its estimation (4.87). This high
deviation is seen in the plot as a red line.

#+begin_src R :results output graphics :file ./images/ex2_plot.png :exports both :width 600 :height 400 :session *R* 
stan_plot(fit)
#+end_src

#+RESULTS:
[[file:./images/ex2_plot.png]]

#+begin_src R :results output graphics :file ./images/ex2_trace.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)
#+end_src

#+RESULTS:
[[file:./images/ex2_trace.png]]

#+begin_src R :results output graphics :file ./images/ex2_hist.png :exports both :width 600 :height 400 :session *R* 
stan_hist(fit)
#+end_src

#+RESULTS:
[[file:./images/ex2_hist.png]]
*** 2019-05-09 thursday
**** Continued to read the book, notes on May 3rd section
**** Continued watching the MOOC, particularly the first and fourth modules :ORGMODE:
**** DONE An example with generated quantities ([[Generated Quantities][Reworked in friday's entry]]) :STAN:R:
:LOGBOOK:  
- State "DONE"       from "STARTED" [2019-05-09 jeu. 13:51]
- State "STARTED"    from "TODO"       [2019-05-09 jeu. 09:42]
:END:      
As usual we start by generating data. We'll take a simple example.

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(rstan)
generate_dataset=function(intercept, coefficient, N, min_x=0, max_x=100, sigma=1){
    x = sample(min_x:max_x,N,replace=T) 
    y = coefficient * x + intercept + rnorm(N,sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}
df=generate_dataset(12, 3, 500, sigma=15)
head(df)
#+end_src

#+RESULTS:
#+begin_example
   x         y
1 50 162.69930
2 46 160.77595
3 16  60.37135
4 24 100.82896
5  7  35.89649
6 62 198.99881
#+end_example

#+begin_src R :results output graphics :file ./images/ex3_figure.png :exports both :width 600 :height 400 :session *R* 
library(ggplot2)
ggplot(df, aes(x=x, y=y))+geom_point(alpha=0.3)
#+end_src

#+RESULTS:
[[file:./images/ex3_figure.png]]

Then, we define a stan model to find our parameters, and we generate
some y after making the sample.

#+begin_src R :results output :session *R* :exports both
library(rstan)

modelString = "data { // the observations
    int<lower=1> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real coefficient;
    real<lower=0> sigma; // indication: sigma cannot be negative
} 
model {
    // We define our priors
    intercept   ~ normal(0, 10); // We know that all the parameters follow a normal distribution
    coefficient ~ normal(0, 10);
    sigma       ~ normal(0, 10);

    // Then, our likelihood function
    y ~ normal(coefficient*x + intercept, sigma);
}
generated quantities {
  vector[N] y_pos; // posterior predictions
  for (n in 1:N)
    y_pos[n] = normal_rng(coefficient*x[n]+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)

#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
#+end_src

#+begin_src R :results output graphics :file ./images/ex3_plot.png :exports both :width 600 :height 400 :session *R* 
stan_plot(fit, pars = c("intercept", "coefficient", "sigma"))
#+end_src

#+RESULTS:
[[file:./images/ex3_plot.png]]

#+begin_src R :results output graphics :file ./images/ex3_trace.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit, pars = c("intercept", "coefficient", "sigma"))
#+end_src

#+RESULTS:
[[file:./images/ex3_trace.png]]

#+begin_src R :results output graphics :file ./images/ex3_hist.png :exports both :width 600 :height 400 :session *R* 
stan_hist(fit, pars = c("intercept", "coefficient", "sigma"))
#+end_src

#+RESULTS:
[[file:./images/ex3_hist.png]]

Now we look at the generated y

#+begin_src R :results output :session *R* :exports both
extracted=rstan::extract(fit)
mean(extracted$y_pos)
mean(data$y)
#+end_src

#+RESULTS:
: 
: [1] 162.9266
: 
: [1] 162.9518

From this mean, we can say the generated y are reasonably close enough
to the data we generated at the beginning, but not exactly similar of
course, since we have an approximation of the parameters (intercept,
coefficient and sigma). We'll compare the two on a same plot.

#+begin_src R :results output graphics :file ./images/ex3_comparative_plot.png :exports both :width 600 :height 400 :session *R* 
library(matrixStats)
median=colMedians(extracted$y_pos, na.rm = TRUE)
mean=colMeans(extracted$y_pos, na.rm = FALSE)
data2=data.frame(x=data$x,y_pos=mean, y_prev=data$y, y_med=median)

ggplot() + geom_point(data=data2, aes(x=x, y=y_prev), alpha=0.3, color="darkgreen")+ geom_point(data = data2,aes(x = x,y = y_pos), alpha=0.5, color="darkblue") + geom_point(data = data2,aes(x = x,y = y_med), alpha=0.1, color="darkred")
#+end_src

#+RESULTS:
[[file:./images/ex3_comparative_plot.png]]

Surprisingly, the y generated by Stan are very linear compared to the
ones we generated in the beginning. It looks as if the noise wasn't
taken in account. It's probably a result of the colMeans. We looked at
colMedian, to see if the result would be different but it seems it
gives almost the same points.
*** 2019-05-10 friday
**** REWORK <<Generated Quantities>>  :STAN:R:
Following Tom's advice, we changed the model for the generated
quantities test.

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(rstan)
generate_dataset=function(intercept, coefficient, N, min_x=0, max_x=100, sigma=1){
    x = sample(min_x:max_x,N,replace=T) 
    y = coefficient * x + intercept + rnorm(N,sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}
df=generate_dataset(12, 3, 500, sigma=15)
#+end_src

#+begin_src R :results output :session *R* :exports both
library(rstan)

modelString = "data { // the observations
    int<lower=1> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real coefficient;
    real<lower=0> sigma; // indication: sigma cannot be negative
} 
model {
    // We define our priors
    intercept   ~ normal(0, 10); // We know that all the parameters follow a normal distribution
    coefficient ~ normal(0, 10);
    sigma       ~ normal(0, 10);

    // Then, our likelihood function
    y ~ normal(coefficient*x + intercept, sigma);
}
generated quantities {
   real x_pos = x[categorical_rng(rep_vector(1,N) / N)];
   real y_pos; // posterior predictions
   y_pos = normal_rng(coefficient*x_pos+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)

#+end_src

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
                mean se_mean    sd     2.5%      25%      50%      75%    97.5%
intercept      11.84    0.04  1.30     9.35    10.93    11.85    12.72    14.35
coefficient     2.98    0.00  0.02     2.94     2.97     2.98     3.00     3.03
sigma          14.76    0.01  0.49    13.83    14.42    14.75    15.08    15.74
x_pos          50.89    0.67 29.37     3.00    25.00    51.00    77.00    98.00
y_pos         164.20    2.02 89.09    15.22    87.91   165.14   242.90   308.71
lp__        -1595.10    0.05  1.25 -1598.47 -1595.65 -1594.77 -1594.20 -1593.70
            n_eff Rhat
intercept     831 1.00
coefficient   871 1.00
sigma        1399 1.00
x_pos        1897 1.00
y_pos        1940 1.00
lp__          636 1.01

Samples were drawn using NUTS(diag_e) at Fri May 10 14:05:37 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

Now we look at the generated y

#+begin_src R :results output graphics :file ./images/ex3_reworked_comparative_plot.png :exports both :width 600 :height 400 :session *R* 
extracted=rstan::extract(fit)
df_generated = data.frame(x=extracted$x_pos, y=extracted$y_pos, origin='generated')
df_generated = df_generated[sample(nrow(df_generated), nrow(df)), ]  # there are more points in df_generated, so to have a nicer plot we sample exactly the same number than in df
df$origin = 'initial'
tmp = rbind(df, df_generated)
ggplot(tmp, aes(x=x, y=y, color=origin)) + geom_point(alpha=0.5)
#+end_src

#+RESULTS:
[[file:./images/ex3_reworked_comparative_plot.png]]

This way we don't have to make a mean that erases the noise's effect,
and the generated y could definitely be used for further calculation. 

**** Tried to use Stan with more complex examples :STAN:R:
***** With a full polynomial model ([[Polynomial Model][Reworked in tuesday's entry]]):
We'll assume for this one that the noise follows a simple model, to
make it easier. We generate data, this time with three
parameters. We'll call them second_ degree, first_ degree and intercept.

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(rstan)

generate_dataset=function(intercept, second_degree, first_degree, N, min_x=0, max_x=100, sigma){
    x = sample(min_x:max_x,N,replace=T) 
    y = second_degree * x^2 + first_degree * x + intercept + rnorm(N, sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}

df=generate_dataset(100,2,6, 500, sigma=150)
head(df)
#+end_src

#+RESULTS:
#+begin_example
   x          y
1  9   419.2124
2 90 16933.1164
3 93 18001.0911
4 89 16522.1687
5 20  1013.0832
6  9   534.0954
#+end_example

#+begin_src R :results output graphics :file ./images/ex4_figure.png :exports both :width 600 :height 400 :session *R* 
ggplot(df, aes(x=x, y=y))+geom_point(alpha=0.3, color="darkblue")
#+end_src

#+RESULTS:
[[file:./images/ex4_figure.png]]

#+begin_src R :results output :session *R* :exports both
modelString = "data { // the observations
    int<lower=0> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real second_degree;
    real first_degree;
    real<lower=0> sigma; // indication: sigma cannot be negative
}

model {
     // We define our priors
    second_degree ~ normal(0, 10);
    first_degree ~ normal(0, 10);
    intercept   ~ normal(0, 10);
    sigma       ~ normal(0, 10);
    // Then, our likelihood function
    for (n in 1:(N)){
      y ~ normal(second_degree*(x[n]*x[n]) + first_degree*x[n] + intercept, sigma);
    }
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Warning messages:
1: There were 155 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
2: There were 8 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
3: Examine the pairs() plot to diagnose sampling problems

Inference for Stan model: e6c4a717223c83fc91c9c4018606219b.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                     mean se_mean      sd        2.5%         25%         50%
intercept          216.01  170.46  347.33        1.02        5.15       16.60
second_degree       -2.18    0.06    0.11       -2.26       -2.24       -2.24
first_degree       262.91    6.67   13.61      237.07      264.29      270.41
sigma             4715.81   10.28   21.48     4672.10     4713.75     4725.89
lp__          -2463195.40  827.57 1668.03 -2464315.63 -2464275.36 -2464166.68
                      75%       97.5% n_eff  Rhat
intercept          184.50      864.26     4  6.60
second_degree       -2.19       -1.96     4  5.75
first_degree       271.17      272.17     4  6.17
sigma             4729.56     4735.32     4  3.92
lp__          -2462778.64 -2460240.49     4 12.27

Samples were drawn using NUTS(diag_e) at Fri May 10 12:34:16 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

The sampling issued several warnings, which isn't surprising because
the parameters found by Stan don't match at all with the ones we chose
to generate the data.

#+begin_src R :results output graphics :file ./images/ex4_trace.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit, pars = c("intercept", "second_degree", "first_degree", "sigma"))+ scale_color_manual(values = blues9)
#+end_src

#+RESULTS:
[[file:./images/ex4_trace.png]]

If we look at the trace it becomes clear that our model must have been
wrong, because the chains don't converge towards each other at
all. It's apparent polynomial regression cannot be done with a
model based on linear regression and only slightly changed.

Stan's warnings advises us to look at the pairs plot.

#+begin_src R :results output graphics :file ./images/ex4_pairs.png :exports both :width 600 :height 400 :session *R* 
pairs(fit)
#+end_src

#+RESULTS:
[[file:./images/ex4_pairs.png]]

Apparently, there should be a negative relationship between lp__ and
energy__, however this plot is quite difficult to understand without
indication.

***** EDIT 2019-05-14 Changed the model following Tom's advice

Tom figured out a mistake in the declaration of the likelihood
function.

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,2,6, 500, sigma=150)

modelString = "data { // the observations
    int<lower=0> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma; // indication: sigma cannot be negative
}

model {
    second_degree ~ normal(0, 10);
    first_degree ~ normal(0, 10);
    intercept   ~ normal(0, 10);
    sigma       ~ normal(0, 10);
    // Then, our likelihood function
    for (n in 1:(N)){
      y[n] ~ normal(second_degree*(x[n]*x[n]) + first_degree*x[n] + intercept, sigma);
    }
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 10461bf4fda3fe871ad7e125904745bb.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean   sd     2.5%      25%      50%      75%
intercept        28.85    0.46 8.98    12.14    22.61    28.56    34.55
first_degree      9.43    0.02 0.55     8.36     9.07     9.44     9.80
second_degree     1.97    0.00 0.01     1.96     1.97     1.97     1.97
sigma           134.36    0.09 3.26   128.36   132.07   134.29   136.65
lp__          -2881.15    0.07 1.43 -2884.70 -2881.83 -2880.84 -2880.10
                 97.5% n_eff Rhat
intercept        47.97   381 1.01
first_degree     10.50   577 1.01
second_degree     1.98   740 1.00
sigma           140.60  1293 1.00
lp__          -2879.40   479 1.01

Samples were drawn using NUTS(diag_e) at Tue May 14 14:42:59 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

Surprisingly, despite the correction, the results are still wildly
off. We think the outcome might be influenced by faulty priors, so we
try removing them.

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,2,6, 500, sigma=150)

modelString = "data { // the observations
    int<lower=0> N; // number of points
    vector[N] x;
    vector[N] y;
}
parameters { // what we want to find
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma; // indication: sigma cannot be negative
}

model {
    // Then, our likelihood function
    for (n in 1:(N)){
      y[n] ~ normal(second_degree*(x[n]*x[n]) + first_degree*x[n] + intercept, sigma);
    }
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: d2b16cc8b2214d87ccd7963b321901c3.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean    sd     2.5%      25%      50%      75%
intercept       105.29    1.07 17.86    71.14    92.35   105.77   117.64
first_degree      5.66    0.05  0.88     4.01     5.05     5.64     6.29
second_degree     2.00    0.00  0.01     1.99     2.00     2.00     2.01
sigma           148.09    0.17  4.88   139.01   144.77   147.95   151.36
lp__          -2743.12    0.05  1.43 -2746.50 -2743.85 -2742.79 -2742.08
                 97.5% n_eff Rhat
intercept       139.21   278 1.02
first_degree      7.34   331 1.02
second_degree     2.02   476 1.01
sigma           158.16   805 1.01
lp__          -2741.28   680 1.01

Samples were drawn using NUTS(diag_e) at Tue May 14 14:49:24 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

This time the results are much closer to the truth, and very similar
to the results we found with the second method (on monday's 13th
entry). The conclusion is that we need to find more precise priors,
or give a very general one (or none) so it won't influence the outcome
in a way we don't want it to.

**** Continued to read the book, notes on May 3rd section
Surprisingly, the part on polynomial regression is very sparse and
doesn't really help us make a functional model.
*** 2019-05-13 monday
**** Continued to read the book, notes on May 3rd section
**** A second try with a polynomial model :STAN:R:
We still have a simple model for the noise, and three parameters:
second_ degree, first_ degree and intercept.

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(rstan)

generate_dataset=function(intercept, first_degree, second_degree, N, min_x=0, max_x=100, sigma){
    x = sample(min_x:max_x,N,replace=T) 
    y = second_degree * (x^2) + first_degree * x + intercept + rnorm(N, sd=sigma)
    df = data.frame(x=x,y=y)
    return(df)
}

df=generate_dataset(100,6,2, 500, sigma=150)
x2=df$x^2 #Need to add it here because we cannot in the stan model
head(df)
#+end_src

#+RESULTS:
#+begin_example
   x          y
1  0   257.9807
2 45  4353.7896
3 65  8872.7711
4 91 17196.1885
5 86 15519.8198
6 42  3911.2678
#+end_example

#+begin_src R :results output :session *R* :exports both
modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] x2;
    vector[N] y;
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}

model {
    // We use no prior, as the previous ones are too vague to be useful for computing the posterior
    y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,x2=x2,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:

#+begin_example
Inference for Stan model: 6001096c2a1f8ca4aec729de06bdbead.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean    sd     2.5%      25%      50%      75%
intercept        74.78    0.87 18.35    39.43    61.72    74.65    87.40
first_degree      6.94    0.04  0.88     5.18     6.36     6.94     7.53
second_degree     1.99    0.00  0.01     1.97     1.98     1.99     1.99
sigma           150.35    0.14  4.71   141.63   146.97   150.29   153.73
lp__          -2751.41    0.05  1.45 -2755.14 -2752.10 -2751.08 -2750.32
                 97.5% n_eff Rhat
intercept       111.46   448 1.01
first_degree      8.70   501 1.01
second_degree     2.01   597 1.01
sigma           159.24  1214 1.00
lp__          -2749.61   713 1.01

Samples were drawn using NUTS(diag_e) at Tue May 14 11:28:37 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

This time, the parameters found by Stan are quite close to the ones we
chose to generate the data, but it's still quite inaccurate,
especially for the intercept.

#+begin_src R :results output graphics :file ./images/ex4_reworked_trace.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)+ scale_color_manual(values = blues9)
#+end_src

#+RESULTS:
[[file:./images/ex4_reworked_trace.png]]

At least this time the chains converge towards each other, so we can
conclude this model is better than the previous but still needs to be
worked on to correct some inaccuracies (we probably need to add some
informative priors).

The surprising part is that depending on the value of sigma, the trace
can show a very different behavior. For example, with a small value of
sigma:

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,6,2, 500, sigma=4)
x2=df$x^2
modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] x2;
    vector[N] y;
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}

model {
    // We use no prior, as the previous ones are too vague to be useful for computing the posterior
    y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
data = list(N=nrow(df),x=df$x,x2=x2,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:

#+begin_example
Warning messages:
1: There were 229 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: There were 1 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
3: There were 4 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
4: Examine the pairs() plot to diagnose sampling problems

Inference for Stan model: 6001096c2a1f8ca4aec729de06bdbead.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean     sd     2.5%      25%     50%     75%   97.5%
intercept        93.74    7.03  15.26    57.08    89.96  100.45  100.97  103.14
first_degree      6.25    0.28   0.61     5.86     5.96    5.98    6.40    7.69
second_degree     2.00    0.00   0.01     1.99     2.00    2.00    2.00    2.00
sigma             6.33    1.70   3.89     3.68     3.87    4.81    7.13   15.05
lp__          -1056.63  103.39 215.81 -1592.99 -1110.91 -951.71 -921.93 -920.60
              n_eff Rhat
intercept         5 3.37
first_degree      5 3.28
second_degree     5 3.22
sigma             5 2.75
lp__              4 4.73

Samples were drawn using NUTS(diag_e) at Tue May 14 11:30:45 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R :results output graphics :file ./images/ex4_reworked_trace_small_sigma.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)+ scale_color_manual(values = blues9)
#+end_src

#+RESULTS:
[[file:./images/ex4_reworked_trace_small_sigma.png]]

This time the chains don't converge again, despite the fact that the
model is still the same.

If we try with a much higher value of sigma, the chains do converge
but the found parameters have a higher standard deviation for their
value (which is not surprising).

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,6,2, 500, sigma=986)
x2=df$x^2
modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] x2;
    vector[N] y;
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}

model {
    // We use no prior, as the previous ones are too vague to be useful for computing the posterior
    y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
data = list(N=nrow(df),x=df$x,x2=x2,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 6001096c2a1f8ca4aec729de06bdbead.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean     sd     2.5%      25%      50%      75%
intercept        54.93    5.63 132.23  -202.71   -34.30    49.64   143.42
first_degree      5.57    0.27   5.96    -6.25     1.39     5.94     9.69
second_degree     2.02    0.00   0.06     1.91     1.98     2.01     2.06
sigma          1006.73    1.16  32.68   945.09   984.52  1006.43  1028.69
lp__          -3700.35    0.06   1.50 -3704.02 -3701.05 -3700.00 -3699.23
                 97.5% n_eff Rhat
intercept       318.61   552 1.01
first_degree     16.65   501 1.01
second_degree     2.13   556 1.01
sigma          1071.30   792 1.01
lp__          -3698.51   672 1.00

Samples were drawn using NUTS(diag_e) at Tue May 14 11:32:12 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

As we can see here, the parameters are much too inaccurate to model
properly the situation.

#+begin_src R :results output graphics :file ./images/ex4_reworked_trace_high_sigma.png :exports both :width 600 :height 400 :session *R* 
stan_trace(fit)+ scale_color_manual(values = blues9)
#+end_src

#+RESULTS:
[[file:./images/ex4_reworked_trace_high_sigma.png]]
**** A look at generated quantities with a polynomial model :STAN:R:

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,6,2, 500, sigma=125)
x2=df$x^2
modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] x2;
    vector[N] y;
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}

model {
    y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
}
generated quantities {
   real x_pos = x[categorical_rng(rep_vector(1,N) / N)];
   real y_pos; // posterior predictions
   y_pos = normal_rng(second_degree*(x_pos^2)+first_degree*x_pos+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
data = list(N=nrow(df),x=df$x,x2=x2,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
#+end_src

#+begin_src R :results output graphics :file ./images/ex4_reworked_comparative_plot.png :exports both :width 600 :height 400 :session *R* 
extracted=rstan::extract(fit)
df_generated = data.frame(x=extracted$x_pos, y=extracted$y_pos, origin='generated')
df_generated = df_generated[sample(nrow(df_generated), nrow(df)), ]
df$origin = 'initial'
tmp = rbind(df, df_generated)
ggplot(tmp, aes(x=x, y=y, color=origin)) + geom_point(alpha=0.6)
#+end_src

#+RESULTS:
[[file:./images/ex4_reworked_comparative_plot.png]]

Once again, the data generated from the parameters found by stan seems
quite precise and close enough to the data we generated at the
beginning.
*** 2019-05-14 tuesday
**** REWORK <<Polynomial Model>>
***** A third model
We already tried two different models for this one; the first ended up
with unprecise parameters found, and so did the second one which also
had the disadvantage to need the vector of x squared as data.

Following Tom's advice, we try to do this in the transformed data
block.

#+begin_src R :results output :session *R* :exports both
df=generate_dataset(100,2,6, 500, sigma=150)

modelString = "data {
    int<lower=0> N;
    vector[N] x;
    vector[N] y;
}
transformed data{
    vector[N] x2;
    for (n in 1:N)
      x2[n]=x[n]*x[n];
}
parameters {
    real intercept;
    real first_degree;
    real second_degree;
    real<lower=0> sigma;
}
model {
    y ~ normal(second_degree*x2+first_degree*x+intercept, sigma);
}
"
sm = stan_model(model_code = modelString)
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
data = list(N=nrow(df),x=df$x,y=df$y)
fit = sampling(sm,data=data, iter=500, chains=8)
print(fit)
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: 91966ecf1347dea6d1c3f249ec4a3d80.
8 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=2000.

                  mean se_mean    sd     2.5%      25%      50%      75%
intercept        96.08    1.35 20.83    57.95    81.53    95.94   110.73
first_degree      5.73    0.05  0.97     3.81     5.07     5.72     6.38
second_degree     2.00    0.00  0.01     1.98     2.00     2.00     2.01
sigma           153.74    0.14  4.77   144.75   150.37   153.67   157.03
lp__          -2761.46    0.07  1.39 -2764.76 -2762.21 -2761.16 -2760.42
                 97.5% n_eff Rhat
intercept       135.65   239 1.01
first_degree      7.61   317 1.01
second_degree     2.02   420 1.01
sigma           163.07  1205 1.00
lp__          -2759.69   435 1.02

Samples were drawn using NUTS(diag_e) at Tue May 14 15:44:11 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

Once again, this new model doesn't change the precision of the results
but it is perhaps easier to understand and use.

***** TODO Taking a look at the priors
***** TODO Figuring out the right priors
